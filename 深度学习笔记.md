1. 深度学习简介

   人工神经网络称为人工智能的连接学派

   1. 感知机到人工神经网络

      + 1957年 弗兰克提出感知机模型（不是直接从功能的角度而是通过结构模拟）
      + 1969年 提出感知机模型连XOR问题无法解出，人工神经网络被打入网络
      + 辛顿发展了人工神经网络的反向传播法，从而可以构造两层以上神经网络，并且可以有效进行学习与训练，两层以上神经网络可以很轻松解决XOR问题。
      + 受限于当时的计算能力，另一方面缺乏大规模高质量的数据，神经网络本身就是一个黑箱，谁也不敢保证深度神经网络在深度这个方向上就能够取得更好的结果和精度。人工神经网络并没有继续沿着深度的方向发展下去。
      + 学术界的焦点朝向了另一个发展方向：寻找神经网络的基础理论。在弗拉基米尔和亚历克塞推进下，统计学习理论蓬勃发展，奠定了模式识别的数学基础，创造出了支持向量机这种极其实用简单的工具。与传统神经网络通过加深网络来提升精度相反，支持向量机的解决方案是将数据的维度提升，在高维种寻找能够将数据进行准确划分的方法，这种方法在数据量不是很大情况下很奏效。支持向量机称为20世纪90年代到21世纪初的宠儿。
      + 2006年 辛顿发表题为《利用神经网络进行数据的维度约减》的文章，提出了深度神经网络模型（DNN），并指出如果我们能够将神经网络的层数加深，并且精心设计训练网络的方式，那么这样深层次的神经网络就会具有超强的表达能力和学习能力。

   2. 深度学习时代

      + 深度学习首先在语音领域取得突破。微软的邓力邀请辛顿加入语音识别的深度神经网络模型的开发，使识别的准确度有了大幅的提升。
      + 机器视觉专家李飞飞，借助网友的力量构造出了ImageNet这样一个大规模、高精度、多标签的图像数据库。于是李飞飞开始每年举办一次图像识别大赛：ImageNet竞赛。2012年，辛顿和他的两个学生采用深层次的卷积神经网络（AlexNet）,在ImageNet竞赛中将分类错误率从25%降到了17%。将深度网络做到8层，且不需要任何预处理就能将图像分类任务做到这么好，还是头一次。深度神经网络称为了ImageNet竞赛的标配，从AlexNet到GoogleNet，网络深度不断增加，准确率不断提升。2012年后深度学习开始在学术圈流行起来。

   3. 深度学习发展

      + 2011年 谷歌X实验室的杰夫和吴恩达等人采用深度学习技术，让谷歌大脑深度神经网络观看了从Youtube上提取出来的30万张图像，并让机器自动进行精简，谷歌大脑自己学出了一张猫脸。看到发展前景后，以谷歌未代表的各大公司开始疯狂并购人工智能、人工智能初创公司和团队，促使更多的人才和创业公司投入到人工智能大潮中。

      + 自然语言处理领域，2013年，谷歌的托马斯提出了Word2Vec技术，它可以非常快捷有效地计算单词间的向量表示，为大规模使用人工神经网络技术处理人类语言奠定了重要基础

      + 2014年，谷歌开始尝试利用深度的神经循环网络来处理各种自然语言任务，包括机器翻译，自动对话，情绪识别、阅读理解等。2016年，谷歌机器翻译技术取得重大突破，采用了先进的深度循环神经网络和注意力机智的机器翻译在多种语言上已经基本接近人类水平。

      + 强化学习与深度学习的结合，深度学习在计算机游戏、博弈等领域同样取得了重大进展。

        2015年被谷歌收购的DeepMind团队研发了一种“通用人工智能算法”，他可以像人类一样，通过观察计算机游戏屏幕进行自我学习，利用同一套网络架构和超参数，从零开始学习每一款游戏，并最终打通了300多款雅达利游戏，在某些游戏上的表现甚至超越了人类。

      + 2016年 DeepMind团队又在博弈领域取得了重大突破。AlphaGo以4:1战胜人类围棋冠军。2017年 DeepMind团队创造的 AlphaGo 升级版 AlphaGo Zero ，它可以完全从零开始学习下围棋，而无需借鉴任何人类的下棋经验。仅经过大约3天训练，AlphaGo Zero就达到了战胜李世石的棋力水平。而到了21天后，世界上已经没有任何人类或程序可以在围棋上战胜它了。AlphaGo 的成功不仅标志着以深度学习技术为支撑的新一代人工智能技术大获全胜，更暗示着人工智能全新时代的来临。  

   4. 深度学习的影响因素

      1. 大数据

         如果没有足够大量的数据输入给深度神经网络，就无法发挥深度的作用。伴随着网络深度的增加，待拟合的参数自然会也会增加，如果没有与其相匹配的海量数据来训练网络，这些参数就完全变成了导致网络过拟合的垃圾，无法发挥作用

      2. 深度网络架构（整个网络体系的构建方式和拓扑连接结构）

         面对具体问题时，应该采用什么样的网络架构，如何选取参数，如何训练这个网络，仍然是影响学习效率和解决问题的重要因素。

         目前主要分为： 

         + 前馈神经网络  

           每一层的节点只跟它相邻层节点而且是全部节点相连（全连接的），分为输入层，隐藏层，输出层

         + 卷积神经网络

           卷积层和池化层

           图中每一个立方体都是一系列规则排列的人工神经元的集合。

           每个神经元到上一层次的连接称为卷积核，它们都是一种局域的小窗口。

           图中的小锥形可以理解为从高层的某一个神经元到低层多个神经元之间的连接。这个小锥形在立方体上逐像素的平移就构成了两层次之间的所有连接。到了最后两层，小立方体被压缩成了一个一维的向量，这就与普通的前馈神经网络没有区别。

           CNN这种特殊的架构可以很好地应用于图像处理，它可以使原始的图像即使在经历过平移、缩放等变换后仍然具有很高的识别准确性。正是因为具有这样特殊的架构，CNN才成功应用于计算机视觉、图像识别、图像生成，甚至AI下围棋、AI打游戏等广阔领域

         + 循环神经网络
      
           输入层，输出层是单层，中间的隐藏层的节点相互连接。隐藏层彼此之间还有大量的连接。
      
           RNN这种特殊的架构使得网络当前的运行不仅跟当前的输入数据有关，而且还与之前的数据有关。因此，这种网络特别适合处理诸如语言、音乐、股票曲线等序列类型的数据，整个网络的循环结构可以很好地应付输入序列之中存在的长程记忆性和周期性
      
         + 训练方式
      
           训练方式也会对结果产生很大影响。如果先将少量特定标签的数据输入网络，然后再拿剩下的数据去训练它，就会比一股脑把所有便签数据都输入更加有效，从而提高网络的“学习”能力。
      
      3. GPU 
      
         GPU非常擅长大规模的张量（高阶矩阵）运算，并且可以为这种运算加速，对包含多个数值的张量运算所需的平均时间远低于对每个数字的运算时间
      
   5. 深度学习的成功

      深度学习重要的本领在于它可以从海量的数据中自动学习，抽取数据中的特征。

      + 特征学习

        深度神经网络会把不同的信息表达到不同层次的网络单元中，这一提炼过程完全不需要手工干预，全凭机器学习过程自动完成。深度学习的本质就是这种自动提取特征的功能。

      + 迁移学习

        把一个训练好的神经网络切开，然后再把它拼接到另一个神经网络上，前半部分用于特征提取，后半部分网络去解决另一个完全不同的问题。

2. PyTorch简介

   1. PyTorch安装

   2. 与Python完美融合

      使用Pytorch与使用其他Python程序包没有任何区别
      
      与此形成鲜明对比的是TensorFlow，TensorFlow会将一个深度学习任务分为定义为计算图和执行计算过程，而定义计算图的过程就好像在使用一套全新的语言。PyTorch就没有这个缺点，从定义计算图到执行计算是一气呵成的。
      
   3. 张量（tensor）计算

      PyTorch的运算单元叫做张量，1阶张量即为1维数组（向量vector），2阶张量为2维数组（矩阵matrix），3阶张量即为3维数组

      ```python
      import torch
      x=torch.rand(5,3)
      x
      tensor([[0.1440, 0.7954, 0.8075],
              [0.1011, 0.0410, 0.4783],
              [0.1711, 0.4874, 0.0733],
              [0.8815, 0.3930, 0.4344],
              [0.8639, 0.1563, 0.8560]])
      
      ```
      
      ```python
      y=torch.ones(5,3)
      y
      tensor([[1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.]])
      ```
      
      ```python
      # pytorh矩阵叉乘法 mm() ,转置t()
      q=x.mm(y.t())
      q
      tensor([[1.7469, 1.7469, 1.7469, 1.7469, 1.7469],
              [0.6204, 0.6204, 0.6204, 0.6204, 0.6204],
              [0.7319, 0.7319, 0.7319, 0.7319, 0.7319],
              [1.7089, 1.7089, 1.7089, 1.7089, 1.7089],
              [1.8762, 1.8762, 1.8762, 1.8762, 1.8762]])
      ```
      
      ```python
      # pytorh张量与Numpy数组之间的转换
      import numpy as np
      x_tensor=torch.randn(2,3)
      y_numpy=np.random.randn(2,3)
      # 将张量转换为numpy
      x_numpy=x_tensor.numpy()
      # 将numpy转化为张量
      y_tensor=torch.from_numpy(y_numpy)
      print(x_tensor)
      print(x_numpy)
      print(y_numpy)
      print(y_tensor)
      
      tensor([[ 1.6723,  0.8274,  1.4538],
              [ 0.5076, -1.6918,  0.1204]])
      [[ 1.672333    0.8274111   1.453827  ]
       [ 0.5075982  -1.691759    0.12042859]]
      [[-1.53580661  1.05183357 -0.13190343]
       [ 0.26008687 -0.76392427  0.49671979]]
      tensor([[-1.5358,  1.0518, -0.1319],
              [ 0.2601, -0.7639,  0.4967]], dtype=torch.float64)
      ```
      
      ```python
      # GPU上的张量计算
      if torch.cuda.is_available():
          x=x.cuda()
          y=y.cuda()
          print(x+y)
          
      tensor([[1.1440, 1.7954, 1.8075],
              [1.1011, 1.0410, 1.4783],
              [1.1711, 1.4874, 1.0733],
              [1.8815, 1.3930, 1.4344],
              [1.8639, 1.1563, 1.8560]], device='cuda:0')
      ```
      
   4. 动态计算图

      计算图用于解决用于解决反向传播算法问题。当前馈运算步骤完成之后，深度学习框架就会自动搭建一个计算图，通过这个图，就可以让反向传播算法进行。

      自动微分变量是通过3个重要的属性data（张量）、grad（梯度值）以及grad_fn（获得计算图的上一个节点）来实现的。在采用了自动微分变量以后，无论一个计算过程多么复杂，系统都会自动构造一张计算图来记录所有的运算过程。

      ```python
      # 导入自动微分变量的包
      # from torch.autograd import Variable
      # requires_grad=True是为了保证在反向传播算法中获得梯度信息
      # x=Variable(torch.ones(2,2),requires_grad=True) Variable被废弃
      x=torch.ones(2,2,requires_grad=True)
      x
      tensor([[1., 1.],
              [1., 1.]], requires_grad=True)
      ```

      ```python
      y=x+2
      y
      tensor([[3., 3.],
              [3., 3.]], grad_fn=<AddBackward0>)
      ```

      ```python
      y.data
      tensor([[3., 3.],
              [3., 3.]])
      ```

      ```python
      #返回上一个计算图节点
      y.grad_fn
      <AddBackward0 at 0x1df0dc7f640>
      ```

      ```python
      # *是点乘，只有相同位置的元素相乘
      z=y*y
      z
      tensor([[9., 9.],
              [9., 9.]], grad_fn=<MulBackward0>)
      ```

      ```python
      z.grad_fn
      <MulBackward0 at 0x1df0dc9b280>
      ```

      ```python
      # torch.mean对矩阵的每个元素求和再除以元素的个数。
      t=torch.mean(z)
      t
      tensor(9., grad_fn=<MeanBackward0>)
      ```

      ```python
      # backward()反向梯度传播，自动进行求导计算
      # 只有叶结点才可以通过.backward()获得梯度信息，z和y不是叶结点，所以没有梯度信息
      # retain_graph=True 使得t.backward能运行很多次，没有backward智能运行一次
      t.backward(retain_graph=True)
      print(z.grad)
      print(y.grad)
      print(x.grad)
      None
      None
      tensor([[3., 3.],
              [3., 3.]])
      ```

   5. PyTorch实例：预测房价

      1. 准备数据

         ```python
         # 构造0~50之间均匀数字作为时间变量。
         x=torch.linspace(0,50,steps=50,requires_grad=True).type(torch.float)
         x
         tensor([ 0.0000,  1.0204,  2.0408,  3.0612,  4.0816,  5.1020,  6.1224,  7.1429,
                  8.1633,  9.1837, 10.2041, 11.2245, 12.2449, 13.2653, 14.2857, 15.3061,
                 16.3265, 17.3469, 18.3673, 19.3878, 20.4082, 21.4286, 22.4490, 23.4694,
                 24.4898, 25.5102, 26.5306, 27.5510, 28.5714, 29.5918, 30.6122, 31.6327,
                 32.6531, 33.6735, 34.6939, 35.7143, 36.7347, 37.7551, 38.7755, 39.7959,
                 40.8163, 41.8367, 42.8571, 43.8776, 44.8980, 45.9184, 46.9388, 47.9592,
                 48.9796, 50.0000])
         ```

         ```python
         # 生成均值为0，方差为5的正态分布。
         rand=torch.randn(50).type(torch.float)*5
         # torch.normal(mean=0, std=10,out=50)
         rand
         tensor([-9.5013, -0.2001, -0.2612,  7.3100,  7.8910, -6.8819, -2.4307, -2.4865,
                  5.9178,  4.6767,  2.6463, -4.2847, -1.1919, -7.6735,  6.0805, -4.8176,
                  4.2334, -6.2658,  6.2607,  0.8406, -8.8556,  6.1883, 12.6984,  0.4164,
                  2.1272,  5.9475, -5.7067,  5.4889,  1.4164,  5.9655,  0.4942,  2.8694,
                 -2.0671,  0.4787, -3.0500,  5.3546, -0.5102,  3.3566, -0.3483,  3.2969,
                  2.4644,  0.0407,  5.4788, -7.3005,  0.5329,  3.5430, -3.5318, -4.6006,
                 -8.2579,  1.2810])
         ```

         ```python
         # 生成房价
         y=x+rand
         y
         tensor([-2.5061, -8.4511,  8.3336,  0.6089,  8.8587,  0.4199,  6.9523,  2.9417,
                 11.2004,  5.9490, 15.7981, 16.6783,  5.6091, 13.6873, 15.6817, 13.3907,
                 28.9034, 15.6045, 19.9990, 24.7943, 22.4937, 24.9002, 19.5600, 18.0025,
                 26.3569, 26.2317, 28.3032, 27.7680, 24.7899, 32.1754, 35.6954, 25.2297,
                 27.5482, 36.3927, 26.6795, 37.5358, 27.3953, 38.8195, 35.6925, 36.0569,
                 36.8523, 53.8654, 39.3955, 43.5647, 45.5984, 47.2127, 46.3957, 46.6790,
                 49.8011, 54.2800])
         ```

         生成训练集和测试集

         ```python
         x_train=x[:-5]
         x_test=x[-5:]
         y_train=y[:-5]
         y_test=y[-5:]
         ```

         对训练数据进行可视化

         ```python
         # 对训练数据进行可视化
         import matplotlib.pyplot as plt
         # 设定绘制窗口大小为8*6inch
         plt.figure(figsize=(8,6))
         # 绘制数据，由于x和y都是Variable，需要用data获取它们包裹的Tensor,并转成Numpy
         plt.plot(x_train.data.numpy(),y_train.data.numpy(),'o')
         # 设置X轴标签
         plt.xlabel('X')
         plt.ylabel('Y')
         plt.show()
         ```

         ![](.\2-1.jpg)

      2. 模型设计

         我们希望得到一条尽可能从中间穿越这些数据散点的拟合直线。设这条直线方程为
         $$
         y=ax+b
         $$
         接下来的问题是，求解出参数a,b的数值。我们可以将每一个数据点代入这个方程中，计算出
         $$
   \hat{y_i}=ax_i+b
         $$
         
         显然
         $$
         \hat{y_i}
         $$
         越靠近
         $$
         y_i
         $$
         越好，定义平均损失函数
         $$
         L=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac{1}{N}\sum_{i=1}^N(y_i-ax_i-b)^2
         $$
         
         并让它尽可能的小。由于
         $$
         x_i和y_i
         $$
         都是固定的数，而只有a和b是变量，那么L本质上就是a和b的函数。所以我们要寻找最优的a、b组合，让L最小化。
         
         我们可以利用梯度下降法来反复迭代a和b，从而让L越变越小。
         $$
         a_{t+1}=a_t-\alpha \left. \frac{\partial L}{\partial a}  \right| _{a=a_t}
         $$
         
         $$
         b_{t+1}=b_t-\alpha \left. \frac{\partial L}{\partial b}  \right| _{b=b_t}
         $$
         
         $$
         \alpha为学习率，它可以调节梯度下降快慢，\alpha越大，a、b更新得越快，但是计算得到的最优值L就有可能越不准
         $$
         
         在计算过程中，我们需要计算出L对a、b的偏导数，利用PyTorch的backward（）可以非常方便地将这两个偏导数计算出来。于是，我们只需要一步步地更新a和b的数值就可以了。当达到一定的迭代步数之后，最终a和b的数值就是我们想要的最优数值。
         
      3. 训练
      
         ```python
         # 定义两个自动微分变量a和b(a,b随机)
         # a=Variable(torch.rand(1),requires_grad=True)  Variable已经被废弃
         # b=Variable(torch.rand(1),requires_grad=True)
         a=torch.rand(1,requires_grad=True)
         b=torch.rand(1,requires_grad=True)
         # 设置学习率
         learning_rate=0.0001
         ```
      
         ```python
         # a和b的迭代计算
         for i in range(500):
             predictions=a*x_train+b
             loss=torch.mean((predictions-y_train)**2)
             print('loss:',loss)
             loss.backward()
         #add_    
             a.data.add_(-learning_rate*a.grad)
             b.data.add_(-learning_rate*b.grad)
         # 在更新完a、b的数值后，需要清空a中的的梯度信息，
         # 否则它会在下一步迭代的时候累加
             a.grad.zero_()
             b.grad.zero_()
         #太长，只写了输出了最后一个loss
         loss: tensor(23.5075, grad_fn=<MeanBackward0>)
         print(a.data,b.data)
         tensor([0.9775]) tensor([0.3045])
         ```
      
         将原始的数据散点联合拟合的直线通过图形画出来
      
         ```python
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         # 绘制拟合直线
         plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 坐标标注
         plt.xlabel("X")
         plt.ylabel("Y")
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2],['train_data',str1])
         plt.show()
         ```
         
      
      ![](.\2-2.jpg)
      
      4. 预测
      
         用测试数据集进行测试
      
         ```python
         predictions=a*x_test+b
         predictions
         tensor([45.6905, 46.6988, 47.7070, 48.7153, 49.7235], grad_fn=<AddBackward0>)
      ```
         
         ```python
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         x_pred=x_test.data.numpy()
         y_pred=y_test.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         plt2, =plt.plot(x_pred,y_pred,'s')
         # 绘制拟合直线
         plt3, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 绘制预测数据
         plt4, =plt.plot(x_pred,a.data.numpy()*x_pred+b.data.numpy())
         plt.xlabel('X')
         plt.ylabel('Y')
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2,plt3],['train_data','test_data',str1])
         plt.savefig("2-3.jpg")
         plt.show()
         ```
         
         ![](.\2-3.jpg)
         
         

3. 单车预测器：你的第一个神经网络

   运用PyTorch动手搭建一个共享单车预测器，在实战中掌握神经元、神经元、激活函数、机器学习等基本概念，以及数据预处理的方法。此外，揭秘神经网络这个“黑箱”，看看它如何工作，哪个神经元起到了关键作用。

   ```python
   #导入需要使用的库
   import numpy as np
   import pandas as pd #读取csv文件的库
   import matplotlib.pyplot as plt
   import torch
   #from torch.autograd import Variable
   import torch.optim as optim
   
   # 让输出的图形直接在Notebook中显示
   %matplotlib inline
   data_path="hour.csv"
   rides = pd.read_csv(data_path)
   # head()读取5行数据并显示，查看数据格式
   rides.head()
   ```

   ![](.\3-1.jpg)

   ```python
   #我们取出最后一列的前50条记录来进行预测
   counts = rides['cnt'][:50]
   
   #获得变量x，它是1，2，……，50
   x = np.arange(len(counts))
   
   # 将counts转成预测变量（标签）：y
   y = np.array(counts)
   
   # 绘制一个图形，展示曲线长的样子
   plt.figure(figsize = (8, 6)) #设定绘图窗口大小
   plt.plot(x, y, 'o-') # 绘制原始数据
   plt.xlabel('X') #更改坐标轴标注
   plt.ylabel('Y') #更改坐标轴标注
   plt.savefig("3-2.jpg")
   plt.show()
   ```

   ![](.\3-2.jpg)

   用线性回归对曲线进行拟合，尽管效果很差（回顾上一章节内容）

   ```python
   #我们取出数据库的最后一列的前50条记录来进行预测
   counts = rides['cnt'][:50]
   
   # 创建变量x，它是1，2，……，50
   x = torch.tensor(np.arange(len(counts)), dtype=torch.double, requires_grad = True)
   
   # 将counts转成预测变量（标签）：y
   y = torch.tensor(np.array(counts), dtype=torch.double, requires_grad = True)
   
   a = torch.rand(1, dtype=torch.double, requires_grad = True) #创建a变量，并随机赋值初始化
   b = torch.rand(1, dtype=torch.double, requires_grad = True) #创建b变量，并随机赋值初始化
   print('Initial parameters:', [a, b])
   learning_rate = 0.00001 #设置学习率
   for i in range(10000):
       ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加
       predictions = a * x+ b  #计算在当前a、b条件下的模型预测数值
       loss = torch.mean((predictions - y) ** 2) #通过与标签数据y比较，计算误差
       print('loss:', loss)
       loss.backward() #对损失函数进行梯度反传
       a.data.add_(- learning_rate * a.grad)  #利用上一步计算中得到的a的梯度信息更新a中的data数值
       b.data.add_(- learning_rate * b.grad)  #利用上一步计算中得到的b的梯度信息更新b中的data数值
       a.grad.zero_() #清空a的梯度数值
       b.grad.zero_() #清空b的梯度数值
   ```

   1. 用线性回归进行拟合

      ```python
      x_data=x.data.numpy()
      y_data=y.data.numpy()
      plt.figure(figsize=(10,7))
      #绘制训练集散点
      plt1, =plt.plot(x_data,y_data,'o')
      # 绘制拟合直线
      plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
      # 坐标标注
      plt.xlabel("X")
      plt.ylabel("Y")
      str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
      plt.legend([plt1,plt2],['data',str1])
      plt.savefig("3-3.jpg")
      plt.show()
      ```

      ![](.\3-3.jpg)

   2. 第一个人工神经网络预测器

      输入层1个单元，隐含层1层有10个单元，输出层1个单元的人工神经网络

4. 机器也懂得感情-中文情绪分类器

5. 手写数字识别-认识卷积神经网络-认识卷积神经网络

6. 手写数字加法机-迁移学习

7. 你自己的Prisma-图像风格迁移

8. 人工智能造假术--图像生成与对抗学习

9. 词汇星空--神经语言模型与Word2Vec

10. LSTM作曲机-序列生成模型

11. 神经翻译机--端到端机器翻译

12. AI游戏高手--深度强化学习

