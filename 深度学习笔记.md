1. 深度学习简介

   人工神经网络称为人工智能的连接学派

   1. 感知机到人工神经网络

      + 1957年 弗兰克提出感知机模型（不是直接从功能的角度而是通过结构模拟）
      + 1969年 提出感知机模型连XOR问题无法解出，人工神经网络被打入网络
      + 辛顿发展了人工神经网络的反向传播法，从而可以构造两层以上神经网络，并且可以有效进行学习与训练，两层以上神经网络可以很轻松解决XOR问题。
      + 受限于当时的计算能力，另一方面缺乏大规模高质量的数据，神经网络本身就是一个黑箱，谁也不敢保证深度神经网络在深度这个方向上就能够取得更好的结果和精度。人工神经网络并没有继续沿着深度的方向发展下去。
      + 学术界的焦点朝向了另一个发展方向：寻找神经网络的基础理论。在弗拉基米尔和亚历克塞推进下，统计学习理论蓬勃发展，奠定了模式识别的数学基础，创造出了支持向量机这种极其实用简单的工具。与传统神经网络通过加深网络来提升精度相反，支持向量机的解决方案是将数据的维度提升，在高维种寻找能够将数据进行准确划分的方法，这种方法在数据量不是很大情况下很奏效。支持向量机称为20世纪90年代到21世纪初的宠儿。
      + 2006年 辛顿发表题为《利用神经网络进行数据的维度约减》的文章，提出了深度神经网络模型（DNN），并指出如果我们能够将神经网络的层数加深，并且精心设计训练网络的方式，那么这样深层次的神经网络就会具有超强的表达能力和学习能力。

   2. 深度学习时代

      + 深度学习首先在语音领域取得突破。微软的邓力邀请辛顿加入语音识别的深度神经网络模型的开发，使识别的准确度有了大幅的提升。
      + 机器视觉专家李飞飞，借助网友的力量构造出了ImageNet这样一个大规模、高精度、多标签的图像数据库。于是李飞飞开始每年举办一次图像识别大赛：ImageNet竞赛。2012年，辛顿和他的两个学生采用深层次的卷积神经网络（AlexNet）,在ImageNet竞赛中将分类错误率从25%降到了17%。将深度网络做到8层，且不需要任何预处理就能将图像分类任务做到这么好，还是头一次。深度神经网络称为了ImageNet竞赛的标配，从AlexNet到GoogleNet，网络深度不断增加，准确率不断提升。2012年后深度学习开始在学术圈流行起来。

   3. 深度学习发展

      + 2011年 谷歌X实验室的杰夫和吴恩达等人采用深度学习技术，让谷歌大脑深度神经网络观看了从Youtube上提取出来的30万张图像，并让机器自动进行精简，谷歌大脑自己学出了一张猫脸。看到发展前景后，以谷歌未代表的各大公司开始疯狂并购人工智能、人工智能初创公司和团队，促使更多的人才和创业公司投入到人工智能大潮中。

      + 自然语言处理领域，2013年，谷歌的托马斯提出了Word2Vec技术，它可以非常快捷有效地计算单词间的向量表示，为大规模使用人工神经网络技术处理人类语言奠定了重要基础

      + 2014年，谷歌开始尝试利用深度的神经循环网络来处理各种自然语言任务，包括机器翻译，自动对话，情绪识别、阅读理解等。2016年，谷歌机器翻译技术取得重大突破，采用了先进的深度循环神经网络和注意力机智的机器翻译在多种语言上已经基本接近人类水平。

      + 强化学习与深度学习的结合，深度学习在计算机游戏、博弈等领域同样取得了重大进展。

        2015年被谷歌收购的DeepMind团队研发了一种“通用人工智能算法”，他可以像人类一样，通过观察计算机游戏屏幕进行自我学习，利用同一套网络架构和超参数，从零开始学习每一款游戏，并最终打通了300多款雅达利游戏，在某些游戏上的表现甚至超越了人类。

      + 2016年 DeepMind团队又在博弈领域取得了重大突破。AlphaGo以4:1战胜人类围棋冠军。2017年 DeepMind团队创造的 AlphaGo 升级版 AlphaGo Zero ，它可以完全从零开始学习下围棋，而无需借鉴任何人类的下棋经验。仅经过大约3天训练，AlphaGo Zero就达到了战胜李世石的棋力水平。而到了21天后，世界上已经没有任何人类或程序可以在围棋上战胜它了。AlphaGo 的成功不仅标志着以深度学习技术为支撑的新一代人工智能技术大获全胜，更暗示着人工智能全新时代的来临。  

   4. 深度学习的影响因素

      1. 大数据

         如果没有足够大量的数据输入给深度神经网络，就无法发挥深度的作用。伴随着网络深度的增加，待拟合的参数自然会也会增加，如果没有与其相匹配的海量数据来训练网络，这些参数就完全变成了导致网络过拟合的垃圾，无法发挥作用

      2. 深度网络架构（整个网络体系的构建方式和拓扑连接结构）

         面对具体问题时，应该采用什么样的网络架构，如何选取参数，如何训练这个网络，仍然是影响学习效率和解决问题的重要因素。

         目前主要分为： 

         + 前馈神经网络  

           每一层的节点只跟它相邻层节点而且是全部节点相连（全连接的），分为输入层，隐藏层，输出层

         + 卷积神经网络

           卷积层和池化层

           图中每一个立方体都是一系列规则排列的人工神经元的集合。

           每个神经元到上一层次的连接称为卷积核，它们都是一种局域的小窗口。

           图中的小锥形可以理解为从高层的某一个神经元到低层多个神经元之间的连接。这个小锥形在立方体上逐像素的平移就构成了两层次之间的所有连接。到了最后两层，小立方体被压缩成了一个一维的向量，这就与普通的前馈神经网络没有区别。

           CNN这种特殊的架构可以很好地应用于图像处理，它可以使原始的图像即使在经历过平移、缩放等变换后仍然具有很高的识别准确性。正是因为具有这样特殊的架构，CNN才成功应用于计算机视觉、图像识别、图像生成，甚至AI下围棋、AI打游戏等广阔领域

         + 循环神经网络
      
           输入层，输出层是单层，中间的隐藏层的节点相互连接。隐藏层彼此之间还有大量的连接。
      
           RNN这种特殊的架构使得网络当前的运行不仅跟当前的输入数据有关，而且还与之前的数据有关。因此，这种网络特别适合处理诸如语言、音乐、股票曲线等序列类型的数据，整个网络的循环结构可以很好地应付输入序列之中存在的长程记忆性和周期性
      
         + 训练方式
      
           训练方式也会对结果产生很大影响。如果先将少量特定标签的数据输入网络，然后再拿剩下的数据去训练它，就会比一股脑把所有便签数据都输入更加有效，从而提高网络的“学习”能力。
      
      3. GPU 
      
         GPU非常擅长大规模的张量（高阶矩阵）运算，并且可以为这种运算加速，对包含多个数值的张量运算所需的平均时间远低于对每个数字的运算时间
      
   5. 深度学习的成功

      深度学习重要的本领在于它可以从海量的数据中自动学习，抽取数据中的特征。

      + 特征学习

        深度神经网络会把不同的信息表达到不同层次的网络单元中，这一提炼过程完全不需要手工干预，全凭机器学习过程自动完成。深度学习的本质就是这种自动提取特征的功能。

      + 迁移学习

        把一个训练好的神经网络切开，然后再把它拼接到另一个神经网络上，前半部分用于特征提取，后半部分网络去解决另一个完全不同的问题。

2. PyTorch简介

   1. PyTorch安装

   2. 与Python完美融合

      使用Pytorch与使用其他Python程序包没有任何区别
      
      与此形成鲜明对比的是TensorFlow，TensorFlow会将一个深度学习任务分为定义为计算图和执行计算过程，而定义计算图的过程就好像在使用一套全新的语言。PyTorch就没有这个缺点，从定义计算图到执行计算是一气呵成的。
      
   3. 张量（tensor）计算

      PyTorch的运算单元叫做张量，1阶张量即为1维数组（向量vector），2阶张量为2维数组（矩阵matrix），3阶张量即为3维数组

      ```python
      import torch
      x=torch.rand(5,3)
      x
      tensor([[0.1440, 0.7954, 0.8075],
              [0.1011, 0.0410, 0.4783],
              [0.1711, 0.4874, 0.0733],
              [0.8815, 0.3930, 0.4344],
              [0.8639, 0.1563, 0.8560]])
      
      ```
      
      ```python
      y=torch.ones(5,3)
      y
      tensor([[1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.]])
      ```
      
      ```python
      # pytorh矩阵叉乘法 mm() ,转置t()
      q=x.mm(y.t())
      q
      tensor([[1.7469, 1.7469, 1.7469, 1.7469, 1.7469],
              [0.6204, 0.6204, 0.6204, 0.6204, 0.6204],
              [0.7319, 0.7319, 0.7319, 0.7319, 0.7319],
              [1.7089, 1.7089, 1.7089, 1.7089, 1.7089],
              [1.8762, 1.8762, 1.8762, 1.8762, 1.8762]])
      ```
      
      ```python
      # pytorh张量与Numpy数组之间的转换
      import numpy as np
      x_tensor=torch.randn(2,3)
      y_numpy=np.random.randn(2,3)
      # 将张量转换为numpy
      x_numpy=x_tensor.numpy()
      # 将numpy转化为张量
      y_tensor=torch.from_numpy(y_numpy)
      print(x_tensor)
      print(x_numpy)
      print(y_numpy)
      print(y_tensor)
      
      tensor([[ 1.6723,  0.8274,  1.4538],
              [ 0.5076, -1.6918,  0.1204]])
      [[ 1.672333    0.8274111   1.453827  ]
       [ 0.5075982  -1.691759    0.12042859]]
      [[-1.53580661  1.05183357 -0.13190343]
       [ 0.26008687 -0.76392427  0.49671979]]
      tensor([[-1.5358,  1.0518, -0.1319],
              [ 0.2601, -0.7639,  0.4967]], dtype=torch.float64)
      ```
      
      ```python
      # GPU上的张量计算
      if torch.cuda.is_available():
          x=x.cuda()
          y=y.cuda()
          print(x+y)
          
      tensor([[1.1440, 1.7954, 1.8075],
              [1.1011, 1.0410, 1.4783],
              [1.1711, 1.4874, 1.0733],
              [1.8815, 1.3930, 1.4344],
              [1.8639, 1.1563, 1.8560]], device='cuda:0')
      ```
      
   4. 动态计算图

      计算图用于解决用于解决反向传播算法问题。当前馈运算步骤完成之后，深度学习框架就会自动搭建一个计算图，通过这个图，就可以让反向传播算法进行。

      自动微分变量是通过3个重要的属性data（张量）、grad（梯度值）以及grad_fn（获得计算图的上一个节点）来实现的。在采用了自动微分变量以后，无论一个计算过程多么复杂，系统都会自动构造一张计算图来记录所有的运算过程。

      ```python
      # 导入自动微分变量的包
      # from torch.autograd import Variable
      # requires_grad=True是为了保证在反向传播算法中获得梯度信息
      # x=Variable(torch.ones(2,2),requires_grad=True) Variable被废弃
      x=torch.ones(2,2,requires_grad=True)
      x
      tensor([[1., 1.],
              [1., 1.]], requires_grad=True)
      ```

      ```python
      y=x+2
      y
      tensor([[3., 3.],
              [3., 3.]], grad_fn=<AddBackward0>)
      ```

      ```python
      y.data
      tensor([[3., 3.],
              [3., 3.]])
      ```

      ```python
      #返回上一个计算图节点
      y.grad_fn
      <AddBackward0 at 0x1df0dc7f640>
      ```

      ```python
      # *是点乘，只有相同位置的元素相乘
      z=y*y
      z
      tensor([[9., 9.],
              [9., 9.]], grad_fn=<MulBackward0>)
      ```

      ```python
      z.grad_fn
      <MulBackward0 at 0x1df0dc9b280>
      ```

      ```python
      # torch.mean对矩阵的每个元素求和再除以元素的个数。
      t=torch.mean(z)
      t
      tensor(9., grad_fn=<MeanBackward0>)
      ```

      ```python
      # backward()反向梯度传播，自动进行求导计算
      # 只有叶结点才可以通过.backward()获得梯度信息，z和y不是叶结点，所以没有梯度信息
      # retain_graph=True 使得t.backward能运行很多次，没有backward智能运行一次
      t.backward(retain_graph=True)
      print(z.grad)
      print(y.grad)
      print(x.grad)
      None
      None
      tensor([[3., 3.],
              [3., 3.]])
      ```

   5. PyTorch实例：预测房价

      1. 准备数据

         ```python
         # 构造0~50之间均匀数字作为时间变量。
         x=torch.linspace(0,50,steps=50,requires_grad=True).type(torch.float)
         x
         tensor([ 0.0000,  1.0204,  2.0408,  3.0612,  4.0816,  5.1020,  6.1224,  7.1429,
                  8.1633,  9.1837, 10.2041, 11.2245, 12.2449, 13.2653, 14.2857, 15.3061,
                 16.3265, 17.3469, 18.3673, 19.3878, 20.4082, 21.4286, 22.4490, 23.4694,
                 24.4898, 25.5102, 26.5306, 27.5510, 28.5714, 29.5918, 30.6122, 31.6327,
                 32.6531, 33.6735, 34.6939, 35.7143, 36.7347, 37.7551, 38.7755, 39.7959,
                 40.8163, 41.8367, 42.8571, 43.8776, 44.8980, 45.9184, 46.9388, 47.9592,
                 48.9796, 50.0000])
         ```

         ```python
         # 生成均值为0，方差为5的正态分布。
         rand=torch.randn(50).type(torch.float)*5
         # torch.normal(mean=0, std=10,out=50)
         rand
         tensor([-9.5013, -0.2001, -0.2612,  7.3100,  7.8910, -6.8819, -2.4307, -2.4865,
                  5.9178,  4.6767,  2.6463, -4.2847, -1.1919, -7.6735,  6.0805, -4.8176,
                  4.2334, -6.2658,  6.2607,  0.8406, -8.8556,  6.1883, 12.6984,  0.4164,
                  2.1272,  5.9475, -5.7067,  5.4889,  1.4164,  5.9655,  0.4942,  2.8694,
                 -2.0671,  0.4787, -3.0500,  5.3546, -0.5102,  3.3566, -0.3483,  3.2969,
                  2.4644,  0.0407,  5.4788, -7.3005,  0.5329,  3.5430, -3.5318, -4.6006,
                 -8.2579,  1.2810])
         ```

         ```python
         # 生成房价
         y=x+rand
         y
         tensor([-2.5061, -8.4511,  8.3336,  0.6089,  8.8587,  0.4199,  6.9523,  2.9417,
                 11.2004,  5.9490, 15.7981, 16.6783,  5.6091, 13.6873, 15.6817, 13.3907,
                 28.9034, 15.6045, 19.9990, 24.7943, 22.4937, 24.9002, 19.5600, 18.0025,
                 26.3569, 26.2317, 28.3032, 27.7680, 24.7899, 32.1754, 35.6954, 25.2297,
                 27.5482, 36.3927, 26.6795, 37.5358, 27.3953, 38.8195, 35.6925, 36.0569,
                 36.8523, 53.8654, 39.3955, 43.5647, 45.5984, 47.2127, 46.3957, 46.6790,
                 49.8011, 54.2800])
         ```

         生成训练集和测试集

         ```python
         x_train=x[:-5]
         x_test=x[-5:]
         y_train=y[:-5]
         y_test=y[-5:]
         ```

         对训练数据进行可视化

         ```python
         # 对训练数据进行可视化
         import matplotlib.pyplot as plt
         # 设定绘制窗口大小为8*6inch
         plt.figure(figsize=(8,6))
         # 绘制数据，由于x和y都是Variable，需要用data获取它们包裹的Tensor,并转成Numpy
         plt.plot(x_train.data.numpy(),y_train.data.numpy(),'o')
         # 设置X轴标签
         plt.xlabel('X')
         plt.ylabel('Y')
         plt.show()
         ```

         ![](.\2-1.jpg)

      2. 模型设计

         我们希望得到一条尽可能从中间穿越这些数据散点的拟合直线。设这条直线方程为
         $$
         y=ax+b
         $$
         接下来的问题是，求解出参数a,b的数值。我们可以将每一个数据点代入这个方程中，计算出
         $$
   \hat{y_i}=ax_i+b
         $$
         
         显然
         $$
         \hat{y_i}
         $$
         越靠近
         $$
         y_i
         $$
         越好，定义平均损失函数
         $$
         L=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac{1}{N}\sum_{i=1}^N(y_i-ax_i-b)^2
         $$
         
         并让它尽可能的小。由于
         $$
         x_i和y_i
         $$
         都是固定的数，而只有a和b是变量，那么L本质上就是a和b的函数。所以我们要寻找最优的a、b组合，让L最小化。
         
         我们可以利用梯度下降法来反复迭代a和b，从而让L越变越小。
         $$
         a_{t+1}=a_t-\alpha \left. \frac{\partial L}{\partial a}  \right| _{a=a_t}
         $$
         
         $$
         b_{t+1}=b_t-\alpha \left. \frac{\partial L}{\partial b}  \right| _{b=b_t}
         $$
         
         $$
         \alpha为学习率，它可以调节梯度下降快慢，\alpha越大，a、b更新得越快，但是计算得到的最优值L就有可能越不准
         $$
         
         在计算过程中，我们需要计算出L对a、b的偏导数，利用PyTorch的backward（）可以非常方便地将这两个偏导数计算出来。于是，我们只需要一步步地更新a和b的数值就可以了。当达到一定的迭代步数之后，最终a和b的数值就是我们想要的最优数值。
         
      3. 训练
      
         ```python
         # 定义两个自动微分变量a和b(a,b随机)
         # a=Variable(torch.rand(1),requires_grad=True)  Variable已经被废弃
         # b=Variable(torch.rand(1),requires_grad=True)
         a=torch.rand(1,requires_grad=True)
         b=torch.rand(1,requires_grad=True)
         # 设置学习率
         learning_rate=0.0001
         ```
      
         ```python
         # a和b的迭代计算
         for i in range(500):
             predictions=a*x_train+b
             loss=torch.mean((predictions-y_train)**2)
             print('loss:',loss)
             loss.backward()
         #add_    
             a.data.add_(-learning_rate*a.grad)
             b.data.add_(-learning_rate*b.grad)
         # 在更新完a、b的数值后，需要清空a中的的梯度信息，
         # 否则它会在下一步迭代的时候累加
             a.grad.zero_()
             b.grad.zero_()
         #太长，只写了输出了最后一个loss
         loss: tensor(23.5075, grad_fn=<MeanBackward0>)
         print(a.data,b.data)
         tensor([0.9775]) tensor([0.3045])
         ```
      
         将原始的数据散点联合拟合的直线通过图形画出来
      
         ```python
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         # 绘制拟合直线
         plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 坐标标注
         plt.xlabel("X")
         plt.ylabel("Y")
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2],['train_data',str1])
         plt.show()
         ```
         
      
      ![](.\2-2.jpg)
      
      4. 预测
      
         用测试数据集进行测试
      
         ```python
         predictions=a*x_test+b
         predictions
         tensor([45.6905, 46.6988, 47.7070, 48.7153, 49.7235], grad_fn=<AddBackward0>)
      
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         x_pred=x_test.data.numpy()
         y_pred=y_test.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         plt2, =plt.plot(x_pred,y_pred,'s')
         # 绘制拟合直线
         plt3, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 绘制预测数据
         plt4, =plt.plot(x_pred,a.data.numpy()*x_pred+b.data.numpy())
         plt.xlabel('X')
         plt.ylabel('Y')
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2,plt3],['train_data','test_data',str1])
         plt.savefig("2-3.jpg")
         plt.show()
         ```
         
         ![](.\2-3.jpg)
         
         

3. 单车预测器：你的第一个神经网络

   运用PyTorch动手搭建一个共享单车预测器，在实战中掌握神经元、神经元、激活函数、机器学习等基本概念，以及数据预处理的方法。此外，揭秘神经网络这个“黑箱”，看看它如何工作，哪个神经元起到了关键作用。

   ```python
   #导入需要使用的库
   import numpy as np
   import pandas as pd #读取csv文件的库
   import matplotlib.pyplot as plt
   import torch
   #from torch.autograd import Variable
   import torch.optim as optim
   
   # 让输出的图形直接在Notebook中显示
   %matplotlib inline
   data_path="hour.csv"
   rides = pd.read_csv(data_path)
   # head()读取5行数据并显示，查看数据格式
   rides.head()
   ```

   ![](.\3-1.jpg)

   ```python
   #我们取出最后一列的前50条记录来进行预测
   counts = rides['cnt'][:50]
   
   #获得变量x，它是1，2，……，50
   x = np.arange(len(counts))
   
   # 将counts转成预测变量（标签）：y
   y = np.array(counts)
   
   # 绘制一个图形，展示曲线长的样子
   plt.figure(figsize = (8, 6)) #设定绘图窗口大小
   plt.plot(x, y, 'o-') # 绘制原始数据
   plt.xlabel('X') #更改坐标轴标注
   plt.ylabel('Y') #更改坐标轴标注
   plt.savefig("3-2.jpg")
   plt.show()
   ```

   ![](.\3-2.jpg)

   1. 用线性回归对曲线进行拟合，尽管效果很差（回顾上一章节内容）

      ```python
      #我们取出数据库的最后一列的前50条记录来进行预测
      counts = rides['cnt'][:50]
      
      # 创建变量x，它是1，2，……，50
      x = torch.tensor(np.arange(len(counts)), dtype=torch.double, requires_grad = True)
      
      # 将counts转成预测变量（标签）：y
      y = torch.tensor(np.array(counts), dtype=torch.double, requires_grad = True)
      
      a = torch.rand(1, dtype=torch.double, requires_grad = True) #创建a变量，并随机赋值初始化
      b = torch.rand(1, dtype=torch.double, requires_grad = True) #创建b变量，并随机赋值初始化
      print('Initial parameters:', [a, b])
      learning_rate = 0.00001 #设置学习率
      for i in range(10000):
          ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加
          predictions = a * x+ b  #计算在当前a、b条件下的模型预测数值
          loss = torch.mean((predictions - y) ** 2) #通过与标签数据y比较，计算误差
          print('loss:', loss)
          loss.backward() #对损失函数进行梯度反传
          a.data.add_(- learning_rate * a.grad)  #利用上一步计算中得到的a的梯度信息更新a中的data数值
          b.data.add_(- learning_rate * b.grad)  #利用上一步计算中得到的b的梯度信息更新b中的data数值
          a.grad.zero_() #清空a的梯度数值
          b.grad.zero_() #清空b的梯度数值
      ```

      用线性回归进行拟合

      ```python
      x_data=x.data.numpy()
      y_data=y.data.numpy()
      plt.figure(figsize=(10,7))
      #绘制训练集散点
      plt1, =plt.plot(x_data,y_data,'o')
      # 绘制拟合直线
      plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
      # 坐标标注
      plt.xlabel("X")
      plt.ylabel("Y")
      str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
      plt.legend([plt1,plt2],['data',str1])
      plt.savefig("3-3.jpg")
      plt.show()
      ```

      ![](.\3-3.jpg)

   2. 第一个人工神经网络预测器（单车预测器1.0）

      输入层1个单元，隐含层1层有10个单元，输出层1个单元的人工神经网络
      
      ```python
      #取出数据库中的最后一列的前50条记录来进行预测
      counts = rides['cnt'][:50]
      
      #创建变量x，它是1，2，……，50
      x = torch.tensor(np.arange(len(counts), dtype = float), requires_grad = True)
      
      # 将counts转成预测变量（标签）：y
      y = torch.tensor(np.array(counts, dtype = float), requires_grad = True)
      
      # 设置隐含层神经元的数量
      sz = 10
      
      # 初始化所有神经网络的权重（weights）和阈值（biases）
      weights = torch.randn((1, sz), dtype = torch.double, requires_grad = True) #1*10的输入到隐含层的权重矩阵
      biases = torch.randn(sz, dtype = torch.double, requires_grad = True) #尺度为10的隐含层节点偏置向量
      weights2 = torch.randn((sz, 1), dtype = torch.double, requires_grad = True) #10*1的隐含到输出层权重矩阵
      
      learning_rate = 0.001 #设置学习率
      losses = []
      
      # 将 x 转换为(50,1)的维度，以便与维度为(1,10)的weights矩阵相乘
      x = x.view(50, -1)
      # 将 y 转换为(50,1)的维度
      y = y.view(50, -1)
      
      for i in range(100000):
          # 从输入层到隐含层的计算
          hidden = x * weights + biases
          # 将sigmoid函数作用在隐含层的每一个神经元上
          hidden = torch.sigmoid(hidden)
          #print(hidden.size())
          # 隐含层输出到输出层，计算得到最终预测
          predictions = hidden.mm(weights2)
          #print(predictions.size())
          # 通过与标签数据y比较，计算均方误差
          loss = torch.mean((predictions - y) ** 2) 
          #print(loss.size())
          losses.append(loss.data.numpy())
          
          # 每隔10000个周期打印一下损失函数数值
          if i % 10000 == 0:
              print('loss:', loss)
              
          #对损失函数进行梯度反传
          loss.backward()
          
          #利用上一步计算中得到的weights，biases等梯度信息更新weights或biases中的data数值
          weights.data.add_(- learning_rate * weights.grad)  
          biases.data.add_(- learning_rate * biases.grad)
          weights2.data.add_(- learning_rate * weights2.grad)
          
          # 清空所有变量的梯度值。
          # 因为pytorch中backward一次梯度信息会自动累加到各个变量上，因此需要清空，否则下一次迭代会累加，造成很大的偏差
          weights.grad.zero_()
          biases.grad.zero_()
          weights2.grad.zero_()
          
      loss: tensor(2256.8876, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(738.2561, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(561.8528, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(507.0011, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(473.5310, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(466.2351, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(462.0210, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(459.2573, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(457.6207, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(456.6714, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # 打印误差曲线
      plt.plot(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.show()
      ```
      
      ![](.\3-4.jpg)
      
      
      
      绘制深度学习拟合曲线
      
      ```python
      x_data = x.data.numpy() # 获得x包裹的数据
      plt.figure(figsize = (10, 7)) #设定绘图窗口大小
      xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据
      
      yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据
      plt.xlabel('X') #更改坐标轴标注
      plt.ylabel('Y') #更改坐标轴标注
      plt.legend([xplot, yplot],['Data', 'Prediction under 1000000 epochs']) #绘制图例
      plt.savefig("3-5.jpg")
      plt.show()
      ```
      
      ![](.\3-5.jpg)
      
      上面的程序之所以跑得很慢，是因为x的取值范围1～50。 而由于所有权重和biases的取值范围被设定为-1,1的正态分布随机数，这样就导致 我们输入给隐含层节点的数值范围为-50~50， 要想将sigmoid函数的多个峰值调节到我们期望的位置需要耗费很多的计算时间。
      
      我们的解决方案就是将输入变量的范围归一化(只需要改变输入变量x)
      
      ```python
      #创建归一化的变量x，它的取值是0.02,0.04,...,1
      x = torch.tensor(np.arange(len(counts), dtype = float) / len(counts), requires_grad = True)
      
      #输出损失函数结果
      loss: tensor(2165.3436, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(940.2517, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(689.8824, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(471.5665, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(231.3214, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(123.2615, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(74.8383, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(56.6815, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(48.7930, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(44.8804, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # 打印误差曲线
      # semilogy()对y轴按对数进行缩放
      plt.semilogy(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.savefig("3-6.jpg")
      plt.show()
      ```
      
      ![](.\3-6.jpg)
      
      绘制拟合曲线（代码同上）
      
      ![](.\3-7.jpg)
      
      对接下来的50个数据进行预测
      
      ```python
      counts_predict = rides['cnt'][50:100] #读取待预测的接下来的50个数据点
      
      #首先对接下来的50个数据点进行选取，注意x应该取51，52，……，100，然后再归一化
      x = torch.tensor((np.arange(len(counts_predict), dtype = float) + len(counts) )/ len(counts_predict)
                       , requires_grad = True)
      y=torch.tensor(np.array(counts_predict),dtype=torch.double,requires_grad=True)
      x=x.view(len(x),-1)
      hidden=x*weights+biases
      # hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)
      hidden=torch.sigmoid(hidden)
      predictions=hidden.mm(weights2)
      loss=torch.mean((y-predictions)**2)
      
      x_data=x.data.numpy()
      plt.figure(figsize=(10,7))
      xplot, =plt.plot(x_data,y.data.numpy(),'o')
      yplot, =plt.plot(x_data,predictions.data.numpy())
      plt.xlabel("X")
      plt.ylabel("y")
      plt.savefig("3-8.jpg")
      plt.show()
      ```
      
      ![](.\3-8.jpg)
      
      预测发现存在着非常严重的过拟合现象，原因是x和y根本没有关系，即单车使用数量不依赖于下标。
      
   3. 人工神经网络Neu（单车预测器2.0）

      1. 数据预处理

         对数值变量（连续）进行标准化处理

         由于我们利用了全部数据来训练神经网络，所以采用之前介绍的一次性在全部数据上训练网络的方法就会很慢， 所以我们将数据划分成了不同的撮（batch），一个批次一个批次地训练神经网络，因此我们还要对数据进行划分。

         对于类型变量（离散）处理，转换成独热编码

         有很多变量都属于类型变量，例如season=1,2,3,4，分四季。我们不能将season变量直接输入到神经网络，这是因为season数值越高并不表示相应的信号强度越大。我们的解决方案是将类型变量用一个“一位热码“（one-hot）来编码，也就是：

         𝑠𝑒𝑎𝑠𝑜𝑛=1→(1,0,0,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=2→(0,1,0,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=3→(0,0,1,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=4→(0,0,0,1)

         因此，如果一个类型变量有n个不同取值，那么我们的“一位热码“所对应的向量长度就为n

         ```python
         #对于类型变量的特殊处理
         # season=1,2,3,4, weathersi=1,2,3, mnth= 1,2,...,12, hr=0,1, ...,23, weekday=0,1,...,6
         # 经过下面的处理后，将会多出若干特征，例如，对于season变量就会有 season_1, season_2, season_3, season_4
         # 这四种不同的特征。
         dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
         for each in dummy_fields:
             #利用pandas对象，我们可以很方便地将一个类型变量属性进行one-hot编码，变成多个属性
             dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)
             rides = pd.concat([rides, dummies], axis=1)
         
         # 把原有的类型变量对应的特征去掉，将一些不相关的特征去掉
         fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                           'weekday', 'atemp', 'mnth', 'workingday', 'hr']
         data = rides.drop(fields_to_drop, axis=1)
         data.head()
         ```
         
         对数值数值类型变量进行标准化
         
         ```python
         # 调整所有的特征，标准化处理
         quant_features = ['cnt', 'temp', 'hum', 'windspeed']
         #quant_features = ['temp', 'hum', 'windspeed']
         
         # 我们将每一个变量的均值和方差都存储到scaled_features变量中。
         scaled_features = {}
         for each in quant_features:
             mean, std = data[each].mean(), data[each].std()
             scaled_features[each] = [mean, std]
             data.loc[:, each] = (data[each] - mean)/std
         ```
         
         将数据集进行分割，分割成测试集和训练集
         
         ```python
         # 将所有的数据集分为测试集和训练集，我们以后21天数据一共21*24个数据点作为测试集，其它是训练集
         test_data = data[-21*24:]
         train_data = data[:-21*24]
         print('训练数据：',len(train_data),'测试数据：',len(test_data))
         
         # 将我们的数据列分为特征列和目标列
         
         #目标列
         target_fields = ['cnt', 'casual', 'registered']
         features, targets = train_data.drop(target_fields, axis=1), train_data[target_fields]
         test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]
         
         # 将数据从pandas dataframe转换为numpy
         X = features.values
         Y = targets['cnt'].values
         Y = Y.astype(float)
         Y = Y.reshape([-1])
         
         训练数据： 16875 测试数据： 504
         ```
         
      2. 构建神经网络并进行训练
      
         1. 手动编写用Tensor运算的人工神经网络
      
            ```python
            # 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层
            input_size = features.shape[1] #输入层单元个数
            hidden_size = 10 #隐含层单元个数
            output_size = 1 #输出层单元个数
            batch_size = 128 #每隔batch的记录数
            weights1 = torch.randn([input_size, hidden_size], dtype = torch.double,  requires_grad = True) #第一到二层权重
            biases1 = torch.randn([hidden_size], dtype = torch.double, requires_grad = True) #隐含层偏置
            weights2 = torch.randn([hidden_size, output_size], dtype = torch.double, requires_grad = True) #隐含层到输出层权重
            def neu(x):
                #计算隐含层输出
                #x为batch_size * input_size的矩阵，weights1为input_size*hidden_size矩阵，
                #biases为hidden_size向量，输出为batch_size * hidden_size矩阵    
                hidden = x.mm(weights1) + biases1.expand(x.size()[0], hidden_size)
                hidden = torch.sigmoid(hidden)
                
                #输入batch_size * hidden_size矩阵，mm上weights2, hidden_size*output_size矩阵，
                #输出batch_size*output_size矩阵
                output = hidden.mm(weights2)
                return output
            def cost(x, y):
                # 计算损失函数
                error = torch.mean((x - y)**2)
                return error
            def zero_grad():
                # 清空每个参数的梯度信息
                if weights1.grad is not None and biases1.grad is not None and weights2.grad is not None:
                    weights1.grad.data.zero_()
                    weights2.grad.data.zero_()
                    biases1.grad.data.zero_()
            def optimizer_step(learning_rate):
                # 梯度下降算法
                weights1.data.add_(- learning_rate * weights1.grad.data)
                weights2.data.add_(- learning_rate * weights2.grad.data)
                biases1.data.add_(- learning_rate * biases1.grad.data)
            ```
      
            ```python
            # 神经网络训练循环
            losses = []
            for i in range(1000):
                # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取
                batch_loss = []
                # start和end分别是提取一个batch数据的起始和终止下标
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.double, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.double, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    zero_grad()
                    loss.backward()
                    optimizer_step(0.01)
                    batch_loss.append(loss.data.numpy())
                
                # 每隔100步输出一下损失值（loss）
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            ```python
            # 打印输出损失值
            fig = plt.figure(figsize=(10, 7))
            plt.plot(np.arange(len(losses))*100,losses, 'o-')
            plt.xlabel('epoch')
            plt.ylabel('MSE')
            plt.savefig("3-9.jpg")
            ```
      
            ![](.\3-9.jpg)
      
         2. 调用PyTorch现成的函数，构建序列化的神经网络
      
            ```python
            # 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层
            input_size = features.shape[1]
            hidden_size = 10
            output_size = 1
            batch_size = 128
            neu = torch.nn.Sequential(
                torch.nn.Linear(input_size, hidden_size),
                torch.nn.Sigmoid(),
                torch.nn.Linear(hidden_size, output_size),
            )
            cost = torch.nn.MSELoss()
            # # pytorch自己会准备参数，直接使用neu.parameters就行
            optimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)
            
            ```
      
            ```python
            # 神经网络训练循环
            losses = []
            for i in range(1000):
                # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取
                batch_loss = []
                # start和end分别是提取一个batch数据的起始和终止下标
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.float, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    batch_loss.append(loss.data.numpy())
                
                # 每隔100步输出一下损失值（loss）
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            调用PyTorch现成函数，收敛更快
         
         3. 测试神经网络
         
            ```python
            # 用训练好的神经网络在测试集上进行预测
            targets = test_targets['cnt'] #读取测试集的cnt数值
            targets = targets.values.reshape([len(targets),1]) #将数据转换成合适的tensor形式
            targets = targets.astype(float) #保证数据为实数
            
            # 将属性和预测变量包裹在Variable型变量中
            x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)
            y = torch.tensor(targets, dtype = torch.float, requires_grad = True)
            
            # 用神经网络进行预测
            predict = neu(x)
            predict = predict.data.numpy()
            # 将后21天的预测数据与真实数据画在一起并比较
            # 横坐标轴是不同的日期，纵坐标轴是预测或者真实数据的值
            fig, ax = plt.subplots(figsize = (10, 7))
            
            mean, std = scaled_features['cnt']
            ax.plot(predict * std + mean, label='Prediction', linestyle = '--')
            ax.plot(targets * std + mean, label='Data', linestyle = '-')
            ax.legend()
            ax.set_xlabel('Date-time')
            ax.set_ylabel('Counts')
            # 对横坐标轴进行标注
            dates = pd.to_datetime(rides.loc[test_data.index]['dteday'])
            # lambda d: d.strftime() d是参数，d.strftime()是返回的结果
            #%b 本地简化的月份名称 %d  月内中的一天（0-31）
            dates = dates.apply(lambda d: d.strftime('%b %d'))
            #set_xticks设置横轴标记
            #set_xticklabels设置横轴标签
            ax.set_xticks(np.arange(len(dates))[12::24])
            ax.set_xticklabels(dates[12::24], rotation=45)
            ```
         
            ![](.\3-11.jpg)
         
         可以看到，两个曲线基本是吻合的 ，但是在12月25日前后几天的实际值和预测值偏差较大。仔细观察数据，我们发现12月25日正好是圣诞节，人们的出行习惯会与往日有很大的不同。在我们的训练样本中，因为整个数据仅有两年长度，所以包含圣诞节前后的样本仅有一次，这就导致我们没办法对这一特殊假期模式进行很好的预测。
      
   4. 剖析神经网络Neu

      对网络出现的问题进行诊断，看看哪一些神经元导致了预测偏差

      ```python
      # 选出三天预测不准的日期：Dec 22，23，24
      # 将这三天的数据聚集到一起，存入subset和subtargets中
      # 根据rides每一行"dteday"项是否等于所给日期，形成一个bool列表
      bool1 = rides['dteday'] == '2012-12-22'
      bool2 = rides['dteday'] == '2012-12-23'
      bool3 = rides['dteday'] == '2012-12-24'
      
      # zip(bool1,bool2,bool2)是将bool1,bool2,bool3每i项,
      # 组成1个3元素列表，然后将所有3元素列表合成一个列表
      #any() 函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，
      # 如果有一个为 True，则返回 True。元素除了是 0、空、FALSE 外都算 TRUE。
      bools = [any(tup) for tup in zip(bool1,bool2,bool3) ]
      # 将相应的变量取出来
      # test_features和test_targets的索引并不是从0开始，而是从截取的地方开始，所以能用loc确定
      # print(test_features.index)
      subset = test_features.loc[rides[bools].index]
      # print(subset)
      subtargets = test_targets.loc[rides[bools].index]
      # print(subtargets)
      subtargets = subtargets['cnt']
      subtargets = subtargets.values.reshape([len(subtargets),1])
      ```

      ```python
      def feature(X, net):
          # 定义了一个函数可以提取网络的权重信息，所有的网络参数信息全部存储在了neu的named_parameters集合中了
          X = torch.tensor(X, dtype = torch.float, requires_grad = False)
          dic = dict(net.named_parameters()) #提取出来这个集合
          weights = dic['0.weight'] #可以按照层数.名称来索引集合中的相应参数值
          biases = dic['0.bias'] #可以按照层数.名称来索引集合中的相应参数值
          h = torch.sigmoid(X.mm(weights.t()) + biases.expand([len(X), len(biases)])) # 隐含层的计算过程
          return h # 返回隐藏层的计算
      
      # 将这几天的数据输入到神经网络中，读取出隐含层神经元的激活数值，存入results中
      # results隐藏层输出数据
      results = feature(subset.values, neu).data.numpy()
      # 这些数据对应的预测值（输出层）
      #predict输出层输出数据
      predict = neu(torch.tensor(subset.values, dtype = torch.float, requires_grad = True)).data.numpy()
      
      #将预测值还原成原始数据的数值范围
      mean, std = scaled_features['cnt']
      predict = predict * std + mean
      subtargets = subtargets * std + mean
      # 将所有的神经元激活水平画在同一张图上，蓝色的是模型预测的数值
      fig, ax = plt.subplots(figsize = (8, 6))
      # results每一列元素是输入的同一行元素的各个神经元输出，每行是rides每一行数据
      # .点 :虚线，alpha透明度
      ax.plot(results[:,:],'.:',alpha = 0.3)
      # b蓝色 s正方形 -线 ，cnt预测值，
      ax.plot((predict - min(predict)) / (max(predict) - min(predict)),'bs-',label='Prediction')
      # r红色 o圆圈 -线,cnt实际情况
      ax.plot((subtargets - min(predict)) / (max(predict) - min(predict)),'ro-',label='Real')
      #:虚线 *五角星 隐藏层第4个神经元输出值
      ax.plot(results[:, 3],':*',alpha=1, label='Neuro 4')
      
      # set_xlim设置横轴范围
      ax.set_xlim(right=len(predict))
      # 显示标签
      ax.legend()
      plt.ylabel('Normalized Values')
      
      dates = pd.to_datetime(rides.loc[subset.index]['dteday'])
      dates = dates.apply(lambda d: d.strftime('%b %d'))
      # set_xticks设置坐标轴刻度
      ax.set_xticks(np.arange(len(dates))[12::24])
      # set_xticklabels设置刻度的显示文本
      ax.set_xticklabels(dates[12::24], rotation=45)
      fig.savefig("3-12.jpg")
      ```

      ![]()

4. 机器也懂得感情-中文情绪分类器

5. 手写数字识别-认识卷积神经网络-认识卷积神经网络

6. 手写数字加法机-迁移学习

7. 你自己的Prisma-图像风格迁移

8. 人工智能造假术--图像生成与对抗学习

9. 词汇星空--神经语言模型与Word2Vec

10. LSTM作曲机-序列生成模型

11. 神经翻译机--端到端机器翻译

12. AI游戏高手--深度强化学习

