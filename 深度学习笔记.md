1. 深度学习简介

   人工神经网络称为人工智能的连接学派

   1. 感知机到人工神经网络

      + 1957年 弗兰克提出感知机模型（不是直接从功能的角度而是通过结构模拟）
      + 1969年 提出感知机模型连XOR问题无法解出，人工神经网络被打入网络
      + 辛顿发展了人工神经网络的反向传播法，从而可以构造两层以上神经网络，并且可以有效进行学习与训练，两层以上神经网络可以很轻松解决XOR问题。
      + 受限于当时的计算能力，另一方面缺乏大规模高质量的数据，神经网络本身就是一个黑箱，谁也不敢保证深度神经网络在深度这个方向上就能够取得更好的结果和精度。人工神经网络并没有继续沿着深度的方向发展下去。
      + 学术界的焦点朝向了另一个发展方向：寻找神经网络的基础理论。在弗拉基米尔和亚历克塞推进下，统计学习理论蓬勃发展，奠定了模式识别的数学基础，创造出了支持向量机这种极其实用简单的工具。与传统神经网络通过加深网络来提升精度相反，支持向量机的解决方案是将数据的维度提升，在高维种寻找能够将数据进行准确划分的方法，这种方法在数据量不是很大情况下很奏效。支持向量机称为20世纪90年代到21世纪初的宠儿。
      + 2006年 辛顿发表题为《利用神经网络进行数据的维度约减》的文章，提出了深度神经网络模型（DNN），并指出如果我们能够将神经网络的层数加深，并且精心设计训练网络的方式，那么这样深层次的神经网络就会具有超强的表达能力和学习能力。

   2. 深度学习时代

      + 深度学习首先在语音领域取得突破。微软的邓力邀请辛顿加入语音识别的深度神经网络模型的开发，使识别的准确度有了大幅的提升。
      + 机器视觉专家李飞飞，借助网友的力量构造出了ImageNet这样一个大规模、高精度、多标签的图像数据库。于是李飞飞开始每年举办一次图像识别大赛：ImageNet竞赛。2012年，辛顿和他的两个学生采用深层次的卷积神经网络（AlexNet）,在ImageNet竞赛中将分类错误率从25%降到了17%。将深度网络做到8层，且不需要任何预处理就能将图像分类任务做到这么好，还是头一次。深度神经网络称为了ImageNet竞赛的标配，从AlexNet到GoogleNet，网络深度不断增加，准确率不断提升。2012年后深度学习开始在学术圈流行起来。

   3. 深度学习发展

      + 2011年 谷歌X实验室的杰夫和吴恩达等人采用深度学习技术，让谷歌大脑深度神经网络观看了从Youtube上提取出来的30万张图像，并让机器自动进行精简，谷歌大脑自己学出了一张猫脸。看到发展前景后，以谷歌未代表的各大公司开始疯狂并购人工智能、人工智能初创公司和团队，促使更多的人才和创业公司投入到人工智能大潮中。

      + 自然语言处理领域，2013年，谷歌的托马斯提出了Word2Vec技术，它可以非常快捷有效地计算单词间的向量表示，为大规模使用人工神经网络技术处理人类语言奠定了重要基础

      + 2014年，谷歌开始尝试利用深度的神经循环网络来处理各种自然语言任务，包括机器翻译，自动对话，情绪识别、阅读理解等。2016年，谷歌机器翻译技术取得重大突破，采用了先进的深度循环神经网络和注意力机智的机器翻译在多种语言上已经基本接近人类水平。

      + 强化学习与深度学习的结合，深度学习在计算机游戏、博弈等领域同样取得了重大进展。

        2015年被谷歌收购的DeepMind团队研发了一种“通用人工智能算法”，他可以像人类一样，通过观察计算机游戏屏幕进行自我学习，利用同一套网络架构和超参数，从零开始学习每一款游戏，并最终打通了300多款雅达利游戏，在某些游戏上的表现甚至超越了人类。

      + 2016年 DeepMind团队又在博弈领域取得了重大突破。AlphaGo以4:1战胜人类围棋冠军。2017年 DeepMind团队创造的 AlphaGo 升级版 AlphaGo Zero ，它可以完全从零开始学习下围棋，而无需借鉴任何人类的下棋经验。仅经过大约3天训练，AlphaGo Zero就达到了战胜李世石的棋力水平。而到了21天后，世界上已经没有任何人类或程序可以在围棋上战胜它了。AlphaGo 的成功不仅标志着以深度学习技术为支撑的新一代人工智能技术大获全胜，更暗示着人工智能全新时代的来临。  

   4. 深度学习的影响因素

      1. 大数据

         如果没有足够大量的数据输入给深度神经网络，就无法发挥深度的作用。伴随着网络深度的增加，待拟合的参数自然会也会增加，如果没有与其相匹配的海量数据来训练网络，这些参数就完全变成了导致网络过拟合的垃圾，无法发挥作用

      2. 深度网络架构（整个网络体系的构建方式和拓扑连接结构）

         面对具体问题时，应该采用什么样的网络架构，如何选取参数，如何训练这个网络，仍然是影响学习效率和解决问题的重要因素。

         目前主要分为： 

         + 前馈神经网络  

           每一层的节点只跟它相邻层节点而且是全部节点相连（全连接的），分为输入层，隐藏层，输出层

         + 卷积神经网络

           卷积层和池化层

           图中每一个立方体都是一系列规则排列的人工神经元的集合。

           每个神经元到上一层次的连接称为卷积核，它们都是一种局域的小窗口。

           图中的小锥形可以理解为从高层的某一个神经元到低层多个神经元之间的连接。这个小锥形在立方体上逐像素的平移就构成了两层次之间的所有连接。到了最后两层，小立方体被压缩成了一个一维的向量，这就与普通的前馈神经网络没有区别。

           CNN这种特殊的架构可以很好地应用于图像处理，它可以使原始的图像即使在经历过平移、缩放等变换后仍然具有很高的识别准确性。正是因为具有这样特殊的架构，CNN才成功应用于计算机视觉、图像识别、图像生成，甚至AI下围棋、AI打游戏等广阔领域

         + 循环神经网络
      
           输入层，输出层是单层，中间的隐藏层的节点相互连接。隐藏层彼此之间还有大量的连接。
      
           RNN这种特殊的架构使得网络当前的运行不仅跟当前的输入数据有关，而且还与之前的数据有关。因此，这种网络特别适合处理诸如语言、音乐、股票曲线等序列类型的数据，整个网络的循环结构可以很好地应付输入序列之中存在的长程记忆性和周期性
      
         + 训练方式
      
           训练方式也会对结果产生很大影响。如果先将少量特定标签的数据输入网络，然后再拿剩下的数据去训练它，就会比一股脑把所有便签数据都输入更加有效，从而提高网络的“学习”能力。
      
      3. GPU 
      
         GPU非常擅长大规模的张量（高阶矩阵）运算，并且可以为这种运算加速，对包含多个数值的张量运算所需的平均时间远低于对每个数字的运算时间
      
   5. 深度学习的成功

      深度学习重要的本领在于它可以从海量的数据中自动学习，抽取数据中的特征。

      + 特征学习

        深度神经网络会把不同的信息表达到不同层次的网络单元中，这一提炼过程完全不需要手工干预，全凭机器学习过程自动完成。深度学习的本质就是这种自动提取特征的功能。

      + 迁移学习

        把一个训练好的神经网络切开，然后再把它拼接到另一个神经网络上，前半部分用于特征提取，后半部分网络去解决另一个完全不同的问题。

2. PyTorch简介

   1. PyTorch安装

   2. 与Python完美融合

      使用Pytorch与使用其他Python程序包没有任何区别
      
      与此形成鲜明对比的是TensorFlow，TensorFlow会将一个深度学习任务分为定义为计算图和执行计算过程，而定义计算图的过程就好像在使用一套全新的语言。PyTorch就没有这个缺点，从定义计算图到执行计算是一气呵成的。
      
   3. 张量（tensor）计算

      PyTorch的运算单元叫做张量，1阶张量即为1维数组（向量vector），2阶张量为2维数组（矩阵matrix），3阶张量即为3维数组

      ```python
      import torch
      x=torch.rand(5,3)
      x
      tensor([[0.1440, 0.7954, 0.8075],
              [0.1011, 0.0410, 0.4783],
              [0.1711, 0.4874, 0.0733],
              [0.8815, 0.3930, 0.4344],
              [0.8639, 0.1563, 0.8560]])
      
      ```
      
      ```python
      y=torch.ones(5,3)
      y
      tensor([[1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.]])
      ```
      
      ```python
      # pytorh矩阵叉乘法 mm() ,转置t()
      q=x.mm(y.t())
      q
      tensor([[1.7469, 1.7469, 1.7469, 1.7469, 1.7469],
              [0.6204, 0.6204, 0.6204, 0.6204, 0.6204],
              [0.7319, 0.7319, 0.7319, 0.7319, 0.7319],
              [1.7089, 1.7089, 1.7089, 1.7089, 1.7089],
              [1.8762, 1.8762, 1.8762, 1.8762, 1.8762]])
      ```
      
      ```python
      # pytorh张量与Numpy数组之间的转换
      import numpy as np
      x_tensor=torch.randn(2,3)
      y_numpy=np.random.randn(2,3)
      # 将张量转换为numpy
      x_numpy=x_tensor.numpy()
      # 将numpy转化为张量
      y_tensor=torch.from_numpy(y_numpy)
      print(x_tensor)
      print(x_numpy)
      print(y_numpy)
      print(y_tensor)
      
      tensor([[ 1.6723,  0.8274,  1.4538],
              [ 0.5076, -1.6918,  0.1204]])
      [[ 1.672333    0.8274111   1.453827  ]
       [ 0.5075982  -1.691759    0.12042859]]
      [[-1.53580661  1.05183357 -0.13190343]
       [ 0.26008687 -0.76392427  0.49671979]]
      tensor([[-1.5358,  1.0518, -0.1319],
              [ 0.2601, -0.7639,  0.4967]], dtype=torch.float64)
      ```
      
      ```python
      # GPU上的张量计算
      if torch.cuda.is_available():
          x=x.cuda()
          y=y.cuda()
          print(x+y)
          
      tensor([[1.1440, 1.7954, 1.8075],
              [1.1011, 1.0410, 1.4783],
              [1.1711, 1.4874, 1.0733],
              [1.8815, 1.3930, 1.4344],
              [1.8639, 1.1563, 1.8560]], device='cuda:0')
      ```
      
   4. 动态计算图

      计算图用于解决用于解决反向传播算法问题。当前馈运算步骤完成之后，深度学习框架就会自动搭建一个计算图，通过这个图，就可以让反向传播算法进行。

      自动微分变量是通过3个重要的属性data（张量）、grad（梯度值）以及grad_fn（获得计算图的上一个节点）来实现的。在采用了自动微分变量以后，无论一个计算过程多么复杂，系统都会自动构造一张计算图来记录所有的运算过程。

      ```python
      # 导入自动微分变量的包
      # from torch.autograd import Variable
      # requires_grad=True是为了保证在反向传播算法中获得梯度信息
      # x=Variable(torch.ones(2,2),requires_grad=True) Variable被废弃
      x=torch.ones(2,2,requires_grad=True)
      x
      tensor([[1., 1.],
              [1., 1.]], requires_grad=True)
      ```

      ```python
      y=x+2
      y
      tensor([[3., 3.],
              [3., 3.]], grad_fn=<AddBackward0>)
      ```

      ```python
      y.data
      tensor([[3., 3.],
              [3., 3.]])
      ```

      ```python
      #返回上一个计算图节点
      y.grad_fn
      <AddBackward0 at 0x1df0dc7f640>
      ```

      ```python
      # *是点乘，只有相同位置的元素相乘
      z=y*y
      z
      tensor([[9., 9.],
              [9., 9.]], grad_fn=<MulBackward0>)
      ```

      ```python
      z.grad_fn
      <MulBackward0 at 0x1df0dc9b280>
      ```

      ```python
      # torch.mean对矩阵的每个元素求和再除以元素的个数。
      t=torch.mean(z)
      t
      tensor(9., grad_fn=<MeanBackward0>)
      ```

      ```python
      # backward()反向梯度传播，自动进行求导计算
      # 只有叶结点才可以通过.backward()获得梯度信息，z和y不是叶结点，所以没有梯度信息
      # retain_graph=True 使得t.backward能运行很多次，没有backward智能运行一次
      t.backward(retain_graph=True)
      print(z.grad)
      print(y.grad)
      print(x.grad)
      None
      None
      tensor([[3., 3.],
              [3., 3.]])
      ```

   5. PyTorch实例：预测房价

      1. 准备数据

         ```python
         # 构造0~50之间均匀数字作为时间变量。
         x=torch.linspace(0,50,steps=50,requires_grad=True).type(torch.float)
         x
         tensor([ 0.0000,  1.0204,  2.0408,  3.0612,  4.0816,  5.1020,  6.1224,  7.1429,
                  8.1633,  9.1837, 10.2041, 11.2245, 12.2449, 13.2653, 14.2857, 15.3061,
                 16.3265, 17.3469, 18.3673, 19.3878, 20.4082, 21.4286, 22.4490, 23.4694,
                 24.4898, 25.5102, 26.5306, 27.5510, 28.5714, 29.5918, 30.6122, 31.6327,
                 32.6531, 33.6735, 34.6939, 35.7143, 36.7347, 37.7551, 38.7755, 39.7959,
                 40.8163, 41.8367, 42.8571, 43.8776, 44.8980, 45.9184, 46.9388, 47.9592,
                 48.9796, 50.0000])
         ```

         ```python
         # 生成均值为0，方差为5的正态分布。
         rand=torch.randn(50).type(torch.float)*5
         # torch.normal(mean=0, std=10,out=50)
         rand
         tensor([-9.5013, -0.2001, -0.2612,  7.3100,  7.8910, -6.8819, -2.4307, -2.4865,
                  5.9178,  4.6767,  2.6463, -4.2847, -1.1919, -7.6735,  6.0805, -4.8176,
                  4.2334, -6.2658,  6.2607,  0.8406, -8.8556,  6.1883, 12.6984,  0.4164,
                  2.1272,  5.9475, -5.7067,  5.4889,  1.4164,  5.9655,  0.4942,  2.8694,
                 -2.0671,  0.4787, -3.0500,  5.3546, -0.5102,  3.3566, -0.3483,  3.2969,
                  2.4644,  0.0407,  5.4788, -7.3005,  0.5329,  3.5430, -3.5318, -4.6006,
                 -8.2579,  1.2810])
         ```

         ```python
         # 生成房价
         y=x+rand
         y
         tensor([-2.5061, -8.4511,  8.3336,  0.6089,  8.8587,  0.4199,  6.9523,  2.9417,
                 11.2004,  5.9490, 15.7981, 16.6783,  5.6091, 13.6873, 15.6817, 13.3907,
                 28.9034, 15.6045, 19.9990, 24.7943, 22.4937, 24.9002, 19.5600, 18.0025,
                 26.3569, 26.2317, 28.3032, 27.7680, 24.7899, 32.1754, 35.6954, 25.2297,
                 27.5482, 36.3927, 26.6795, 37.5358, 27.3953, 38.8195, 35.6925, 36.0569,
                 36.8523, 53.8654, 39.3955, 43.5647, 45.5984, 47.2127, 46.3957, 46.6790,
                 49.8011, 54.2800])
         ```

         生成训练集和测试集

         ```python
         x_train=x[:-5]
         x_test=x[-5:]
         y_train=y[:-5]
         y_test=y[-5:]
         ```

         对训练数据进行可视化

         ```python
         # 对训练数据进行可视化
         import matplotlib.pyplot as plt
         # 设定绘制窗口大小为8*6inch
         plt.figure(figsize=(8,6))
         # 绘制数据，由于x和y都是Variable，需要用data获取它们包裹的Tensor,并转成Numpy
         plt.plot(x_train.data.numpy(),y_train.data.numpy(),'o')
         # 设置X轴标签
         plt.xlabel('X')
         plt.ylabel('Y')
         plt.show()
         ```

         ![](.\2-1.jpg)

      2. 模型设计

         我们希望得到一条尽可能从中间穿越这些数据散点的拟合直线。设这条直线方程为
         $$
         y=ax+b
         $$
         接下来的问题是，求解出参数a,b的数值。我们可以将每一个数据点代入这个方程中，计算出
         $$
   \hat{y_i}=ax_i+b
         $$
         
         显然
         $$
         \hat{y_i}
         $$
         越靠近
         $$
         y_i
         $$
         越好，定义平均损失函数
         $$
         L=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac{1}{N}\sum_{i=1}^N(y_i-ax_i-b)^2
         $$
         
         并让它尽可能的小。由于
         $$
         x_i和y_i
         $$
         都是固定的数，而只有a和b是变量，那么L本质上就是a和b的函数。所以我们要寻找最优的a、b组合，让L最小化。
         
         我们可以利用梯度下降法来反复迭代a和b，从而让L越变越小。
         $$
         a_{t+1}=a_t-\alpha \left. \frac{\partial L}{\partial a}  \right| _{a=a_t}
         $$
         
         $$
         b_{t+1}=b_t-\alpha \left. \frac{\partial L}{\partial b}  \right| _{b=b_t}
         $$
         
         $$
         \alpha为学习率，它可以调节梯度下降快慢，\alpha越大，a、b更新得越快，但是计算得到的最优值L就有可能越不准
         $$
         
         在计算过程中，我们需要计算出L对a、b的偏导数，利用PyTorch的backward（）可以非常方便地将这两个偏导数计算出来。于是，我们只需要一步步地更新a和b的数值就可以了。当达到一定的迭代步数之后，最终a和b的数值就是我们想要的最优数值。
         
      3. 训练
      
         ```python
         # 定义两个自动微分变量a和b(a,b随机)
         # a=Variable(torch.rand(1),requires_grad=True)  Variable已经被废弃
         # b=Variable(torch.rand(1),requires_grad=True)
         a=torch.rand(1,requires_grad=True)
         b=torch.rand(1,requires_grad=True)
         # 设置学习率
         learning_rate=0.0001
         ```
      
         ```python
         # a和b的迭代计算
         for i in range(500):
             predictions=a*x_train+b
             loss=torch.mean((predictions-y_train)**2)
             print('loss:',loss)
             loss.backward()
         #add_    
             a.data.add_(-learning_rate*a.grad)
             b.data.add_(-learning_rate*b.grad)
         # 在更新完a、b的数值后，需要清空a中的的梯度信息，
         # 否则它会在下一步迭代的时候累加
             a.grad.zero_()
             b.grad.zero_()
         #太长，只写了输出了最后一个loss
         loss: tensor(23.5075, grad_fn=<MeanBackward0>)
         print(a.data,b.data)
         tensor([0.9775]) tensor([0.3045])
         ```
      
         将原始的数据散点联合拟合的直线通过图形画出来
      
         ```python
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         # 绘制拟合直线
         plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 坐标标注
         plt.xlabel("X")
         plt.ylabel("Y")
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2],['train_data',str1])
         plt.show()
         ```
         
      
      ![](.\2-2.jpg)
      
      4. 预测
      
         用测试数据集进行测试
      
         ```python
         predictions=a*x_test+b
         predictions
         tensor([45.6905, 46.6988, 47.7070, 48.7153, 49.7235], grad_fn=<AddBackward0>)
      
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         x_pred=x_test.data.numpy()
         y_pred=y_test.data.numpy()
         plt.figure(figsize=(10,7))
         #绘制训练集散点
         plt1, =plt.plot(x_data,y_data,'o')
         plt2, =plt.plot(x_pred,y_pred,'s')
         # 绘制拟合直线
         plt3, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # 绘制预测数据
         plt4, =plt.plot(x_pred,a.data.numpy()*x_pred+b.data.numpy())
         plt.xlabel('X')
         plt.ylabel('Y')
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2,plt3],['train_data','test_data',str1])
         plt.savefig("2-3.jpg")
         plt.show()
         ```
         
         ![](.\2-3.jpg)
         
         

3. 单车预测器：你的第一个神经网络

   运用PyTorch动手搭建一个共享单车预测器，在实战中掌握神经元、神经元、激活函数、机器学习等基本概念，以及数据预处理的方法。此外，揭秘神经网络这个“黑箱”，看看它如何工作，哪个神经元起到了关键作用。

   ```python
   #导入需要使用的库
   import numpy as np
   import pandas as pd #读取csv文件的库
   import matplotlib.pyplot as plt
   import torch
   #from torch.autograd import Variable
   import torch.optim as optim
   
   # 让输出的图形直接在Notebook中显示
   %matplotlib inline
   data_path="hour.csv"
   rides = pd.read_csv(data_path)
   # head()读取5行数据并显示，查看数据格式
   rides.head()
   ```

   ![](.\3-1.jpg)

   ```python
   #我们取出最后一列的前50条记录来进行预测
   counts = rides['cnt'][:50]
   
   #获得变量x，它是1，2，……，50
   x = np.arange(len(counts))
   
   # 将counts转成预测变量（标签）：y
   y = np.array(counts)
   
   # 绘制一个图形，展示曲线长的样子
   plt.figure(figsize = (8, 6)) #设定绘图窗口大小
   plt.plot(x, y, 'o-') # 绘制原始数据
   plt.xlabel('X') #更改坐标轴标注
   plt.ylabel('Y') #更改坐标轴标注
   plt.savefig("3-2.jpg")
   plt.show()
   ```

   ![](.\3-2.jpg)

   1. 用线性回归对曲线进行拟合，尽管效果很差（回顾上一章节内容）

      ```python
      #我们取出数据库的最后一列的前50条记录来进行预测
      counts = rides['cnt'][:50]
      
      # 创建变量x，它是1，2，……，50
      x = torch.tensor(np.arange(len(counts)), dtype=torch.double, requires_grad = True)
      
      # 将counts转成预测变量（标签）：y
      y = torch.tensor(np.array(counts), dtype=torch.double, requires_grad = True)
      
      a = torch.rand(1, dtype=torch.double, requires_grad = True) #创建a变量，并随机赋值初始化
      b = torch.rand(1, dtype=torch.double, requires_grad = True) #创建b变量，并随机赋值初始化
      print('Initial parameters:', [a, b])
      learning_rate = 0.00001 #设置学习率
      for i in range(10000):
          ### 增加了这部分代码，清空存储在变量a，b中的梯度信息，以免在backward的过程中会反复不停地累加
          predictions = a * x+ b  #计算在当前a、b条件下的模型预测数值
          loss = torch.mean((predictions - y) ** 2) #通过与标签数据y比较，计算误差
          print('loss:', loss)
          loss.backward() #对损失函数进行梯度反传
          a.data.add_(- learning_rate * a.grad)  #利用上一步计算中得到的a的梯度信息更新a中的data数值
          b.data.add_(- learning_rate * b.grad)  #利用上一步计算中得到的b的梯度信息更新b中的data数值
          a.grad.zero_() #清空a的梯度数值
          b.grad.zero_() #清空b的梯度数值
      ```

      用线性回归进行拟合

      ```python
      x_data=x.data.numpy()
      y_data=y.data.numpy()
      plt.figure(figsize=(10,7))
      #绘制训练集散点
      plt1, =plt.plot(x_data,y_data,'o')
      # 绘制拟合直线
      plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
      # 坐标标注
      plt.xlabel("X")
      plt.ylabel("Y")
      str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
      plt.legend([plt1,plt2],['data',str1])
      plt.savefig("3-3.jpg")
      plt.show()
      ```

      ![](.\3-3.jpg)

   2. 第一个人工神经网络预测器（单车预测器1.0）

      输入层1个单元，隐含层1层有10个单元，输出层1个单元的人工神经网络
      
      ```python
      #取出数据库中的最后一列的前50条记录来进行预测
      counts = rides['cnt'][:50]
      
      #创建变量x，它是1，2，……，50
      x = torch.tensor(np.arange(len(counts), dtype = float), requires_grad = True)
      
      # 将counts转成预测变量（标签）：y
      y = torch.tensor(np.array(counts, dtype = float), requires_grad = True)
      
      # 设置隐含层神经元的数量
      sz = 10
      
      # 初始化所有神经网络的权重（weights）和阈值（biases）
      weights = torch.randn((1, sz), dtype = torch.double, requires_grad = True) #1*10的输入到隐含层的权重矩阵
      biases = torch.randn(sz, dtype = torch.double, requires_grad = True) #尺度为10的隐含层节点偏置向量
      weights2 = torch.randn((sz, 1), dtype = torch.double, requires_grad = True) #10*1的隐含到输出层权重矩阵
      
      learning_rate = 0.001 #设置学习率
      losses = []
      
      # 将 x 转换为(50,1)的维度，以便与维度为(1,10)的weights矩阵相乘
      x = x.view(50, -1)
      # 将 y 转换为(50,1)的维度
      y = y.view(50, -1)
      
      for i in range(100000):
          # 从输入层到隐含层的计算
          hidden = x * weights + biases
          # 将sigmoid函数作用在隐含层的每一个神经元上
          hidden = torch.sigmoid(hidden)
          #print(hidden.size())
          # 隐含层输出到输出层，计算得到最终预测
          predictions = hidden.mm(weights2)
          #print(predictions.size())
          # 通过与标签数据y比较，计算均方误差
          loss = torch.mean((predictions - y) ** 2) 
          #print(loss.size())
          losses.append(loss.data.numpy())
          
          # 每隔10000个周期打印一下损失函数数值
          if i % 10000 == 0:
              print('loss:', loss)
              
          #对损失函数进行梯度反传
          loss.backward()
          
          #利用上一步计算中得到的weights，biases等梯度信息更新weights或biases中的data数值
          weights.data.add_(- learning_rate * weights.grad)  
          biases.data.add_(- learning_rate * biases.grad)
          weights2.data.add_(- learning_rate * weights2.grad)
          
          # 清空所有变量的梯度值。
          # 因为pytorch中backward一次梯度信息会自动累加到各个变量上，因此需要清空，否则下一次迭代会累加，造成很大的偏差
          weights.grad.zero_()
          biases.grad.zero_()
          weights2.grad.zero_()
          
      loss: tensor(2256.8876, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(738.2561, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(561.8528, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(507.0011, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(473.5310, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(466.2351, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(462.0210, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(459.2573, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(457.6207, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(456.6714, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # 打印误差曲线
      plt.plot(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.show()
      ```
      
      ![](.\3-4.jpg)
      
      
      
      绘制深度学习拟合曲线
      
      ```python
      x_data = x.data.numpy() # 获得x包裹的数据
      plt.figure(figsize = (10, 7)) #设定绘图窗口大小
      xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据
      
      yplot, = plt.plot(x_data, predictions.data.numpy())  #绘制拟合数据
      plt.xlabel('X') #更改坐标轴标注
      plt.ylabel('Y') #更改坐标轴标注
      plt.legend([xplot, yplot],['Data', 'Prediction under 1000000 epochs']) #绘制图例
      plt.savefig("3-5.jpg")
      plt.show()
      ```
      
      ![](.\3-5.jpg)
      
      上面的程序之所以跑得很慢，是因为x的取值范围1～50。 而由于所有权重和biases的取值范围被设定为-1,1的正态分布随机数，这样就导致 我们输入给隐含层节点的数值范围为-50~50， 要想将sigmoid函数的多个峰值调节到我们期望的位置需要耗费很多的计算时间。
      
      我们的解决方案就是将输入变量的范围归一化(只需要改变输入变量x)
      
      ```python
      #创建归一化的变量x，它的取值是0.02,0.04,...,1
      x = torch.tensor(np.arange(len(counts), dtype = float) / len(counts), requires_grad = True)
      
      #输出损失函数结果
      loss: tensor(2165.3436, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(940.2517, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(689.8824, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(471.5665, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(231.3214, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(123.2615, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(74.8383, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(56.6815, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(48.7930, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(44.8804, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # 打印误差曲线
      # semilogy()对y轴按对数进行缩放
      plt.semilogy(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.savefig("3-6.jpg")
      plt.show()
      ```
      
      ![](.\3-6.jpg)
      
      绘制拟合曲线（代码同上）
      
      ![](.\3-7.jpg)
      
      对接下来的50个数据进行预测
      
      ```python
      counts_predict = rides['cnt'][50:100] #读取待预测的接下来的50个数据点
      
      #首先对接下来的50个数据点进行选取，注意x应该取51，52，……，100，然后再归一化
      x = torch.tensor((np.arange(len(counts_predict), dtype = float) + len(counts) )/ len(counts_predict)
                       , requires_grad = True)
      y=torch.tensor(np.array(counts_predict),dtype=torch.double,requires_grad=True)
      x=x.view(len(x),-1)
      hidden=x*weights+biases
      # hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)
      hidden=torch.sigmoid(hidden)
      predictions=hidden.mm(weights2)
      loss=torch.mean((y-predictions)**2)
      
      x_data=x.data.numpy()
      plt.figure(figsize=(10,7))
      xplot, =plt.plot(x_data,y.data.numpy(),'o')
      yplot, =plt.plot(x_data,predictions.data.numpy())
      plt.xlabel("X")
      plt.ylabel("y")
      plt.savefig("3-8.jpg")
      plt.show()
      ```
      
      ![](.\3-8.jpg)
      
      预测发现存在着非常严重的过拟合现象，原因是x和y根本没有关系，即单车使用数量不依赖于下标。
      
   3. 人工神经网络Neu（单车预测器2.0）

      1. 数据预处理

         对数值变量（连续）进行标准化处理

         由于我们利用了全部数据来训练神经网络，所以采用之前介绍的一次性在全部数据上训练网络的方法就会很慢， 所以我们将数据划分成了不同的撮（batch），一个批次一个批次地训练神经网络，因此我们还要对数据进行划分。

         对于类型变量（离散）处理，转换成独热编码

         有很多变量都属于类型变量，例如season=1,2,3,4，分四季。我们不能将season变量直接输入到神经网络，这是因为season数值越高并不表示相应的信号强度越大。我们的解决方案是将类型变量用一个“一位热码“（one-hot）来编码，也就是：

         𝑠𝑒𝑎𝑠𝑜𝑛=1→(1,0,0,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=2→(0,1,0,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=3→(0,0,1,0)

         𝑠𝑒𝑎𝑠𝑜𝑛=4→(0,0,0,1)

         因此，如果一个类型变量有n个不同取值，那么我们的“一位热码“所对应的向量长度就为n

         ```python
         #对于类型变量的特殊处理
         # season=1,2,3,4, weathersi=1,2,3, mnth= 1,2,...,12, hr=0,1, ...,23, weekday=0,1,...,6
         # 经过下面的处理后，将会多出若干特征，例如，对于season变量就会有 season_1, season_2, season_3, season_4
         # 这四种不同的特征。
         dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
         for each in dummy_fields:
             #利用pandas对象，我们可以很方便地将一个类型变量属性进行one-hot编码，变成多个属性
             dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)
             rides = pd.concat([rides, dummies], axis=1)
         
         # 把原有的类型变量对应的特征去掉，将一些不相关的特征去掉
         fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                           'weekday', 'atemp', 'mnth', 'workingday', 'hr']
         data = rides.drop(fields_to_drop, axis=1)
         data.head()
         ```
         
         对数值数值类型变量进行标准化
         
         ```python
         # 调整所有的特征，标准化处理
         quant_features = ['cnt', 'temp', 'hum', 'windspeed']
         #quant_features = ['temp', 'hum', 'windspeed']
         
         # 我们将每一个变量的均值和方差都存储到scaled_features变量中。
         scaled_features = {}
         for each in quant_features:
             mean, std = data[each].mean(), data[each].std()
             scaled_features[each] = [mean, std]
             data.loc[:, each] = (data[each] - mean)/std
         ```
         
         将数据集进行分割，分割成测试集和训练集
         
         ```python
         # 将所有的数据集分为测试集和训练集，我们以后21天数据一共21*24个数据点作为测试集，其它是训练集
         test_data = data[-21*24:]
         train_data = data[:-21*24]
         print('训练数据：',len(train_data),'测试数据：',len(test_data))
         
         # 将我们的数据列分为特征列和目标列
         
         #目标列
         target_fields = ['cnt', 'casual', 'registered']
         features, targets = train_data.drop(target_fields, axis=1), train_data[target_fields]
         test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]
         
         # 将数据从pandas dataframe转换为numpy
         X = features.values
         Y = targets['cnt'].values
         Y = Y.astype(float)
         Y = Y.reshape([-1])
         
         训练数据： 16875 测试数据： 504
         ```
         
      2. 构建神经网络并进行训练
      
         1. 手动编写用Tensor运算的人工神经网络
      
            ```python
            # 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层
            input_size = features.shape[1] #输入层单元个数
            hidden_size = 10 #隐含层单元个数
            output_size = 1 #输出层单元个数
            batch_size = 128 #每隔batch的记录数
            weights1 = torch.randn([input_size, hidden_size], dtype = torch.double,  requires_grad = True) #第一到二层权重
            biases1 = torch.randn([hidden_size], dtype = torch.double, requires_grad = True) #隐含层偏置
            weights2 = torch.randn([hidden_size, output_size], dtype = torch.double, requires_grad = True) #隐含层到输出层权重
            def neu(x):
                #计算隐含层输出
                #x为batch_size * input_size的矩阵，weights1为input_size*hidden_size矩阵，
                #biases为hidden_size向量，输出为batch_size * hidden_size矩阵    
                hidden = x.mm(weights1) + biases1.expand(x.size()[0], hidden_size)
                hidden = torch.sigmoid(hidden)
                
                #输入batch_size * hidden_size矩阵，mm上weights2, hidden_size*output_size矩阵，
                #输出batch_size*output_size矩阵
                output = hidden.mm(weights2)
                return output
            def cost(x, y):
                # 计算损失函数
                error = torch.mean((x - y)**2)
                return error
            def zero_grad():
                # 清空每个参数的梯度信息
                if weights1.grad is not None and biases1.grad is not None and weights2.grad is not None:
                    weights1.grad.data.zero_()
                    weights2.grad.data.zero_()
                    biases1.grad.data.zero_()
            def optimizer_step(learning_rate):
                # 梯度下降算法
                weights1.data.add_(- learning_rate * weights1.grad.data)
                weights2.data.add_(- learning_rate * weights2.grad.data)
                biases1.data.add_(- learning_rate * biases1.grad.data)
            ```
      
            ```python
            # 神经网络训练循环
            losses = []
            for i in range(1000):
                # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取
                batch_loss = []
                # start和end分别是提取一个batch数据的起始和终止下标
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.double, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.double, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    zero_grad()
                    loss.backward()
                    optimizer_step(0.01)
                    batch_loss.append(loss.data.numpy())
                
                # 每隔100步输出一下损失值（loss）
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            ```python
            # 打印输出损失值
            fig = plt.figure(figsize=(10, 7))
            plt.plot(np.arange(len(losses))*100,losses, 'o-')
            plt.xlabel('epoch')
            plt.ylabel('MSE')
            plt.savefig("3-9.jpg")
            ```
      
            ![](.\3-9.jpg)
      
         2. 调用PyTorch现成的函数，构建序列化的神经网络
      
            ```python
            # 定义神经网络架构，features.shape[1]个输入层单元，10个隐含层，1个输出层
            input_size = features.shape[1]
            hidden_size = 10
            output_size = 1
            batch_size = 128
            neu = torch.nn.Sequential(
                torch.nn.Linear(input_size, hidden_size),
                torch.nn.Sigmoid(),
                torch.nn.Linear(hidden_size, output_size),
            )
            cost = torch.nn.MSELoss()
            # # pytorch自己会准备参数，直接使用neu.parameters就行
            optimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)
            
            ```
      
            ```python
            # 神经网络训练循环
            losses = []
            for i in range(1000):
                # 每128个样本点被划分为一个撮，在循环的时候一批一批地读取
                batch_loss = []
                # start和end分别是提取一个batch数据的起始和终止下标
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.float, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    batch_loss.append(loss.data.numpy())
                
                # 每隔100步输出一下损失值（loss）
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            调用PyTorch现成函数，收敛更快
         
         3. 测试神经网络
         
            ```python
            # 用训练好的神经网络在测试集上进行预测
            targets = test_targets['cnt'] #读取测试集的cnt数值
            targets = targets.values.reshape([len(targets),1]) #将数据转换成合适的tensor形式
            targets = targets.astype(float) #保证数据为实数
            
            # 将属性和预测变量包裹在Variable型变量中
            x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)
            y = torch.tensor(targets, dtype = torch.float, requires_grad = True)
            
            # 用神经网络进行预测
            predict = neu(x)
            predict = predict.data.numpy()
            # 将后21天的预测数据与真实数据画在一起并比较
            # 横坐标轴是不同的日期，纵坐标轴是预测或者真实数据的值
            fig, ax = plt.subplots(figsize = (10, 7))
            
            mean, std = scaled_features['cnt']
            ax.plot(predict * std + mean, label='Prediction', linestyle = '--')
            ax.plot(targets * std + mean, label='Data', linestyle = '-')
            ax.legend()
            ax.set_xlabel('Date-time')
            ax.set_ylabel('Counts')
            # 对横坐标轴进行标注
            dates = pd.to_datetime(rides.loc[test_data.index]['dteday'])
            # lambda d: d.strftime() d是参数，d.strftime()是返回的结果
            #%b 本地简化的月份名称 %d  月内中的一天（0-31）
            dates = dates.apply(lambda d: d.strftime('%b %d'))
            #set_xticks设置横轴标记
            #set_xticklabels设置横轴标签
            ax.set_xticks(np.arange(len(dates))[12::24])
            ax.set_xticklabels(dates[12::24], rotation=45)
            ```
         
            ![](.\3-11.jpg)
         
         可以看到，两个曲线基本是吻合的 ，但是在12月25日前后几天的实际值和预测值偏差较大。仔细观察数据，我们发现12月25日正好是圣诞节，人们的出行习惯会与往日有很大的不同。在我们的训练样本中，因为整个数据仅有两年长度，所以包含圣诞节前后的样本仅有一次，这就导致我们没办法对这一特殊假期模式进行很好的预测。

         4. 剖析神经网络Neu
         
            对网络出现的问题进行诊断，看看哪一些神经元导致了预测偏差
         
            ```python
            # 选出三天预测不准的日期：Dec 22，23，24
            # 将这三天的数据聚集到一起，存入subset和subtargets中
            # 根据rides每一行"dteday"项是否等于所给日期，形成一个bool列表
            bool1 = rides['dteday'] == '2012-12-22'
            bool2 = rides['dteday'] == '2012-12-23'
            bool3 = rides['dteday'] == '2012-12-24'
            
            # zip(bool1,bool2,bool2)是将bool1,bool2,bool3每i项,
            # 组成1个3元素列表，然后将所有3元素列表合成一个列表
            #any() 函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，
            # 如果有一个为 True，则返回 True。元素除了是 0、空、FALSE 外都算 TRUE。
            bools = [any(tup) for tup in zip(bool1,bool2,bool3) ]
            # 将相应的变量取出来
            # test_features和test_targets的索引并不是从0开始，而是从截取的地方开始，所以能用loc确定
            # print(test_features.index)
            subset = test_features.loc[rides[bools].index]
            # print(subset)
            subtargets = test_targets.loc[rides[bools].index]
            # print(subtargets)
            subtargets = subtargets['cnt']
            subtargets = subtargets.values.reshape([len(subtargets),1])
            
            def feature(X, net):
                # 定义了一个函数可以提取网络的权重信息，所有的网络参数信息全部存储在了neu的named_parameters集合中了
                X = torch.tensor(X, dtype = torch.float, requires_grad = False)
                dic = dict(net.named_parameters()) #提取出来这个集合
                weights = dic['0.weight'] #可以按照层数.名称来索引集合中的相应参数值
                biases = dic['0.bias'] #可以按照层数.名称来索引集合中的相应参数值
                h = torch.sigmoid(X.mm(weights.t()) + biases.expand([len(X), len(biases)])) # 隐含层的计算过程
                return h # 返回隐藏层的计算
            
            # 将这几天的数据输入到神经网络中，读取出隐含层神经元的激活数值，存入results中
            # results隐藏层输出数据
            results = feature(subset.values, neu).data.numpy()
            # 这些数据对应的预测值（输出层）
            #predict输出层输出数据
            predict = neu(torch.tensor(subset.values, dtype = torch.float, requires_grad = True)).data.numpy()
            
            #将预测值还原成原始数据的数值范围
            mean, std = scaled_features['cnt']
            predict = predict * std + mean
            subtargets = subtargets * std + mean
            # 将所有的神经元激活水平画在同一张图上，蓝色的是模型预测的数值
            fig, ax = plt.subplots(figsize = (8, 6))
            # results每一列元素是输入的同一行元素的各个神经元输出，每行是rides每一行数据
            # .点 :虚线，alpha透明度
            ax.plot(results[:,:],'.:',alpha = 0.3)
            # b蓝色 s正方形 -线 ，cnt预测值，
            ax.plot((predict - min(predict)) / (max(predict) - min(predict)),'bs-',label='Prediction')
            # r红色 o圆圈 -线,cnt实际情况
            ax.plot((subtargets - min(predict)) / (max(predict) - min(predict)),'ro-',label='Real')
            #:虚线 *五角星 隐藏层第5个神经元输出值
            ax.plot(results[:, 4],':*',alpha=1, label='Neuro 5')
            
            # set_xlim设置横轴范围
            ax.set_xlim(right=len(predict))
            # 显示标签
            ax.legend()
            plt.ylabel('Normalized Values')
            
            dates = pd.to_datetime(rides.loc[subset.index]['dteday'])
            dates = dates.apply(lambda d: d.strftime('%b %d'))
            # set_xticks设置坐标轴刻度
            ax.set_xticks(np.arange(len(dates))[12::24])
            # set_xticklabels设置刻度的显示文本
            ax.set_xticklabels(dates[12::24], rotation=45)
            fig.savefig("3-12.jpg")
            ```
         
            ![](.\3-12.jpg)
         
            可以发现，4号神经元的输出曲线与真实输出曲线比较接近。因此，可以认为该神经元对提高预测准确性有更高的贡献。
         
            同时，我们还想知道Neuro 5神经元表现较好的原因以及它的激活是由谁决定的。进一步分析它的影响因素，可以知道是从输入层指向它的权重。
         
            对权重进行可视化
         
            输出神经网络所有的参数
         
            ```python
            for para in neu.named_parameters():
                print(para) 
            #由显示可以看出，每个list是所有输入层节点到单个隐藏层节点的权重
            ('0.weight', Parameter containing:
            tensor([[-2.1517e-01, -4.0614e-02, -1.0516e-01,  5.4308e-02, -1.1437e-01,
                     -6.2382e-02,  7.6552e-02,  5.8724e-02,  1.6933e-01,  6.4778e-02,
                     -1.0376e-03,  1.1390e-01,  5.9447e-02,  1.9477e-02,  9.5131e-02,
                      7.1202e-03, -3.4932e-03,  1.8305e-02,  1.5940e-01, -3.6335e-02,
                     -9.0728e-02, -5.4818e-03,  1.8764e-02, -1.2173e-01, -1.5531e-01,
                      2.9141e-02,  1.1026e-01,  6.3001e-03, -5.6181e-02, -2.1444e-03,
                      2.0125e-02,  3.7873e-03, -9.8517e-02, -2.6571e-02,  2.8248e-01,
                     -1.0817e-02,  3.3487e-02,  2.3034e-01,  8.3346e-02,  1.3844e-01,
                      7.7592e-02,  1.2086e-01, -1.8834e-01, -1.1458e-01, -9.3430e-02,
                     -1.1976e-01, -1.1405e-01,  6.5605e-04, -3.8784e-02, -4.9509e-02,
                      8.3201e-02,  1.4002e-02,  8.9738e-02, -1.0683e-01,  9.2824e-02,
                     -9.4244e-02],
                    [-4.5748e-02, -5.6792e-02,  3.4704e-03, -8.0855e-02,  8.0398e-03,
                     -8.4377e-02, -7.2572e-02,  1.2197e-01, -1.2841e-02, -4.6603e-02,
                     -3.4122e-02, -6.0449e-02, -5.5149e-02,  1.1901e-01,  2.2428e-03,
                     -5.9819e-03,  1.8554e-02, -3.5701e-02, -6.2146e-02,  2.8831e-02,
                      1.0577e-01,  3.2351e-03, -3.5365e-02,  9.5441e-03, -9.0695e-02,
                      1.6664e-02, -8.1705e-02,  2.3343e-02,  5.9412e-02, -6.8365e-02,
                      9.0864e-02,  2.3068e-02,  4.9710e-02,  8.5133e-02,  1.9855e-01,
                     -1.4844e-02, -5.8998e-02,  4.5605e-02, -9.2452e-02,  5.2184e-02,
                     -1.4754e-02, -1.1379e-01, -8.7882e-02, -1.2133e-01,  2.7798e-03,
                      8.9572e-02,  4.5472e-02,  8.7699e-02,  1.0058e-01,  2.3036e-02,
                      5.2959e-03,  2.3517e-02, -1.8909e-02,  6.5946e-02,  1.0925e-01,
                      1.2119e-02],
                    [-3.8906e-02,  2.5722e-01, -1.7514e+00,  1.2741e-01,  8.8538e-02,
                      4.8992e-01,  2.7472e-02, -4.5769e-02, -2.9506e-02,  1.1365e-01,
                      3.0334e-01,  2.5514e-01,  4.0342e-02,  2.1697e-01,  2.1132e-01,
                      1.5857e-01, -1.0447e-01, -1.7504e-01, -5.4133e-02, -1.1488e-01,
                      2.6780e-01, -2.0297e-02, -5.0323e-02,  7.1371e-03,  3.4310e-02,
                      1.6122e-01,  2.4689e-01,  2.9974e-01,  2.6373e-01,  2.5165e-01,
                      3.2716e-01,  2.8187e-01,  8.1486e-02,  1.8649e-01, -1.6147e-01,
                     -6.5835e-02, -5.9280e-02, -2.1905e-01, -1.9243e-01, -1.6769e-01,
                     -1.7760e-01, -2.9353e-01, -4.6459e-01, -3.7805e-01, -1.1472e-01,
                      1.1463e-01,  1.8257e-01,  1.4568e-01,  1.1474e-01,  2.3399e-02,
                      2.6706e-01,  2.7155e-01,  2.8522e-01,  1.3846e-01, -7.1238e-02,
                     -2.4729e-01],
                    [ 3.3648e-01, -6.3774e-01,  1.4335e+00,  5.5639e-01,  2.7001e-04,
                     -8.2939e-01, -3.4847e-01, -6.4637e-02, -2.0774e-01, -3.2223e-01,
                     -3.8961e-01, -2.1898e-01, -5.5440e-02, -2.2807e-01, -7.8723e-02,
                     -4.8461e-01, -1.9355e-01,  3.7654e-02,  1.4289e-01, -8.0443e-02,
                      1.3063e-01,  7.4473e-02, -4.7923e-02, -3.3240e-01, -2.2588e-01,
                      3.9526e-02,  1.4570e-01,  1.3986e-01,  2.6883e-01,  3.3923e-01,
                      2.8692e-01,  1.4695e-01,  2.2072e-01,  3.4411e-01,  3.3423e-01,
                      6.6300e-03, -1.1355e-01, -1.1812e-01, -5.0629e-02, -5.0916e-02,
                     -7.2028e-02, -2.6627e-01, -5.6601e-01, -6.2902e-01, -6.4426e-01,
                     -5.7755e-01, -4.1909e-01, -2.4197e-01,  2.4454e-02, -1.6738e-01,
                     -5.9531e-02, -9.3488e-02, -1.6436e-01, -6.9497e-02,  5.1467e-02,
                     -6.7463e-02],
                    [ 5.8546e-02, -1.7747e-01,  6.0087e-01, -5.2252e-04,  2.4187e-02,
                     -2.8056e-02, -6.0695e-02,  2.3104e-01,  1.6305e-01,  1.0924e-01,
                      8.8236e-02,  5.9427e-03,  4.2962e-02,  1.0267e-01,  1.3196e-01,
                     -1.0830e-01, -8.8868e-02,  2.9426e-01,  6.0332e-02,  2.2017e-02,
                      8.6852e-03, -7.6501e-02, -1.0688e-01, -2.0578e-01, -1.3539e-02,
                      2.5629e-02, -1.2307e-01, -4.1725e-02, -1.8924e-02, -1.4393e-01,
                     -2.7134e-01, -9.7338e-02, -1.3694e-01, -3.3830e-02,  4.9438e-01,
                      2.1309e-01,  2.4636e-01,  1.2325e-01,  1.5591e-01,  3.2296e-02,
                      1.1432e-01,  1.3553e-01, -1.0793e-01, -1.6454e-01, -1.0692e-01,
                     -1.6603e-01,  8.9779e-03, -7.9099e-02, -6.8704e-02, -6.0136e-02,
                      1.4625e-01,  8.0313e-02, -4.8874e-02,  5.5786e-02,  8.1427e-02,
                     -2.2995e-02],
                    [-1.1119e+00, -9.7381e-01, -3.7606e-01,  1.5340e-01,  5.6004e-02,
                      7.9537e-01,  5.1514e-01,  6.7084e-01,  3.6069e-01,  4.6672e-01,
                      6.3586e-01,  1.2361e+00,  5.4458e-02,  4.4888e-01,  3.4623e-01,
                     -7.8211e-02,  1.7609e-01,  2.5912e-01,  2.9812e-01,  2.3110e-01,
                      2.0134e-01, -6.1154e-02, -6.4564e-03,  3.1285e-01,  3.6553e-02,
                      4.4154e-01,  6.3237e-01,  8.4420e-01,  1.1231e+00,  1.3096e+00,
                      1.3350e+00,  1.3169e+00,  1.1446e+00,  5.8702e-01,  8.7077e-02,
                     -7.6181e-01, -1.0734e+00, -1.2484e+00, -1.2376e+00, -1.2156e+00,
                     -1.1791e+00, -9.7212e-01, -6.4376e-01, -3.4615e-01, -1.1356e-01,
                      2.5381e-01,  4.5731e-01,  6.0883e-01,  6.8720e-01, -8.3891e-01,
                      6.2651e-01,  8.1837e-01,  8.9260e-01,  9.1260e-01,  3.2260e-01,
                     -9.8295e-01],
                    [ 9.1535e-01, -1.8074e+00,  1.8815e-01, -6.2041e-02, -1.2599e-02,
                     -7.4529e-01, -5.4648e-01, -5.8447e-01, -4.5230e-01, -3.9348e-01,
                     -4.8381e-01, -1.0659e+00, -2.9770e-01, -5.1499e-01, -4.2147e-01,
                     -2.4996e-01, -2.5094e-01, -2.4765e-01, -1.1093e-01, -8.9495e-02,
                     -3.9652e-02,  3.5043e-02, -2.2566e-02, -2.3263e-01, -1.6356e-01,
                     -1.1322e+00, -1.3055e+00, -1.2845e+00, -1.0519e+00, -9.2288e-01,
                     -5.4272e-01,  3.5072e-01,  1.5925e+00,  2.3092e+00,  6.5942e-01,
                     -8.3098e-01, -1.0067e+00, -8.9203e-01, -1.0039e+00, -1.1965e+00,
                     -8.5223e-01,  2.2133e-01,  1.7310e+00,  1.6965e+00,  1.0636e+00,
                      6.4949e-01,  2.6775e-01, -4.7165e-02, -3.7327e-01, -2.8686e+00,
                      6.4927e-01,  6.9670e-01,  7.0795e-01,  7.1478e-01,  3.1973e-01,
                     -2.5693e+00],
                    [-7.3934e-02,  7.0198e-02,  2.2590e-01,  4.9484e-02, -1.0391e-01,
                     -1.6684e-01, -1.1900e-02, -2.3277e-02,  6.3789e-02, -1.2004e-02,
                      1.4248e-01, -6.8000e-02, -2.2170e-02,  6.0601e-02,  5.4319e-02,
                      5.3706e-03,  3.6641e-02,  4.4790e-02,  5.2938e-03, -3.6688e-02,
                      3.3774e-02,  1.2327e-02, -1.7138e-01, -1.1597e-02,  6.7697e-02,
                     -8.6218e-02, -1.5837e-02, -3.3704e-03, -6.9258e-02, -1.4105e-01,
                      2.5695e-03, -2.0978e-01, -3.4207e-01, -2.1496e-01,  2.4776e-01,
                      2.0992e-01,  6.4422e-02,  5.0799e-02,  1.4712e-01,  3.2227e-02,
                      1.4373e-01,  1.5062e-01,  6.3096e-03, -1.7201e-01,  4.6564e-02,
                      9.5400e-02,  6.6297e-03,  2.8192e-02, -3.6574e-02,  3.1847e-02,
                     -2.9076e-02, -7.6315e-02,  7.6755e-03,  6.6343e-02,  9.9783e-02,
                     -5.2374e-02],
                    [-2.5522e-01,  4.6821e-01, -9.5963e-01,  5.8046e-01,  1.7474e-01,
                      4.1009e-01,  2.7997e-01,  4.3716e-01,  3.1789e-02,  4.4291e-02,
                      2.4212e-01,  9.2161e-01,  2.2880e-03,  2.0859e-01,  4.3318e-02,
                     -5.1349e-02, -9.7250e-02, -4.0511e-02,  1.9764e-01,  1.2849e-01,
                      4.3351e-01,  2.4613e-02,  5.4710e-02,  4.4987e-02, -8.5056e-02,
                     -3.1730e-01, -3.5047e-01, -4.1137e-01, -4.7066e-01, -3.7748e-01,
                     -3.7737e-01, -3.0269e-01, -1.9244e-01, -2.2264e-01, -1.7633e-01,
                     -8.7452e-02,  5.8684e-02,  2.2100e-01,  3.3221e-01,  3.2697e-01,
                      2.8390e-01,  4.8635e-01,  6.0471e-01,  6.6388e-01,  5.3513e-01,
                      4.5366e-01,  3.5571e-01,  1.8685e-01, -3.5974e-02,  4.6679e-01,
                     -1.1762e-01,  1.6446e-01,  1.6282e-01,  8.7484e-02,  1.6215e-01,
                      4.2863e-01],
                    [ 1.7679e-02, -1.7197e-01,  1.1369e-01,  4.6212e-01, -1.0654e-01,
                     -2.3073e-01,  1.8183e-01,  5.7336e-02,  3.0706e-02, -1.4014e-01,
                      5.0070e-02,  3.2003e-02,  7.0526e-02,  6.3089e-02,  3.8743e-02,
                     -5.6618e-02,  2.5570e-02, -4.6891e-02,  5.3802e-02,  2.0393e-03,
                     -4.1424e-02,  1.3109e-03,  1.3134e-01, -1.3910e-01, -9.1821e-02,
                      2.2063e-01,  9.2872e-02,  2.9796e-01,  3.4915e-01,  2.6936e-01,
                      2.6538e-01,  2.0978e-01,  2.9419e-01, -1.1763e-01, -1.7401e-01,
                     -2.1172e-01, -8.9650e-02, -2.0237e-01, -1.7990e-01, -1.4242e-01,
                     -2.7767e-01, -1.2547e-01, -1.4733e-01, -2.0316e-01, -4.9766e-02,
                     -3.6871e-02,  4.7350e-02,  1.1792e-01,  5.1592e-02,  1.6399e-02,
                     -1.1771e-01,  1.4295e-01,  8.6799e-02, -1.0577e-01,  7.2028e-02,
                     -7.2114e-02]], requires_grad=True))
            ('0.bias', Parameter containing:
            tensor([-1.5522e-03,  6.9083e-02,  6.7617e-01, -1.0894e+00,  2.1665e-01,
                     2.1052e+00, -2.3456e+00,  1.1926e-01,  1.2289e+00, -2.6396e-02],
                   requires_grad=True))
            ('2.weight', Parameter containing:
            tensor([[ 0.7876,  0.4478, -1.7032, -1.8252,  1.1987, -4.3419,  6.1314,  0.9146,
                      2.4574, -0.3667]], requires_grad=True))
            ('2.bias', Parameter containing:
            tensor([1.3419], requires_grad=True))
            ```
         
            显示隐藏层所有节点到输出层的权重
         
            ```python
            dic = dict(neu.named_parameters())
            weights = dic['2.weight']
            # 因为weights.data.numpy()是1*1*10的矩阵，取[0]变成1*10的矩阵
            fig=plt.figure()
            plt.plot(weights.data.numpy()[0],'o-')
            plt.xlabel('hidden Neurons')
            plt.ylabel('hidden-output Weight')
            fig.savefig("3-13.jpg")
            ```
         
            ![](.\3-13.jpg)
         
            把输入层到隐藏层第5号节点的权重都显示出来
         
            ```python
            # 找到了与峰值相应的神经元，把它到输入层的权重输出出来
            dic = dict(neu.named_parameters())
            weights = dic['0.weight'][4]
            fig=plt.figure()
            plt.plot(weights.data.numpy(),'o-')
            plt.xlabel('Input Neurons')
            plt.ylabel('Weight')
            fig.savefig("3-14.jpg")
            ```
         
            ![](.\3-14.jpg)
         
            ```python
            # 列出所有的features中的数据列，找到对应的编号
            for (i, c) in zip(range(len(features.columns)), features.columns):
                print(i,c)
            0 yr
            1 holiday
            2 temp
            3 hum
            4 windspeed
            5 season_1
            6 season_2
            7 season_3
            8 season_4
            9 weathersit_1
            10 weathersit_2
            11 weathersit_3
            12 weathersit_4
            13 mnth_1
            14 mnth_2
            15 mnth_3
            16 mnth_4
            17 mnth_5
            18 mnth_6
            19 mnth_7
            20 mnth_8
            21 mnth_9
            22 mnth_10
            23 mnth_11
            24 mnth_12
            25 hr_0
            26 hr_1
            27 hr_2
            28 hr_3
            29 hr_4
            30 hr_5
            31 hr_6
            32 hr_7
            33 hr_8
            34 hr_9
            35 hr_10
            36 hr_11
            37 hr_12
            38 hr_13
            39 hr_14
            40 hr_15
            41 hr_16
            42 hr_17
            43 hr_18
            44 hr_19
            45 hr_20
            46 hr_21
            47 hr_22
            48 hr_23
            49 weekday_0
            50 weekday_1
            51 weekday_2
            52 weekday_3
            53 weekday_4
            54 weekday_5
            55 weekday_6
            ```
         
            由此可以得到该神经元检测的是 索引2温度，索引7 秋天，索引17五月，索引34 上午九点
         
            纵轴的值为正就是促进，值为负就是抑制，图中的波峰就是让该神经元激活，波谷就是神经元未激活
      
   4. 分类人工神经网路Neuc

      我们用神经网络解决一个分类问题，即将预测数值根据大于或者小于预测数量的平均值来分成两类 我们只需要对Neuc进行小小的更改，将其输出单元数量设置为2，并加上Sigmoid函数就可以了。对于Neuc来说，它的输出是两个数值，分别表示属于第0类和第1类的概率

      交叉熵具体实现
      
      ```python
      import torch
      import torch.nn.functional as F
      import torch.nn as nn
      x = torch.randn(5, 5)
      target = torch.tensor([0, 2, 3, 1, 4]) # 标签 这里还有一个torch.tensor与torch.Tensor的知识点https://blog.csdn.net/weixin_40607008/article/details/107348254
      # one_hot每一行就是一个样本，每一行的每一列对应一个类别
      one_hot = F.one_hot(target).float() # 对标签进行one_hot编码
      # reshape展开成1列
      print(one_hot)
      # print(x)
      # print(torch.sum(torch.exp(x),dim=1))
      softmax = torch.exp(x)/torch.sum(torch.exp(x), dim = 1).reshape(-1, 1)
      logsoftmax = torch.log(softmax)
      print(logsoftmax)
      print(one_hot*logsoftmax)
      nllloss = -torch.sum(one_hot*logsoftmax)/target.shape[0]
      nllloss
      
      tensor([[1., 0., 0., 0., 0.],
              [0., 0., 1., 0., 0.],
           [0., 0., 0., 1., 0.],
              [0., 1., 0., 0., 0.],
              [0., 0., 0., 0., 1.]])
      tensor([[-4.3277, -1.2263, -0.9527, -1.4808, -2.5225],
              [-2.1342, -1.3695, -3.6855, -0.6750, -2.3730],
              [-2.4937, -2.1367, -1.6788, -1.2862, -1.0893],
              [-2.3628, -1.9214, -1.6912, -0.5993, -3.6517],
              [-0.4310, -2.0773, -5.0553, -4.5454, -1.5709]])
      tensor([[-4.3277, -0.0000, -0.0000, -0.0000, -0.0000],
              [-0.0000, -0.0000, -3.6855, -0.0000, -0.0000],
              [-0.0000, -0.0000, -0.0000, -1.2862, -0.0000],
              [-0.0000, -1.9214, -0.0000, -0.0000, -0.0000],
              [-0.0000, -0.0000, -0.0000, -0.0000, -1.5709]])
      tensor(2.5583)
      ```
      
      二分类问题，人工神经网络实现
      
      ```python
      # 重新构造用于分类的人工神经网络Neuc
      
      input_size = features.shape[1]
      hidden_size = 10
      output_size = 2
      batch_size = 128
      neuc = torch.nn.Sequential(
          torch.nn.Linear(input_size, hidden_size),
          torch.nn.Sigmoid(),
          torch.nn.Linear(hidden_size, output_size),
          torch.nn.Sigmoid(),
      )
      # 将损失函数定义为交叉熵
      # 为什么不采用均方误差，主要原因是逻辑回归配合MSE损失函数时
      # 采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况（MSE损失函数）。
      cost = torch.nn.CrossEntropyLoss()
      optimizer = torch.optim.SGD(neuc.parameters(), lr = 0.1)
      
      # Y是训练数据cnt列
      Y_labels = Y > np.mean(Y)
      Y_labels = Y_labels.astype(int)
      Y_labels = Y_labels.reshape(-1)
      Y_labels
      ```
      
      ```python
      # 定义一个专门计算分类错误率的函数，它的基本思想是，对于预测向量predictions的每一行，
      # 取最大的那个元素的下标，与标签labels中的元素做比较,大的那个输出值对应预测类别
      def error_rate(predictions, labels):
          """计算预测错误率的函数，其中predictions是模型给出的一组预测结果，labels是数据之中的正确答案"""
      #     取每一行关于列的最大    
          predictions = np.argmax(predictions, 1)
          return 100.0 - (100.0 *np.sum( predictions == labels) /predictions.shape[0])
      
      # 神经网络训练循环
      losses = []
      errors = []
      for i in range(4000):
          # 每128个样本点被划分为一个撮
       batch_loss = []
          batch_errors = []
          for start, end in zip(range(0, len(X), batch_size), range(batch_size, len(X)+1, batch_size)):
      #        start与end相差128,只是为了取数据
      # 
              xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)
              #yy是类别标签
              yy = torch.tensor(Y_labels[start:end], dtype = torch.long)
              #predict 是输出层两个节点的输出值
              predict = neuc(xx)
              #loss是交叉熵
              loss = cost(predict, yy)
              err = error_rate(predict.data.numpy(), yy.data.numpy())
              optimizer.zero_grad()
              loss.backward()
              optimizer.step()
           batch_loss.append(loss.data.numpy())
              batch_errors.append(err)
       
          # 每隔100步输出一下损失值（loss）
       if i % 100==0:
              losses.append(np.mean(batch_loss))
           errors.append(np.mean(batch_errors))
              print(i, np.mean(batch_loss), np.mean(batch_errors))
          
      0 0.67895323 41.19155534351145
      100 0.4432484 12.750477099236642
      200 0.43353826 12.213740458015268
      300 0.42870235 11.838024809160306
      400 0.41688758 10.239742366412214
      500 0.4048622 8.778625954198473
      600 0.39466304 7.615696564885496
      700 0.38711914 6.882156488549619
      800 0.3819186 6.309637404580153
      900 0.3783126 5.898139312977099
      1000 0.3756589 5.689408396946565
      1100 0.373587 5.516459923664122
      1200 0.3719284 5.337547709923665
      1300 0.37057498 5.194417938931298
      1400 0.36945093 5.093034351145038
      1500 0.36849403 5.01550572519084
      1600 0.36765516 4.9856870229007635
      1700 0.36690468 4.9439408396946565
      1800 0.36622706 4.890267175572519
      1900 0.3656102 4.818702290076335
      2000 0.36504015 4.747137404580153
      2100 0.36450353 4.675572519083969
      2200 0.363987 4.639790076335878
      2300 0.36347347 4.58611641221374
      2400 0.36297363 4.538406488549619
      2500 0.36251283 4.550333969465649
      2600 0.36208788 4.490696564885496
      2700 0.36169383 4.413167938931298
      2800 0.36132643 4.383349236641221
      2900 0.3609817 4.341603053435114
      3000 0.360656 4.293893129770993
      3100 0.36034608 4.276001908396947
      3200 0.36004904 4.270038167938932
      3300 0.3597618 4.270038167938932
      3400 0.35948175 4.2461832061068705
      3500 0.35920674 4.240219465648855
      3600 0.35893834 4.2044370229007635
      3700 0.35867992 4.198473282442748
      3800 0.35843205 4.168654580152672
      3900 0.35819414 4.138835877862595
      ```
      
      ```python
      # 打印输出交叉熵损失值和错误率
      fig=plt.figure()
      plt.plot(np.arange(len(losses))*100,losses, label = 'Cross Entropy')
      plt.plot(np.arange(len(losses))*100, np.array(errors) / float(100), label = 'Error Rate')
      plt.xlabel('epoch')
      plt.ylabel('Cross Entropy/Error rates')
      plt.legend()
      plt.savefig("3-15.jpg")
      ```
      
      ![](.\3-15.jpg)
      
      将神经网络的分类结果通过图像显示
      
      ```python
      # 读取测试数据
      targets = test_targets['cnt']
      targets = targets.values.reshape(-1, 1)
      Y_labels = targets > np.mean(Y)
      Y_labels = Y_labels.astype(int)
      Y_labels = Y_labels.reshape(-1)
      x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)
      
      # 打印神经网络预测的错误率
      predict = neuc(x)
      print("error_rate ",error_rate(predict.data.numpy(), Y_labels))
      
      # 接下来，我们把预测正确的数据和错误的数据分别画出来，纵坐标分别是预测正确的概率和预测错误的概率
      prob = predict.data.numpy()
      # rights 预测正确的行号为true，预测错误的行号为false，返回一个true ，false的列表
      rights = np.argmax(prob, 1) == Y_labels
      wrongs = np.argmax(prob, 1) != Y_labels
      # 将rights中为true的行号代入，并返回标签
      right_labels = Y_labels[rights]
      wrong_labels = Y_labels[wrongs]
      probs = prob[rights, :]
      probs1 = prob[wrongs, :]
      # rightness最大的输出值预测正确，最大输出值的所形成的list
      rightness = [probs[i, right_labels[i]] for i in range(len(right_labels))]
      # right_index是targets中预测正确的下标
      right_index = np.arange(len(targets))[rights]
      # wrongness最大的输出值预测错误，最大输出值的所形成的list
      wrongness = [probs1[i, wrong_labels[i]] for i in range(len(wrong_labels))]
      # wrong_index是targets中预测正确的下标
      wrong_index = np.arange(len(targets))[wrongs]
      fig, ax = plt.subplots(figsize = (8, 6))
      ax.plot(right_index, rightness, '.', label='Right')
      ax.plot(wrong_index, wrongness,'o',label='Wrong')
      
      ax.legend()
      plt.ylabel('Probabilities')
      
      dates = pd.to_datetime(rides.loc[test_features.index]['dteday'])
      dates = dates.apply(lambda d: d.strftime('%b %d'))
      ax.set_xticks(np.arange(len(dates))[12::24])
      ax.set_xticklabels(dates[12::24], rotation=45)
      fig.savefig("3-16.jpg")
      ```
      
      ![](.\3-16.jpg)

4. 机器也懂得感情-中文情绪分类器

   1. 数据处理

      1. 对京东上抓取的评论进行预处理

         ```python
         # 数据来源文件
         good_file = 'data/good.txt'
         bad_file  = 'data/bad.txt'
         
         # 将文本中的标点符号过滤掉
         def filter_punc(sentence):
             sentence = re.sub("[\s+\.\!\/_,$%^*(+\"\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+", "", sentence)  
             return(sentence)
         
         #扫描所有的文本，分词、建立词典，分出正向还是负向的评论，is_filter可以过滤是否筛选掉标点符号
         def Prepare_data(good_file, bad_file, is_filter = True):
             all_words = [] #存储所有的单词
             pos_sentences = [] #存储正向的评论
             neg_sentences = [] #存储负向的评论
             with open(good_file, 'r',encoding = 'UTF-8') as fr:
                 for idx, line in enumerate(fr):
                     if is_filter:
                         #过滤标点符号
                         line = filter_punc(line)
                     #分词,将句子line分成多个分词，形成1个分词list
                     words = jieba.lcut(line)
         #             print(line,words)
                     if len(words) > 0:
                         all_words += words
                         pos_sentences.append(words)
             print('{0} 包含 {1} 行, {2} 个词.'.format(good_file, idx+1, len(all_words)))
         
             count = len(all_words)
             with open(bad_file,'r',encoding = 'UTF-8') as fr:
                 for idx, line in enumerate(fr):
                     if is_filter:
                         line = filter_punc(line)
                     words = jieba.lcut(line)
                     if len(words) > 0:
         #                 +=list去掉括号再加入
                         all_words += words
         #               append不去掉括号把整个list加入    
                         neg_sentences.append(words)
             print('{0} 包含 {1} 行, {2} 个词.'.format(bad_file, idx+1, len(all_words)-count))
         
             #建立词典，diction的每一项为{w:[id, 单词出现次数]}
             diction = {}
             cnt = Counter(all_words)
             for word, freq in cnt.items():
                 #第一项为词的标号，第二项为词的出现次数
                 diction[word] = [len(diction), freq]
             print('字典大小：{}'.format(len(diction)))
             return(pos_sentences, neg_sentences, diction)
         
         #根据单词返还单词的编码
         def word2index(word, diction):
             if word in diction:
                 value = diction[word][0]
             else:
                 value = -1
             return(value)
         
         #根据编码获得单词
         def index2word(index, diction):
             for w,v in diction.items():
                 if v[0] == index:
                     return(w)
             return(None)
         
         pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, True)
         st = sorted([(v[1], w) for w, v in diction.items()])
         st
         ```

   2. 词袋模型

      词袋模型实际上是一种对文本进行向量化的手段，通过统计出词表上的每个单词出现的频率，从而将一篇文章向量化。

      1. 训练数据准备

         ```python
         # 输入一个句子和相应的词典，得到这个句子的向量化表示
         # 向量的尺寸为词典中词汇的个数，i位置上面的数值为第i个单词在sentence中出现的频率
         def sentence2vec(sentence, dictionary):
             #vector是字典中所有分词的向量
             vector = np.zeros(len(dictionary))
             for l in sentence:
                 vector[l] += 1
             #vector向量元素和为1
             return(1.0 * vector / len(sentence))
         
         # 遍历所有句子，将每一个词映射成编码
         dataset = [] #数据集(存放所有句子的编码)
         labels = [] #标签
         sentences = [] #原始句子，调试用
         # 处理正向评论
         for sentence in pos_sentences:
             new_sentence = []
             for l in sentence:
                 if l in diction:
                     #根据分词得到编码，并形成分词索引形成的列表（一个句子）
                     new_sentence.append(word2index(l, diction))
             #把所有的分词索引所形成的列表（一个句子）转换成关于字典的向量
             dataset.append(sentence2vec(new_sentence, diction))
             labels.append(0) #正标签为0
             sentences.append(sentence)
         
         # 处理负向评论
         for sentence in neg_sentences:
             new_sentence = []
             for l in sentence:
                 if l in diction:
                     new_sentence.append(word2index(l, diction))
             dataset.append(sentence2vec(new_sentence, diction))
             labels.append(1) #负标签为1
             sentences.append(sentence)
         
         #打乱所有的数据顺序，形成数据集
         # indices为所有数据下标的一个全排列
         # permutation随机排列一个数组
         indices = np.random.permutation(len(dataset))
         
         #重新根据打乱的下标生成数据集dataset，标签集labels，以及对应的原始句子sentences
         dataset = [dataset[i] for i in indices]
         labels = [labels[i] for i in indices]
         sentences = [sentences[i] for i in indices]
         
         #对整个数据集进行划分，分为：训练集、校准集和测试集，其中校准和测试集合的长度都是整个数据集的10分之一
         test_size = len(dataset) // 10
         # 训练集
         train_data = dataset[2 * test_size :]
         train_label = labels[2 * test_size :]
         
         # 校准集
         valid_data = dataset[: test_size]
         valid_label = labels[: test_size]
         
         # 测试集
         test_data = dataset[test_size : 2 * test_size]
         test_label = labels[test_size : 2 * test_size]
         ```
         
      2. 定义模型

         ```python
         # 一个简单的前馈神经网络，三层，第一层线性层，加一个非线性ReLU，第二层线性层，中间有10个隐含层神经元
         
         # 输入维度为词典的大小：每一段评论的词袋模型
         model = nn.Sequential(
             nn.Linear(len(diction), 10),
             nn.ReLU(),
             nn.Linear(10, 2),
             nn.LogSoftmax(),
         )
         
         def rightness(predictions, labels):
             """计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案"""
             pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标
             rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量
             return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素
         ```

      3. 模型训练

         ```python
         # 损失函数为交叉熵
         cost = torch.nn.NLLLoss()
         # 优化算法为Adam，可以自动调节学习率
         optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
         records = []
         
         #循环10个Epoch
         losses = []
         for epoch in range(10):
             for i, data in enumerate(zip(train_data, train_label)):
                 x, y = data
                 
                 # 需要将输入的数据进行适当的变形，主要是要多出一个batch_size的维度，也即第一个为1的维度
                 x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)
                 # x的尺寸：batch_size=1, len_dictionary
                 # 标签也要加一层外衣以变成1*1的张量
                 y = torch.tensor(np.array([y]), dtype = torch.long)
                 # y的尺寸：batch_size=1, 1
                 
                 # 清空梯度
                 optimizer.zero_grad()
                 # 模型预测
                 predict = model(x)
                 # 计算损失函数
                 loss = cost(predict, y)
                 # 将损失函数数值加入到列表中
                 losses.append(loss.data.numpy())
                 # 开始进行梯度反传
                 loss.backward()
                 # 开始对参数进行一步优化
                 optimizer.step()
                 
                 # 每隔3000步，跑一下校验数据集的数据，输出临时结果
                 if i % 3000 == 0:
                     val_losses = []
                     rights = []
                     # 在所有校验数据集上实验
                     #j是标号，val是(valid_data,valid_label)的组合
                     for j, val in enumerate(zip(valid_data, valid_label)):
                         x, y = val
                         # x展开成1行多列
                         x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)
                         y = torch.tensor(np.array([y]), dtype = torch.long)
                         predict = model(x)
                         # 调用rightness函数计算准确度
                         right = rightness(predict, y)
                         rights.append(right)
                         loss = cost(predict, y)
                         val_losses.append(loss.data.numpy())
                         
                     # 将校验集合上面的平均准确度计算出来
                     right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])
                     print('第{}轮，训练损失：{:.2f}, 校验损失：{:.2f}, 校验准确率: {:.2f}'.format(epoch, np.mean(losses),
                                                                                 np.mean(val_losses), right_ratio))
                     records.append([np.mean(losses), np.mean(val_losses), right_ratio])
         
         第0轮，训练损失：0.72, 校验损失：0.70, 校验准确率: 0.40
         第0轮，训练损失：0.35, 校验损失：0.32, 校验准确率: 0.89
         第0轮，训练损失：0.31, 校验损失：0.29, 校验准确率: 0.91
         第0轮，训练损失：0.30, 校验损失：0.28, 校验准确率: 0.90
         第1轮，训练损失：0.30, 校验损失：0.28, 校验准确率: 0.90
         第1轮，训练损失：0.29, 校验损失：0.30, 校验准确率: 0.90
         第1轮，训练损失：0.27, 校验损失：0.29, 校验准确率: 0.91
         第1轮，训练损失：0.27, 校验损失：0.28, 校验准确率: 0.91
         第2轮，训练损失：0.26, 校验损失：0.28, 校验准确率: 0.91
         第2轮，训练损失：0.26, 校验损失：0.31, 校验准确率: 0.91
         第2轮，训练损失：0.25, 校验损失：0.30, 校验准确率: 0.90
         第2轮，训练损失：0.25, 校验损失：0.29, 校验准确率: 0.91
         第3轮，训练损失：0.24, 校验损失：0.29, 校验准确率: 0.91
         第3轮，训练损失：0.24, 校验损失：0.32, 校验准确率: 0.90
         第3轮，训练损失：0.23, 校验损失：0.32, 校验准确率: 0.90
         第3轮，训练损失：0.23, 校验损失：0.30, 校验准确率: 0.91
         第4轮，训练损失：0.23, 校验损失：0.30, 校验准确率: 0.90
         第4轮，训练损失：0.23, 校验损失：0.35, 校验准确率: 0.90
         第4轮，训练损失：0.22, 校验损失：0.34, 校验准确率: 0.90
         第4轮，训练损失：0.22, 校验损失：0.31, 校验准确率: 0.91
         第5轮，训练损失：0.22, 校验损失：0.32, 校验准确率: 0.90
         第5轮，训练损失：0.22, 校验损失：0.37, 校验准确率: 0.90
         第5轮，训练损失：0.21, 校验损失：0.35, 校验准确率: 0.91
         第5轮，训练损失：0.21, 校验损失：0.32, 校验准确率: 0.91
         第6轮，训练损失：0.21, 校验损失：0.32, 校验准确率: 0.91
         第6轮，训练损失：0.21, 校验损失：0.39, 校验准确率: 0.90
         第6轮，训练损失：0.21, 校验损失：0.36, 校验准确率: 0.90
         第6轮，训练损失：0.21, 校验损失：0.34, 校验准确率: 0.91
         第7轮，训练损失：0.21, 校验损失：0.33, 校验准确率: 0.90
         第7轮，训练损失：0.20, 校验损失：0.42, 校验准确率: 0.90
         第7轮，训练损失：0.20, 校验损失：0.37, 校验准确率: 0.91
         第7轮，训练损失：0.20, 校验损失：0.36, 校验准确率: 0.91
         第8轮，训练损失：0.20, 校验损失：0.35, 校验准确率: 0.90
         第8轮，训练损失：0.20, 校验损失：0.46, 校验准确率: 0.89
         第8轮，训练损失：0.20, 校验损失：0.41, 校验准确率: 0.90
         第8轮，训练损失：0.19, 校验损失：0.39, 校验准确率: 0.90
         第9轮，训练损失：0.19, 校验损失：0.37, 校验准确率: 0.90
         第9轮，训练损失：0.19, 校验损失：0.45, 校验准确率: 0.90
         第9轮，训练损失：0.19, 校验损失：0.42, 校验准确率: 0.90
         第9轮，训练损失：0.19, 校验损失：0.40, 校验准确率: 0.91
         ```

         绘制训练损失，教研损失，校验准确率训练曲线

         ```python
         # 绘制误差曲线
         a = [i[0] for i in records]
         b = [i[1] for i in records]
         c = [i[2] for i in records]
         fig=plt.figure()
         plt.plot(a, label = 'Train Loss')
         plt.plot(b, label = 'Valid Loss')
         plt.plot(c, label = 'Valid Accuracy')
         plt.xlabel('Steps')
         plt.ylabel('Loss & Accuracy')
         plt.legend()
         fig.savefig("4-1.jpg")
         ```

         ![](.\4-1.jpg)

         

         计算测试集预测准确率

         ```python
         #在测试集上分批运行，并计算总的正确率
         vals = [] #记录准确率所用列表
         
         #对测试数据集进行循环
         for data, target in zip(test_data, test_label):
             data, target = torch.tensor(data, dtype = torch.float).view(1,-1), torch.tensor(np.array([target]), dtype = torch.long)
             output = model(data) #将特征数据喂入网络，得到分类的输出
             val = rightness(output, target) #获得正确样本数以及总样本数
             vals.append(val) #记录结果
         
         #计算准确率
         rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))
         right_rate = 1.0 * rights[0].data.numpy() / rights[1]
         right_rate
         
         0.8925556408288565
         ```

      4. 分析神经网络

         1. 查看每一层的模式

            ```python
            # 将神经网络的架构打印出来，方便后面的访问
            model.named_parameters
            
            <bound method Module.named_parameters of Sequential(
              (0): Linear(in_features=7135, out_features=10, bias=True)
              (1): ReLU()
              (2): Linear(in_features=10, out_features=2, bias=True)
              (3): LogSoftmax(dim=None)
            )>
            ```

            绘制第二个全链接层权重大小

            **可以看出来权重是转置的，偏置是1维的**

            ```python
            # 绘制出第二个全链接层的权重大小
            # model[2]即提取第2层，网络一共4层
            # 第0层为线性神经元，包括weight和bias，第1层为ReLU，
            # 第2层为第二层神经原链接，包括weight和bias，第3层为logsoftmax
            plt.figure(figsize = (10, 7))
            print(model[0].weight.size())
            print(model[0].bias.size())
            # model[2].weight
            for i in range(model[2].weight.size()[0]):
                #if i == 1:
                    weights = model[2].weight[i].data.numpy()
                    plt.plot(weights, 'o-', label = i)
            plt.legend()
            plt.xlabel('Neuron in Hidden Layer')
            plt.ylabel('Weights')
            plt.savefig("4-2.jpg")
            ```

            ![](.\4-2.jpg)

            ```python
            # 将第一层神经元的权重都打印出来，一条曲线表示一个隐含层神经元。横坐标为输入层神经元编号，纵坐标为权重值大小
            plt.figure(figsize = (10, 7))
            print(model[0].weight.size())
            for i in range(model[0].weight.size()[0]):
                #if i == 1:
                    weights = model[0].weight[i].data.numpy()
                    plt.plot(weights, alpha = 0.5, label = i)
            plt.legend()
            plt.xlabel('Neuron in Input Layer')
            plt.ylabel('Weights
            
            torch.Size([10, 7135])           
            ```

            ![](.\4-3.jpg)

            ```python
            # 将第二层的各个神经元与输入层的链接权重，挑出来最大的权重和最小的权重，并考察每一个权重所对应的单词是什么，把单词打印出来
            # model[0]是取出第一层的神经元
            
            for i in range(len(model[0].weight)):
                print('\n')
                print('第{}个神经元'.format(i))
                print('max:')
                #i是序号，w是值
                st = sorted([(w,i) for i,w in enumerate(model[0].weight[i].data.numpy())])
                for i in range(1, 20):
            #         st[][1]返回的是字典中的索引值
                    word = index2word(st[-i][1],diction)
                    print(word)
                print('min:')
                for i in range(20):
                    word = index2word(st[i][1],diction)
                    print(word)
            #只是显示了第一的神经元最敏感和最不敏感的单词
            第0个神经元
            max:
            严重
            破
            不了
            退
            开胶
            没法
            很差
            粗糙
            星期
            坑
            坑人
            差评
            上面
            丢
            不如
            找
            醉
            发错
            好差
            min:
            很棒
            惊喜
            时尚
            物美价廉
            力
            实惠
            光临
            谢谢
            完全一致
            可
            继续
            试穿
            托
            帅气
            不错
            漂亮
            完美
            却是
            吸引
            物超所值
            ```

         2. 寻找判断错误的原因

            ```python
            # 收集到在测试集中判断错误的句子
            # wrong_sentences存放判断出错的句子
            wrong_sentences = []
            # target存放判断出错的目标值
            targets = []
            j = 0
            # 存放判断出错的下标
            sent_indices = []
            for data, target in zip(test_data, test_label):
                predictions = model(torch.tensor(data, dtype = torch.float).view(1,-1))
                pred = torch.max(predictions.data, 1)[1]
                target = torch.tensor(np.array([target]), dtype = torch.long).view_as(pred)
                rights = pred.eq(target)
            #     print(rights,np.where(rights.numpy() == 0)[0])
                indices = np.where(rights.numpy() == 0)[0]
            #     print("\n")
            # 
                for i in indices:
                    wrong_sentences.append(data)
                    targets.append(target[i])
                    sent_indices.append(test_size + j + i)
                j += len(target)
            ```
         
            

   3. 

      

      

      

      

5. 手写数字识别-认识卷积神经网络-认识卷积神经网络

6. 手写数字加法机-迁移学习

7. 你自己的Prisma-图像风格迁移

8. 人工智能造假术--图像生成与对抗学习

9. 词汇星空--神经语言模型与Word2Vec

10. LSTM作曲机-序列生成模型

11. 神经翻译机--端到端机器翻译

12. AI游戏高手--深度强化学习

