1. 深度学习简介

   人工神经网络称为人工智能的连接学派

   1. 感知机到人工神经网络

      + 1957年 弗兰克提出感知机模型（不是直接从功能的角度而是通过结构模拟）
      + 1969年 提出感知机模型连XOR问题无法解出，人工神经网络被打入网络
      + 辛顿发展了人工神经网络的反向传播法，从而可以构造两层以上神经网络，并且可以有效进行学习与训练，两层以上神经网络可以很轻松解决XOR问题。
      + 受限于当时的计算能力，另一方面缺乏大规模高质量的数据，神经网络本身就是一个黑箱，谁也不敢保证深度神经网络在深度这个方向上就能够取得更好的结果和精度。人工神经网络并没有继续沿着深度的方向发展下去。
      + 学术界的焦点朝向了另一个发展方向：寻找神经网络的基础理论。在弗拉基米尔和亚历克塞推进下，统计学习理论蓬勃发展，奠定了模式识别的数学基础，创造出了支持向量机这种极其实用简单的工具。与传统神经网络通过加深网络来提升精度相反，支持向量机的解决方案是将数据的维度提升，在高维种寻找能够将数据进行准确划分的方法，这种方法在数据量不是很大情况下很奏效。支持向量机称为20世纪90年代到21世纪初的宠儿。
      + 2006年 辛顿发表题为《利用神经网络进行数据的维度约减》的文章，提出了深度神经网络模型（DNN），并指出如果我们能够将神经网络的层数加深，并且精心设计训练网络的方式，那么这样深层次的神经网络就会具有超强的表达能力和学习能力。

   2. 深度学习时代

      + 深度学习首先在语音领域取得突破。微软的邓力邀请辛顿加入语音识别的深度神经网络模型的开发，使识别的准确度有了大幅的提升。
      + 机器视觉专家李飞飞，借助网友的力量构造出了ImageNet这样一个大规模、高精度、多标签的图像数据库。于是李飞飞开始每年举办一次图像识别大赛：ImageNet竞赛。2012年，辛顿和他的两个学生采用深层次的卷积神经网络（AlexNet）,在ImageNet竞赛中将分类错误率从25%降到了17%。将深度网络做到8层，且不需要任何预处理就能将图像分类任务做到这么好，还是头一次。深度神经网络称为了ImageNet竞赛的标配，从AlexNet到GoogleNet，网络深度不断增加，准确率不断提升。2012年后深度学习开始在学术圈流行起来。

   3. 深度学习发展

      + 2011年 谷歌X实验室的杰夫和吴恩达等人采用深度学习技术，让谷歌大脑深度神经网络观看了从Youtube上提取出来的30万张图像，并让机器自动进行精简，谷歌大脑自己学出了一张猫脸。看到发展前景后，以谷歌未代表的各大公司开始疯狂并购人工智能、人工智能初创公司和团队，促使更多的人才和创业公司投入到人工智能大潮中。

      + 自然语言处理领域，2013年，谷歌的托马斯提出了Word2Vec技术，它可以非常快捷有效地计算单词间的向量表示，为大规模使用人工神经网络技术处理人类语言奠定了重要基础

      + 2014年，谷歌开始尝试利用深度的神经循环网络来处理各种自然语言任务，包括机器翻译，自动对话，情绪识别、阅读理解等。2016年，谷歌机器翻译技术取得重大突破，采用了先进的深度循环神经网络和注意力机智的机器翻译在多种语言上已经基本接近人类水平。

      + 强化学习与深度学习的结合，深度学习在计算机游戏、博弈等领域同样取得了重大进展。

        2015年被谷歌收购的DeepMind团队研发了一种“通用人工智能算法”，他可以像人类一样，通过观察计算机游戏屏幕进行自我学习，利用同一套网络架构和超参数，从零开始学习每一款游戏，并最终打通了300多款雅达利游戏，在某些游戏上的表现甚至超越了人类。

      + 2016年 DeepMind团队又在博弈领域取得了重大突破。AlphaGo以4:1战胜人类围棋冠军。2017年 DeepMind团队创造的 AlphaGo 升级版 AlphaGo Zero ，它可以完全从零开始学习下围棋，而无需借鉴任何人类的下棋经验。仅经过大约3天训练，AlphaGo Zero就达到了战胜李世石的棋力水平。而到了21天后，世界上已经没有任何人类或程序可以在围棋上战胜它了。AlphaGo 的成功不仅标志着以深度学习技术为支撑的新一代人工智能技术大获全胜，更暗示着人工智能全新时代的来临。  

   4. 深度学习的影响因素

      1. 大数据

         如果没有足够大量的数据输入给深度神经网络，就无法发挥深度的作用。伴随着网络深度的增加，待拟合的参数自然会也会增加，如果没有与其相匹配的海量数据来训练网络，这些参数就完全变成了导致网络过拟合的垃圾，无法发挥作用

      2. 深度网络架构（整个网络体系的构建方式和拓扑连接结构）

         面对具体问题时，应该采用什么样的网络架构，如何选取参数，如何训练这个网络，仍然是影响学习效率和解决问题的重要因素。

         目前主要分为： 

         + 前馈神经网络  

           每一层的节点只跟它相邻层节点而且是全部节点相连（全连接的），分为输入层，隐藏层，输出层

         + 卷积神经网络

           卷积层和池化层

           图中每一个立方体都是一系列规则排列的人工神经元的集合。

           每个神经元到上一层次的连接称为卷积核，它们都是一种局域的小窗口。

           图中的小锥形可以理解为从高层的某一个神经元到低层多个神经元之间的连接。这个小锥形在立方体上逐像素的平移就构成了两层次之间的所有连接。到了最后两层，小立方体被压缩成了一个一维的向量，这就与普通的前馈神经网络没有区别。

           CNN这种特殊的架构可以很好地应用于图像处理，它可以使原始的图像即使在经历过平移、缩放等变换后仍然具有很高的识别准确性。正是因为具有这样特殊的架构，CNN才成功应用于计算机视觉、图像识别、图像生成，甚至AI下围棋、AI打游戏等广阔领域

         + 循环神经网络
      
           输入层，输出层是单层，中间的隐藏层的节点相互连接。隐藏层彼此之间还有大量的连接。
      
           RNN这种特殊的架构使得网络当前的运行不仅跟当前的输入数据有关，而且还与之前的数据有关。因此，这种网络特别适合处理诸如语言、音乐、股票曲线等序列类型的数据，整个网络的循环结构可以很好地应付输入序列之中存在的长程记忆性和周期性
      
         + 训练方式
      
           训练方式也会对结果产生很大影响。如果先将少量特定标签的数据输入网络，然后再拿剩下的数据去训练它，就会比一股脑把所有便签数据都输入更加有效，从而提高网络的“学习”能力。
      
      3. GPU 
      
         GPU非常擅长大规模的张量（高阶矩阵）运算，并且可以为这种运算加速，对包含多个数值的张量运算所需的平均时间远低于对每个数字的运算时间
      
   5. 深度学习的成功

      深度学习重要的本领在于它可以从海量的数据中自动学习，抽取数据中的特征。

      + 特征学习

        深度神经网络会把不同的信息表达到不同层次的网络单元中，这一提炼过程完全不需要手工干预，全凭机器学习过程自动完成。深度学习的本质就是这种自动提取特征的功能。

      + 迁移学习

2. PyTorch简介

3. 单车预测器：你的第一个神经网络

4. 机器也懂得感情-中文情绪分类器

5. 手写数字识别-认识卷积神经网络-认识卷积神经网络

6. 手写数字加法机-迁移学习

7. 你自己的Prisma-图像风格迁移

8. 人工智能造假术--图像生成与对抗学习

9. 词汇星空--神经语言模型与Word2Vec

10. LSTM作曲机-序列生成模型

11. 神经翻译机--端到端机器翻译

12. AI游戏高手--深度强化学习

