1. æ·±åº¦å­¦ä¹ ç®€ä»‹

   äººå·¥ç¥ç»ç½‘ç»œç§°ä¸ºäººå·¥æ™ºèƒ½çš„è¿æ¥å­¦æ´¾

   1. æ„ŸçŸ¥æœºåˆ°äººå·¥ç¥ç»ç½‘ç»œ

      + 1957å¹´ å¼—å…°å…‹æå‡ºæ„ŸçŸ¥æœºæ¨¡å‹ï¼ˆä¸æ˜¯ç›´æ¥ä»åŠŸèƒ½çš„è§’åº¦è€Œæ˜¯é€šè¿‡ç»“æ„æ¨¡æ‹Ÿï¼‰
      + 1969å¹´ æå‡ºæ„ŸçŸ¥æœºæ¨¡å‹è¿XORé—®é¢˜æ— æ³•è§£å‡ºï¼Œäººå·¥ç¥ç»ç½‘ç»œè¢«æ‰“å…¥ç½‘ç»œ
      + è¾›é¡¿å‘å±•äº†äººå·¥ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­æ³•ï¼Œä»è€Œå¯ä»¥æ„é€ ä¸¤å±‚ä»¥ä¸Šç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆè¿›è¡Œå­¦ä¹ ä¸è®­ç»ƒï¼Œä¸¤å±‚ä»¥ä¸Šç¥ç»ç½‘ç»œå¯ä»¥å¾ˆè½»æ¾è§£å†³XORé—®é¢˜ã€‚
      + å—é™äºå½“æ—¶çš„è®¡ç®—èƒ½åŠ›ï¼Œå¦ä¸€æ–¹é¢ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„æ•°æ®ï¼Œç¥ç»ç½‘ç»œæœ¬èº«å°±æ˜¯ä¸€ä¸ªé»‘ç®±ï¼Œè°ä¹Ÿä¸æ•¢ä¿è¯æ·±åº¦ç¥ç»ç½‘ç»œåœ¨æ·±åº¦è¿™ä¸ªæ–¹å‘ä¸Šå°±èƒ½å¤Ÿå–å¾—æ›´å¥½çš„ç»“æœå’Œç²¾åº¦ã€‚äººå·¥ç¥ç»ç½‘ç»œå¹¶æ²¡æœ‰ç»§ç»­æ²¿ç€æ·±åº¦çš„æ–¹å‘å‘å±•ä¸‹å»ã€‚
      + å­¦æœ¯ç•Œçš„ç„¦ç‚¹æœå‘äº†å¦ä¸€ä¸ªå‘å±•æ–¹å‘ï¼šå¯»æ‰¾ç¥ç»ç½‘ç»œçš„åŸºç¡€ç†è®ºã€‚åœ¨å¼—æ‹‰åŸºç±³å°”å’Œäºšå†å…‹å¡æ¨è¿›ä¸‹ï¼Œç»Ÿè®¡å­¦ä¹ ç†è®ºè“¬å‹ƒå‘å±•ï¼Œå¥ å®šäº†æ¨¡å¼è¯†åˆ«çš„æ•°å­¦åŸºç¡€ï¼Œåˆ›é€ å‡ºäº†æ”¯æŒå‘é‡æœºè¿™ç§æå…¶å®ç”¨ç®€å•çš„å·¥å…·ã€‚ä¸ä¼ ç»Ÿç¥ç»ç½‘ç»œé€šè¿‡åŠ æ·±ç½‘ç»œæ¥æå‡ç²¾åº¦ç›¸åï¼Œæ”¯æŒå‘é‡æœºçš„è§£å†³æ–¹æ¡ˆæ˜¯å°†æ•°æ®çš„ç»´åº¦æå‡ï¼Œåœ¨é«˜ç»´ç§å¯»æ‰¾èƒ½å¤Ÿå°†æ•°æ®è¿›è¡Œå‡†ç¡®åˆ’åˆ†çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•°æ®é‡ä¸æ˜¯å¾ˆå¤§æƒ…å†µä¸‹å¾ˆå¥æ•ˆã€‚æ”¯æŒå‘é‡æœºç§°ä¸º20ä¸–çºª90å¹´ä»£åˆ°21ä¸–çºªåˆçš„å® å„¿ã€‚
      + 2006å¹´ è¾›é¡¿å‘è¡¨é¢˜ä¸ºã€Šåˆ©ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ•°æ®çš„ç»´åº¦çº¦å‡ã€‹çš„æ–‡ç« ï¼Œæå‡ºäº†æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆDNNï¼‰ï¼Œå¹¶æŒ‡å‡ºå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå°†ç¥ç»ç½‘ç»œçš„å±‚æ•°åŠ æ·±ï¼Œå¹¶ä¸”ç²¾å¿ƒè®¾è®¡è®­ç»ƒç½‘ç»œçš„æ–¹å¼ï¼Œé‚£ä¹ˆè¿™æ ·æ·±å±‚æ¬¡çš„ç¥ç»ç½‘ç»œå°±ä¼šå…·æœ‰è¶…å¼ºçš„è¡¨è¾¾èƒ½åŠ›å’Œå­¦ä¹ èƒ½åŠ›ã€‚

   2. æ·±åº¦å­¦ä¹ æ—¶ä»£

      + æ·±åº¦å­¦ä¹ é¦–å…ˆåœ¨è¯­éŸ³é¢†åŸŸå–å¾—çªç ´ã€‚å¾®è½¯çš„é‚“åŠ›é‚€è¯·è¾›é¡¿åŠ å…¥è¯­éŸ³è¯†åˆ«çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„å¼€å‘ï¼Œä½¿è¯†åˆ«çš„å‡†ç¡®åº¦æœ‰äº†å¤§å¹…çš„æå‡ã€‚
      + æœºå™¨è§†è§‰ä¸“å®¶æé£é£ï¼Œå€ŸåŠ©ç½‘å‹çš„åŠ›é‡æ„é€ å‡ºäº†ImageNetè¿™æ ·ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜ç²¾åº¦ã€å¤šæ ‡ç­¾çš„å›¾åƒæ•°æ®åº“ã€‚äºæ˜¯æé£é£å¼€å§‹æ¯å¹´ä¸¾åŠä¸€æ¬¡å›¾åƒè¯†åˆ«å¤§èµ›ï¼šImageNetç«èµ›ã€‚2012å¹´ï¼Œè¾›é¡¿å’Œä»–çš„ä¸¤ä¸ªå­¦ç”Ÿé‡‡ç”¨æ·±å±‚æ¬¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆAlexNetï¼‰,åœ¨ImageNetç«èµ›ä¸­å°†åˆ†ç±»é”™è¯¯ç‡ä»25%é™åˆ°äº†17%ã€‚å°†æ·±åº¦ç½‘ç»œåšåˆ°8å±‚ï¼Œä¸”ä¸éœ€è¦ä»»ä½•é¢„å¤„ç†å°±èƒ½å°†å›¾åƒåˆ†ç±»ä»»åŠ¡åšåˆ°è¿™ä¹ˆå¥½ï¼Œè¿˜æ˜¯å¤´ä¸€æ¬¡ã€‚æ·±åº¦ç¥ç»ç½‘ç»œç§°ä¸ºäº†ImageNetç«èµ›çš„æ ‡é…ï¼Œä»AlexNetåˆ°GoogleNetï¼Œç½‘ç»œæ·±åº¦ä¸æ–­å¢åŠ ï¼Œå‡†ç¡®ç‡ä¸æ–­æå‡ã€‚2012å¹´åæ·±åº¦å­¦ä¹ å¼€å§‹åœ¨å­¦æœ¯åœˆæµè¡Œèµ·æ¥ã€‚

   3. æ·±åº¦å­¦ä¹ å‘å±•

      + 2011å¹´ è°·æ­ŒXå®éªŒå®¤çš„æ°å¤«å’Œå´æ©è¾¾ç­‰äººé‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè®©è°·æ­Œå¤§è„‘æ·±åº¦ç¥ç»ç½‘ç»œè§‚çœ‹äº†ä»Youtubeä¸Šæå–å‡ºæ¥çš„30ä¸‡å¼ å›¾åƒï¼Œå¹¶è®©æœºå™¨è‡ªåŠ¨è¿›è¡Œç²¾ç®€ï¼Œè°·æ­Œå¤§è„‘è‡ªå·±å­¦å‡ºäº†ä¸€å¼ çŒ«è„¸ã€‚çœ‹åˆ°å‘å±•å‰æ™¯åï¼Œä»¥è°·æ­Œæœªä»£è¡¨çš„å„å¤§å…¬å¸å¼€å§‹ç–¯ç‹‚å¹¶è´­äººå·¥æ™ºèƒ½ã€äººå·¥æ™ºèƒ½åˆåˆ›å…¬å¸å’Œå›¢é˜Ÿï¼Œä¿ƒä½¿æ›´å¤šçš„äººæ‰å’Œåˆ›ä¸šå…¬å¸æŠ•å…¥åˆ°äººå·¥æ™ºèƒ½å¤§æ½®ä¸­ã€‚

      + è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œ2013å¹´ï¼Œè°·æ­Œçš„æ‰˜é©¬æ–¯æå‡ºäº†Word2VecæŠ€æœ¯ï¼Œå®ƒå¯ä»¥éå¸¸å¿«æ·æœ‰æ•ˆåœ°è®¡ç®—å•è¯é—´çš„å‘é‡è¡¨ç¤ºï¼Œä¸ºå¤§è§„æ¨¡ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæŠ€æœ¯å¤„ç†äººç±»è¯­è¨€å¥ å®šäº†é‡è¦åŸºç¡€

      + 2014å¹´ï¼Œè°·æ­Œå¼€å§‹å°è¯•åˆ©ç”¨æ·±åº¦çš„ç¥ç»å¾ªç¯ç½‘ç»œæ¥å¤„ç†å„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ï¼Œè‡ªåŠ¨å¯¹è¯ï¼Œæƒ…ç»ªè¯†åˆ«ã€é˜…è¯»ç†è§£ç­‰ã€‚2016å¹´ï¼Œè°·æ­Œæœºå™¨ç¿»è¯‘æŠ€æœ¯å–å¾—é‡å¤§çªç ´ï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œå’Œæ³¨æ„åŠ›æœºæ™ºçš„æœºå™¨ç¿»è¯‘åœ¨å¤šç§è¯­è¨€ä¸Šå·²ç»åŸºæœ¬æ¥è¿‘äººç±»æ°´å¹³ã€‚

      + å¼ºåŒ–å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ çš„ç»“åˆï¼Œæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºæ¸¸æˆã€åšå¼ˆç­‰é¢†åŸŸåŒæ ·å–å¾—äº†é‡å¤§è¿›å±•ã€‚

        2015å¹´è¢«è°·æ­Œæ”¶è´­çš„DeepMindå›¢é˜Ÿç ”å‘äº†ä¸€ç§â€œé€šç”¨äººå·¥æ™ºèƒ½ç®—æ³•â€ï¼Œä»–å¯ä»¥åƒäººç±»ä¸€æ ·ï¼Œé€šè¿‡è§‚å¯Ÿè®¡ç®—æœºæ¸¸æˆå±å¹•è¿›è¡Œè‡ªæˆ‘å­¦ä¹ ï¼Œåˆ©ç”¨åŒä¸€å¥—ç½‘ç»œæ¶æ„å’Œè¶…å‚æ•°ï¼Œä»é›¶å¼€å§‹å­¦ä¹ æ¯ä¸€æ¬¾æ¸¸æˆï¼Œå¹¶æœ€ç»ˆæ‰“é€šäº†300å¤šæ¬¾é›…è¾¾åˆ©æ¸¸æˆï¼Œåœ¨æŸäº›æ¸¸æˆä¸Šçš„è¡¨ç°ç”šè‡³è¶…è¶Šäº†äººç±»ã€‚

      + 2016å¹´ DeepMindå›¢é˜Ÿåˆåœ¨åšå¼ˆé¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚AlphaGoä»¥4:1æˆ˜èƒœäººç±»å›´æ£‹å† å†›ã€‚2017å¹´ DeepMindå›¢é˜Ÿåˆ›é€ çš„ AlphaGo å‡çº§ç‰ˆ AlphaGo Zero ï¼Œå®ƒå¯ä»¥å®Œå…¨ä»é›¶å¼€å§‹å­¦ä¹ ä¸‹å›´æ£‹ï¼Œè€Œæ— éœ€å€Ÿé‰´ä»»ä½•äººç±»çš„ä¸‹æ£‹ç»éªŒã€‚ä»…ç»è¿‡å¤§çº¦3å¤©è®­ç»ƒï¼ŒAlphaGo Zeroå°±è¾¾åˆ°äº†æˆ˜èƒœæä¸–çŸ³çš„æ£‹åŠ›æ°´å¹³ã€‚è€Œåˆ°äº†21å¤©åï¼Œä¸–ç•Œä¸Šå·²ç»æ²¡æœ‰ä»»ä½•äººç±»æˆ–ç¨‹åºå¯ä»¥åœ¨å›´æ£‹ä¸Šæˆ˜èƒœå®ƒäº†ã€‚AlphaGo çš„æˆåŠŸä¸ä»…æ ‡å¿—ç€ä»¥æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ºæ”¯æ’‘çš„æ–°ä¸€ä»£äººå·¥æ™ºèƒ½æŠ€æœ¯å¤§è·å…¨èƒœï¼Œæ›´æš—ç¤ºç€äººå·¥æ™ºèƒ½å…¨æ–°æ—¶ä»£çš„æ¥ä¸´ã€‚  

   4. æ·±åº¦å­¦ä¹ çš„å½±å“å› ç´ 

      1. å¤§æ•°æ®

         å¦‚æœæ²¡æœ‰è¶³å¤Ÿå¤§é‡çš„æ•°æ®è¾“å…¥ç»™æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå°±æ— æ³•å‘æŒ¥æ·±åº¦çš„ä½œç”¨ã€‚ä¼´éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ ï¼Œå¾…æ‹Ÿåˆçš„å‚æ•°è‡ªç„¶ä¼šä¹Ÿä¼šå¢åŠ ï¼Œå¦‚æœæ²¡æœ‰ä¸å…¶ç›¸åŒ¹é…çš„æµ·é‡æ•°æ®æ¥è®­ç»ƒç½‘ç»œï¼Œè¿™äº›å‚æ•°å°±å®Œå…¨å˜æˆäº†å¯¼è‡´ç½‘ç»œè¿‡æ‹Ÿåˆçš„åƒåœ¾ï¼Œæ— æ³•å‘æŒ¥ä½œç”¨

      2. æ·±åº¦ç½‘ç»œæ¶æ„ï¼ˆæ•´ä¸ªç½‘ç»œä½“ç³»çš„æ„å»ºæ–¹å¼å’Œæ‹“æ‰‘è¿æ¥ç»“æ„ï¼‰

         é¢å¯¹å…·ä½“é—®é¢˜æ—¶ï¼Œåº”è¯¥é‡‡ç”¨ä»€ä¹ˆæ ·çš„ç½‘ç»œæ¶æ„ï¼Œå¦‚ä½•é€‰å–å‚æ•°ï¼Œå¦‚ä½•è®­ç»ƒè¿™ä¸ªç½‘ç»œï¼Œä»ç„¶æ˜¯å½±å“å­¦ä¹ æ•ˆç‡å’Œè§£å†³é—®é¢˜çš„é‡è¦å› ç´ ã€‚

         ç›®å‰ä¸»è¦åˆ†ä¸ºï¼š 

         + å‰é¦ˆç¥ç»ç½‘ç»œ  

           æ¯ä¸€å±‚çš„èŠ‚ç‚¹åªè·Ÿå®ƒç›¸é‚»å±‚èŠ‚ç‚¹è€Œä¸”æ˜¯å…¨éƒ¨èŠ‚ç‚¹ç›¸è¿ï¼ˆå…¨è¿æ¥çš„ï¼‰ï¼Œåˆ†ä¸ºè¾“å…¥å±‚ï¼Œéšè—å±‚ï¼Œè¾“å‡ºå±‚

         + å·ç§¯ç¥ç»ç½‘ç»œ

           å·ç§¯å±‚å’Œæ± åŒ–å±‚

           å›¾ä¸­æ¯ä¸€ä¸ªç«‹æ–¹ä½“éƒ½æ˜¯ä¸€ç³»åˆ—è§„åˆ™æ’åˆ—çš„äººå·¥ç¥ç»å…ƒçš„é›†åˆã€‚

           æ¯ä¸ªç¥ç»å…ƒåˆ°ä¸Šä¸€å±‚æ¬¡çš„è¿æ¥ç§°ä¸ºå·ç§¯æ ¸ï¼Œå®ƒä»¬éƒ½æ˜¯ä¸€ç§å±€åŸŸçš„å°çª—å£ã€‚

           å›¾ä¸­çš„å°é”¥å½¢å¯ä»¥ç†è§£ä¸ºä»é«˜å±‚çš„æŸä¸€ä¸ªç¥ç»å…ƒåˆ°ä½å±‚å¤šä¸ªç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ã€‚è¿™ä¸ªå°é”¥å½¢åœ¨ç«‹æ–¹ä½“ä¸Šé€åƒç´ çš„å¹³ç§»å°±æ„æˆäº†ä¸¤å±‚æ¬¡ä¹‹é—´çš„æ‰€æœ‰è¿æ¥ã€‚åˆ°äº†æœ€åä¸¤å±‚ï¼Œå°ç«‹æ–¹ä½“è¢«å‹ç¼©æˆäº†ä¸€ä¸ªä¸€ç»´çš„å‘é‡ï¼Œè¿™å°±ä¸æ™®é€šçš„å‰é¦ˆç¥ç»ç½‘ç»œæ²¡æœ‰åŒºåˆ«ã€‚

           CNNè¿™ç§ç‰¹æ®Šçš„æ¶æ„å¯ä»¥å¾ˆå¥½åœ°åº”ç”¨äºå›¾åƒå¤„ç†ï¼Œå®ƒå¯ä»¥ä½¿åŸå§‹çš„å›¾åƒå³ä½¿åœ¨ç»å†è¿‡å¹³ç§»ã€ç¼©æ”¾ç­‰å˜æ¢åä»ç„¶å…·æœ‰å¾ˆé«˜çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚æ­£æ˜¯å› ä¸ºå…·æœ‰è¿™æ ·ç‰¹æ®Šçš„æ¶æ„ï¼ŒCNNæ‰æˆåŠŸåº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€å›¾åƒè¯†åˆ«ã€å›¾åƒç”Ÿæˆï¼Œç”šè‡³AIä¸‹å›´æ£‹ã€AIæ‰“æ¸¸æˆç­‰å¹¿é˜”é¢†åŸŸ

         + å¾ªç¯ç¥ç»ç½‘ç»œ
      
           è¾“å…¥å±‚ï¼Œè¾“å‡ºå±‚æ˜¯å•å±‚ï¼Œä¸­é—´çš„éšè—å±‚çš„èŠ‚ç‚¹ç›¸äº’è¿æ¥ã€‚éšè—å±‚å½¼æ­¤ä¹‹é—´è¿˜æœ‰å¤§é‡çš„è¿æ¥ã€‚
      
           RNNè¿™ç§ç‰¹æ®Šçš„æ¶æ„ä½¿å¾—ç½‘ç»œå½“å‰çš„è¿è¡Œä¸ä»…è·Ÿå½“å‰çš„è¾“å…¥æ•°æ®æœ‰å…³ï¼Œè€Œä¸”è¿˜ä¸ä¹‹å‰çš„æ•°æ®æœ‰å…³ã€‚å› æ­¤ï¼Œè¿™ç§ç½‘ç»œç‰¹åˆ«é€‚åˆå¤„ç†è¯¸å¦‚è¯­è¨€ã€éŸ³ä¹ã€è‚¡ç¥¨æ›²çº¿ç­‰åºåˆ—ç±»å‹çš„æ•°æ®ï¼Œæ•´ä¸ªç½‘ç»œçš„å¾ªç¯ç»“æ„å¯ä»¥å¾ˆå¥½åœ°åº”ä»˜è¾“å…¥åºåˆ—ä¹‹ä¸­å­˜åœ¨çš„é•¿ç¨‹è®°å¿†æ€§å’Œå‘¨æœŸæ€§
      
         + è®­ç»ƒæ–¹å¼
      
           è®­ç»ƒæ–¹å¼ä¹Ÿä¼šå¯¹ç»“æœäº§ç”Ÿå¾ˆå¤§å½±å“ã€‚å¦‚æœå…ˆå°†å°‘é‡ç‰¹å®šæ ‡ç­¾çš„æ•°æ®è¾“å…¥ç½‘ç»œï¼Œç„¶åå†æ‹¿å‰©ä¸‹çš„æ•°æ®å»è®­ç»ƒå®ƒï¼Œå°±ä¼šæ¯”ä¸€è‚¡è„‘æŠŠæ‰€æœ‰ä¾¿ç­¾æ•°æ®éƒ½è¾“å…¥æ›´åŠ æœ‰æ•ˆï¼Œä»è€Œæé«˜ç½‘ç»œçš„â€œå­¦ä¹ â€èƒ½åŠ›ã€‚
      
      3. GPU 
      
         GPUéå¸¸æ“…é•¿å¤§è§„æ¨¡çš„å¼ é‡ï¼ˆé«˜é˜¶çŸ©é˜µï¼‰è¿ç®—ï¼Œå¹¶ä¸”å¯ä»¥ä¸ºè¿™ç§è¿ç®—åŠ é€Ÿï¼Œå¯¹åŒ…å«å¤šä¸ªæ•°å€¼çš„å¼ é‡è¿ç®—æ‰€éœ€çš„å¹³å‡æ—¶é—´è¿œä½äºå¯¹æ¯ä¸ªæ•°å­—çš„è¿ç®—æ—¶é—´
      
   5. æ·±åº¦å­¦ä¹ çš„æˆåŠŸ

      æ·±åº¦å­¦ä¹ é‡è¦çš„æœ¬é¢†åœ¨äºå®ƒå¯ä»¥ä»æµ·é‡çš„æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ ï¼ŒæŠ½å–æ•°æ®ä¸­çš„ç‰¹å¾ã€‚

      + ç‰¹å¾å­¦ä¹ 

        æ·±åº¦ç¥ç»ç½‘ç»œä¼šæŠŠä¸åŒçš„ä¿¡æ¯è¡¨è¾¾åˆ°ä¸åŒå±‚æ¬¡çš„ç½‘ç»œå•å…ƒä¸­ï¼Œè¿™ä¸€æç‚¼è¿‡ç¨‹å®Œå…¨ä¸éœ€è¦æ‰‹å·¥å¹²é¢„ï¼Œå…¨å‡­æœºå™¨å­¦ä¹ è¿‡ç¨‹è‡ªåŠ¨å®Œæˆã€‚æ·±åº¦å­¦ä¹ çš„æœ¬è´¨å°±æ˜¯è¿™ç§è‡ªåŠ¨æå–ç‰¹å¾çš„åŠŸèƒ½ã€‚

      + è¿ç§»å­¦ä¹ 

        æŠŠä¸€ä¸ªè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œåˆ‡å¼€ï¼Œç„¶åå†æŠŠå®ƒæ‹¼æ¥åˆ°å¦ä¸€ä¸ªç¥ç»ç½‘ç»œä¸Šï¼Œå‰åŠéƒ¨åˆ†ç”¨äºç‰¹å¾æå–ï¼ŒååŠéƒ¨åˆ†ç½‘ç»œå»è§£å†³å¦ä¸€ä¸ªå®Œå…¨ä¸åŒçš„é—®é¢˜ã€‚

2. PyTorchç®€ä»‹

   1. PyTorchå®‰è£…

   2. ä¸Pythonå®Œç¾èåˆ

      ä½¿ç”¨Pytorchä¸ä½¿ç”¨å…¶ä»–Pythonç¨‹åºåŒ…æ²¡æœ‰ä»»ä½•åŒºåˆ«
      
      ä¸æ­¤å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯TensorFlowï¼ŒTensorFlowä¼šå°†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä»»åŠ¡åˆ†ä¸ºå®šä¹‰ä¸ºè®¡ç®—å›¾å’Œæ‰§è¡Œè®¡ç®—è¿‡ç¨‹ï¼Œè€Œå®šä¹‰è®¡ç®—å›¾çš„è¿‡ç¨‹å°±å¥½åƒåœ¨ä½¿ç”¨ä¸€å¥—å…¨æ–°çš„è¯­è¨€ã€‚PyTorchå°±æ²¡æœ‰è¿™ä¸ªç¼ºç‚¹ï¼Œä»å®šä¹‰è®¡ç®—å›¾åˆ°æ‰§è¡Œè®¡ç®—æ˜¯ä¸€æ°”å‘µæˆçš„ã€‚
      
   3. å¼ é‡ï¼ˆtensorï¼‰è®¡ç®—

      PyTorchçš„è¿ç®—å•å…ƒå«åšå¼ é‡ï¼Œ1é˜¶å¼ é‡å³ä¸º1ç»´æ•°ç»„ï¼ˆå‘é‡vectorï¼‰ï¼Œ2é˜¶å¼ é‡ä¸º2ç»´æ•°ç»„ï¼ˆçŸ©é˜µmatrixï¼‰ï¼Œ3é˜¶å¼ é‡å³ä¸º3ç»´æ•°ç»„

      ```python
      import torch
      x=torch.rand(5,3)
      x
      tensor([[0.1440, 0.7954, 0.8075],
              [0.1011, 0.0410, 0.4783],
              [0.1711, 0.4874, 0.0733],
              [0.8815, 0.3930, 0.4344],
              [0.8639, 0.1563, 0.8560]])
      
      ```
      
      ```python
      y=torch.ones(5,3)
      y
      tensor([[1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.],
              [1., 1., 1.]])
      ```
      
      ```python
      # pytorhçŸ©é˜µå‰ä¹˜æ³• mm() ,è½¬ç½®t()
      q=x.mm(y.t())
      q
      tensor([[1.7469, 1.7469, 1.7469, 1.7469, 1.7469],
              [0.6204, 0.6204, 0.6204, 0.6204, 0.6204],
              [0.7319, 0.7319, 0.7319, 0.7319, 0.7319],
              [1.7089, 1.7089, 1.7089, 1.7089, 1.7089],
              [1.8762, 1.8762, 1.8762, 1.8762, 1.8762]])
      ```
      
      ```python
      # pytorhå¼ é‡ä¸Numpyæ•°ç»„ä¹‹é—´çš„è½¬æ¢
      import numpy as np
      x_tensor=torch.randn(2,3)
      y_numpy=np.random.randn(2,3)
      # å°†å¼ é‡è½¬æ¢ä¸ºnumpy
      x_numpy=x_tensor.numpy()
      # å°†numpyè½¬åŒ–ä¸ºå¼ é‡
      y_tensor=torch.from_numpy(y_numpy)
      print(x_tensor)
      print(x_numpy)
      print(y_numpy)
      print(y_tensor)
      
      tensor([[ 1.6723,  0.8274,  1.4538],
              [ 0.5076, -1.6918,  0.1204]])
      [[ 1.672333    0.8274111   1.453827  ]
       [ 0.5075982  -1.691759    0.12042859]]
      [[-1.53580661  1.05183357 -0.13190343]
       [ 0.26008687 -0.76392427  0.49671979]]
      tensor([[-1.5358,  1.0518, -0.1319],
              [ 0.2601, -0.7639,  0.4967]], dtype=torch.float64)
      ```
      
      ```python
      # GPUä¸Šçš„å¼ é‡è®¡ç®—
      if torch.cuda.is_available():
          x=x.cuda()
          y=y.cuda()
          print(x+y)
          
      tensor([[1.1440, 1.7954, 1.8075],
              [1.1011, 1.0410, 1.4783],
              [1.1711, 1.4874, 1.0733],
              [1.8815, 1.3930, 1.4344],
              [1.8639, 1.1563, 1.8560]], device='cuda:0')
      ```
      
   4. åŠ¨æ€è®¡ç®—å›¾

      è®¡ç®—å›¾ç”¨äºè§£å†³ç”¨äºè§£å†³åå‘ä¼ æ’­ç®—æ³•é—®é¢˜ã€‚å½“å‰é¦ˆè¿ç®—æ­¥éª¤å®Œæˆä¹‹åï¼Œæ·±åº¦å­¦ä¹ æ¡†æ¶å°±ä¼šè‡ªåŠ¨æ­å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼Œé€šè¿‡è¿™ä¸ªå›¾ï¼Œå°±å¯ä»¥è®©åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œã€‚

      è‡ªåŠ¨å¾®åˆ†å˜é‡æ˜¯é€šè¿‡3ä¸ªé‡è¦çš„å±æ€§dataï¼ˆå¼ é‡ï¼‰ã€gradï¼ˆæ¢¯åº¦å€¼ï¼‰ä»¥åŠgrad_fnï¼ˆè·å¾—è®¡ç®—å›¾çš„ä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼‰æ¥å®ç°çš„ã€‚åœ¨é‡‡ç”¨äº†è‡ªåŠ¨å¾®åˆ†å˜é‡ä»¥åï¼Œæ— è®ºä¸€ä¸ªè®¡ç®—è¿‡ç¨‹å¤šä¹ˆå¤æ‚ï¼Œç³»ç»Ÿéƒ½ä¼šè‡ªåŠ¨æ„é€ ä¸€å¼ è®¡ç®—å›¾æ¥è®°å½•æ‰€æœ‰çš„è¿ç®—è¿‡ç¨‹ã€‚

      ```python
      # å¯¼å…¥è‡ªåŠ¨å¾®åˆ†å˜é‡çš„åŒ…
      # from torch.autograd import Variable
      # requires_grad=Trueæ˜¯ä¸ºäº†ä¿è¯åœ¨åå‘ä¼ æ’­ç®—æ³•ä¸­è·å¾—æ¢¯åº¦ä¿¡æ¯
      # x=Variable(torch.ones(2,2),requires_grad=True) Variableè¢«åºŸå¼ƒ
      x=torch.ones(2,2,requires_grad=True)
      x
      tensor([[1., 1.],
              [1., 1.]], requires_grad=True)
      ```

      ```python
      y=x+2
      y
      tensor([[3., 3.],
              [3., 3.]], grad_fn=<AddBackward0>)
      ```

      ```python
      y.data
      tensor([[3., 3.],
              [3., 3.]])
      ```

      ```python
      #è¿”å›ä¸Šä¸€ä¸ªè®¡ç®—å›¾èŠ‚ç‚¹
      y.grad_fn
      <AddBackward0 at 0x1df0dc7f640>
      ```

      ```python
      # *æ˜¯ç‚¹ä¹˜ï¼Œåªæœ‰ç›¸åŒä½ç½®çš„å…ƒç´ ç›¸ä¹˜
      z=y*y
      z
      tensor([[9., 9.],
              [9., 9.]], grad_fn=<MulBackward0>)
      ```

      ```python
      z.grad_fn
      <MulBackward0 at 0x1df0dc9b280>
      ```

      ```python
      # torch.meanå¯¹çŸ©é˜µçš„æ¯ä¸ªå…ƒç´ æ±‚å’Œå†é™¤ä»¥å…ƒç´ çš„ä¸ªæ•°ã€‚
      t=torch.mean(z)
      t
      tensor(9., grad_fn=<MeanBackward0>)
      ```

      ```python
      # backward()åå‘æ¢¯åº¦ä¼ æ’­ï¼Œè‡ªåŠ¨è¿›è¡Œæ±‚å¯¼è®¡ç®—
      # åªæœ‰å¶ç»“ç‚¹æ‰å¯ä»¥é€šè¿‡.backward()è·å¾—æ¢¯åº¦ä¿¡æ¯ï¼Œzå’Œyä¸æ˜¯å¶ç»“ç‚¹ï¼Œæ‰€ä»¥æ²¡æœ‰æ¢¯åº¦ä¿¡æ¯
      # retain_graph=True ä½¿å¾—t.backwardèƒ½è¿è¡Œå¾ˆå¤šæ¬¡ï¼Œæ²¡æœ‰backwardæ™ºèƒ½è¿è¡Œä¸€æ¬¡
      t.backward(retain_graph=True)
      print(z.grad)
      print(y.grad)
      print(x.grad)
      None
      None
      tensor([[3., 3.],
              [3., 3.]])
      ```

   5. PyTorchå®ä¾‹ï¼šé¢„æµ‹æˆ¿ä»·

      1. å‡†å¤‡æ•°æ®

         ```python
         # æ„é€ 0~50ä¹‹é—´å‡åŒ€æ•°å­—ä½œä¸ºæ—¶é—´å˜é‡ã€‚
         x=torch.linspace(0,50,steps=50,requires_grad=True).type(torch.float)
         x
         tensor([ 0.0000,  1.0204,  2.0408,  3.0612,  4.0816,  5.1020,  6.1224,  7.1429,
                  8.1633,  9.1837, 10.2041, 11.2245, 12.2449, 13.2653, 14.2857, 15.3061,
                 16.3265, 17.3469, 18.3673, 19.3878, 20.4082, 21.4286, 22.4490, 23.4694,
                 24.4898, 25.5102, 26.5306, 27.5510, 28.5714, 29.5918, 30.6122, 31.6327,
                 32.6531, 33.6735, 34.6939, 35.7143, 36.7347, 37.7551, 38.7755, 39.7959,
                 40.8163, 41.8367, 42.8571, 43.8776, 44.8980, 45.9184, 46.9388, 47.9592,
                 48.9796, 50.0000])
         ```

         ```python
         # ç”Ÿæˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º5çš„æ­£æ€åˆ†å¸ƒã€‚
         rand=torch.randn(50).type(torch.float)*5
         # torch.normal(mean=0, std=10,out=50)
         rand
         tensor([-9.5013, -0.2001, -0.2612,  7.3100,  7.8910, -6.8819, -2.4307, -2.4865,
                  5.9178,  4.6767,  2.6463, -4.2847, -1.1919, -7.6735,  6.0805, -4.8176,
                  4.2334, -6.2658,  6.2607,  0.8406, -8.8556,  6.1883, 12.6984,  0.4164,
                  2.1272,  5.9475, -5.7067,  5.4889,  1.4164,  5.9655,  0.4942,  2.8694,
                 -2.0671,  0.4787, -3.0500,  5.3546, -0.5102,  3.3566, -0.3483,  3.2969,
                  2.4644,  0.0407,  5.4788, -7.3005,  0.5329,  3.5430, -3.5318, -4.6006,
                 -8.2579,  1.2810])
         ```

         ```python
         # ç”Ÿæˆæˆ¿ä»·
         y=x+rand
         y
         tensor([-2.5061, -8.4511,  8.3336,  0.6089,  8.8587,  0.4199,  6.9523,  2.9417,
                 11.2004,  5.9490, 15.7981, 16.6783,  5.6091, 13.6873, 15.6817, 13.3907,
                 28.9034, 15.6045, 19.9990, 24.7943, 22.4937, 24.9002, 19.5600, 18.0025,
                 26.3569, 26.2317, 28.3032, 27.7680, 24.7899, 32.1754, 35.6954, 25.2297,
                 27.5482, 36.3927, 26.6795, 37.5358, 27.3953, 38.8195, 35.6925, 36.0569,
                 36.8523, 53.8654, 39.3955, 43.5647, 45.5984, 47.2127, 46.3957, 46.6790,
                 49.8011, 54.2800])
         ```

         ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†

         ```python
         x_train=x[:-5]
         x_test=x[-5:]
         y_train=y[:-5]
         y_test=y[-5:]
         ```

         å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¯è§†åŒ–

         ```python
         # å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¯è§†åŒ–
         import matplotlib.pyplot as plt
         # è®¾å®šç»˜åˆ¶çª—å£å¤§å°ä¸º8*6inch
         plt.figure(figsize=(8,6))
         # ç»˜åˆ¶æ•°æ®ï¼Œç”±äºxå’Œyéƒ½æ˜¯Variableï¼Œéœ€è¦ç”¨dataè·å–å®ƒä»¬åŒ…è£¹çš„Tensor,å¹¶è½¬æˆNumpy
         plt.plot(x_train.data.numpy(),y_train.data.numpy(),'o')
         # è®¾ç½®Xè½´æ ‡ç­¾
         plt.xlabel('X')
         plt.ylabel('Y')
         plt.show()
         ```

         ![](.\2-1.jpg)

      2. æ¨¡å‹è®¾è®¡

         æˆ‘ä»¬å¸Œæœ›å¾—åˆ°ä¸€æ¡å°½å¯èƒ½ä»ä¸­é—´ç©¿è¶Šè¿™äº›æ•°æ®æ•£ç‚¹çš„æ‹Ÿåˆç›´çº¿ã€‚è®¾è¿™æ¡ç›´çº¿æ–¹ç¨‹ä¸º
         $$
         y=ax+b
         $$
         æ¥ä¸‹æ¥çš„é—®é¢˜æ˜¯ï¼Œæ±‚è§£å‡ºå‚æ•°a,bçš„æ•°å€¼ã€‚æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸€ä¸ªæ•°æ®ç‚¹ä»£å…¥è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼Œè®¡ç®—å‡º
         $$
   \hat{y_i}=ax_i+b
         $$
         
         æ˜¾ç„¶
         $$
         \hat{y_i}
         $$
         è¶Šé è¿‘
         $$
         y_i
         $$
         è¶Šå¥½ï¼Œå®šä¹‰å¹³å‡æŸå¤±å‡½æ•°
         $$
         L=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac{1}{N}\sum_{i=1}^N(y_i-ax_i-b)^2
         $$
         
         å¹¶è®©å®ƒå°½å¯èƒ½çš„å°ã€‚ç”±äº
         $$
         x_iå’Œy_i
         $$
         éƒ½æ˜¯å›ºå®šçš„æ•°ï¼Œè€Œåªæœ‰aå’Œbæ˜¯å˜é‡ï¼Œé‚£ä¹ˆLæœ¬è´¨ä¸Šå°±æ˜¯aå’Œbçš„å‡½æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬è¦å¯»æ‰¾æœ€ä¼˜çš„aã€bç»„åˆï¼Œè®©Læœ€å°åŒ–ã€‚
         
         æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥åå¤è¿­ä»£aå’Œbï¼Œä»è€Œè®©Lè¶Šå˜è¶Šå°ã€‚
         $$
         a_{t+1}=a_t-\alpha \left. \frac{\partial L}{\partial a}  \right| _{a=a_t}
         $$
         
         $$
         b_{t+1}=b_t-\alpha \left. \frac{\partial L}{\partial b}  \right| _{b=b_t}
         $$
         
         $$
         \alphaä¸ºå­¦ä¹ ç‡ï¼Œå®ƒå¯ä»¥è°ƒèŠ‚æ¢¯åº¦ä¸‹é™å¿«æ…¢ï¼Œ\alphaè¶Šå¤§ï¼Œaã€bæ›´æ–°å¾—è¶Šå¿«ï¼Œä½†æ˜¯è®¡ç®—å¾—åˆ°çš„æœ€ä¼˜å€¼Lå°±æœ‰å¯èƒ½è¶Šä¸å‡†
         $$
         
         åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å‡ºLå¯¹aã€bçš„åå¯¼æ•°ï¼Œåˆ©ç”¨PyTorchçš„backwardï¼ˆï¼‰å¯ä»¥éå¸¸æ–¹ä¾¿åœ°å°†è¿™ä¸¤ä¸ªåå¯¼æ•°è®¡ç®—å‡ºæ¥ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€æ­¥æ­¥åœ°æ›´æ–°aå’Œbçš„æ•°å€¼å°±å¯ä»¥äº†ã€‚å½“è¾¾åˆ°ä¸€å®šçš„è¿­ä»£æ­¥æ•°ä¹‹åï¼Œæœ€ç»ˆaå’Œbçš„æ•°å€¼å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„æœ€ä¼˜æ•°å€¼ã€‚
         
      3. è®­ç»ƒ
      
         ```python
         # å®šä¹‰ä¸¤ä¸ªè‡ªåŠ¨å¾®åˆ†å˜é‡aå’Œb(a,béšæœº)
         # a=Variable(torch.rand(1),requires_grad=True)  Variableå·²ç»è¢«åºŸå¼ƒ
         # b=Variable(torch.rand(1),requires_grad=True)
         a=torch.rand(1,requires_grad=True)
         b=torch.rand(1,requires_grad=True)
         # è®¾ç½®å­¦ä¹ ç‡
         learning_rate=0.0001
         ```
      
         ```python
         # aå’Œbçš„è¿­ä»£è®¡ç®—
         for i in range(500):
             predictions=a*x_train+b
             loss=torch.mean((predictions-y_train)**2)
             print('loss:',loss)
             loss.backward()
         #add_    
             a.data.add_(-learning_rate*a.grad)
             b.data.add_(-learning_rate*b.grad)
         # åœ¨æ›´æ–°å®Œaã€bçš„æ•°å€¼åï¼Œéœ€è¦æ¸…ç©ºaä¸­çš„çš„æ¢¯åº¦ä¿¡æ¯ï¼Œ
         # å¦åˆ™å®ƒä¼šåœ¨ä¸‹ä¸€æ­¥è¿­ä»£çš„æ—¶å€™ç´¯åŠ 
             a.grad.zero_()
             b.grad.zero_()
         #å¤ªé•¿ï¼Œåªå†™äº†è¾“å‡ºäº†æœ€åä¸€ä¸ªloss
         loss: tensor(23.5075, grad_fn=<MeanBackward0>)
         print(a.data,b.data)
         tensor([0.9775]) tensor([0.3045])
         ```
      
         å°†åŸå§‹çš„æ•°æ®æ•£ç‚¹è”åˆæ‹Ÿåˆçš„ç›´çº¿é€šè¿‡å›¾å½¢ç”»å‡ºæ¥
      
         ```python
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         plt.figure(figsize=(10,7))
         #ç»˜åˆ¶è®­ç»ƒé›†æ•£ç‚¹
         plt1, =plt.plot(x_data,y_data,'o')
         # ç»˜åˆ¶æ‹Ÿåˆç›´çº¿
         plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # åæ ‡æ ‡æ³¨
         plt.xlabel("X")
         plt.ylabel("Y")
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2],['train_data',str1])
         plt.show()
         ```
         
      
      ![](.\2-2.jpg)
      
      4. é¢„æµ‹
      
         ç”¨æµ‹è¯•æ•°æ®é›†è¿›è¡Œæµ‹è¯•
      
         ```python
         predictions=a*x_test+b
         predictions
         tensor([45.6905, 46.6988, 47.7070, 48.7153, 49.7235], grad_fn=<AddBackward0>)
      
         x_data=x_train.data.numpy()
         y_data=y_train.data.numpy()
         x_pred=x_test.data.numpy()
         y_pred=y_test.data.numpy()
         plt.figure(figsize=(10,7))
         #ç»˜åˆ¶è®­ç»ƒé›†æ•£ç‚¹
         plt1, =plt.plot(x_data,y_data,'o')
         plt2, =plt.plot(x_pred,y_pred,'s')
         # ç»˜åˆ¶æ‹Ÿåˆç›´çº¿
         plt3, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
         # ç»˜åˆ¶é¢„æµ‹æ•°æ®
         plt4, =plt.plot(x_pred,a.data.numpy()*x_pred+b.data.numpy())
         plt.xlabel('X')
         plt.ylabel('Y')
         str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
         plt.legend([plt1,plt2,plt3],['train_data','test_data',str1])
         plt.savefig("2-3.jpg")
         plt.show()
         ```
         
         ![](.\2-3.jpg)
         
         

3. å•è½¦é¢„æµ‹å™¨ï¼šä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

   è¿ç”¨PyTorchåŠ¨æ‰‹æ­å»ºä¸€ä¸ªå…±äº«å•è½¦é¢„æµ‹å™¨ï¼Œåœ¨å®æˆ˜ä¸­æŒæ¡ç¥ç»å…ƒã€ç¥ç»å…ƒã€æ¿€æ´»å‡½æ•°ã€æœºå™¨å­¦ä¹ ç­‰åŸºæœ¬æ¦‚å¿µï¼Œä»¥åŠæ•°æ®é¢„å¤„ç†çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ­ç§˜ç¥ç»ç½‘ç»œè¿™ä¸ªâ€œé»‘ç®±â€ï¼Œçœ‹çœ‹å®ƒå¦‚ä½•å·¥ä½œï¼Œå“ªä¸ªç¥ç»å…ƒèµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

   ```python
   #å¯¼å…¥éœ€è¦ä½¿ç”¨çš„åº“
   import numpy as np
   import pandas as pd #è¯»å–csvæ–‡ä»¶çš„åº“
   import matplotlib.pyplot as plt
   import torch
   #from torch.autograd import Variable
   import torch.optim as optim
   
   # è®©è¾“å‡ºçš„å›¾å½¢ç›´æ¥åœ¨Notebookä¸­æ˜¾ç¤º
   %matplotlib inline
   data_path="hour.csv"
   rides = pd.read_csv(data_path)
   # head()è¯»å–5è¡Œæ•°æ®å¹¶æ˜¾ç¤ºï¼ŒæŸ¥çœ‹æ•°æ®æ ¼å¼
   rides.head()
   ```

   ![](.\3-1.jpg)

   ```python
   #æˆ‘ä»¬å–å‡ºæœ€åä¸€åˆ—çš„å‰50æ¡è®°å½•æ¥è¿›è¡Œé¢„æµ‹
   counts = rides['cnt'][:50]
   
   #è·å¾—å˜é‡xï¼Œå®ƒæ˜¯1ï¼Œ2ï¼Œâ€¦â€¦ï¼Œ50
   x = np.arange(len(counts))
   
   # å°†countsè½¬æˆé¢„æµ‹å˜é‡ï¼ˆæ ‡ç­¾ï¼‰ï¼šy
   y = np.array(counts)
   
   # ç»˜åˆ¶ä¸€ä¸ªå›¾å½¢ï¼Œå±•ç¤ºæ›²çº¿é•¿çš„æ ·å­
   plt.figure(figsize = (8, 6)) #è®¾å®šç»˜å›¾çª—å£å¤§å°
   plt.plot(x, y, 'o-') # ç»˜åˆ¶åŸå§‹æ•°æ®
   plt.xlabel('X') #æ›´æ”¹åæ ‡è½´æ ‡æ³¨
   plt.ylabel('Y') #æ›´æ”¹åæ ‡è½´æ ‡æ³¨
   plt.savefig("3-2.jpg")
   plt.show()
   ```

   ![](.\3-2.jpg)

   1. ç”¨çº¿æ€§å›å½’å¯¹æ›²çº¿è¿›è¡Œæ‹Ÿåˆï¼Œå°½ç®¡æ•ˆæœå¾ˆå·®ï¼ˆå›é¡¾ä¸Šä¸€ç« èŠ‚å†…å®¹ï¼‰

      ```python
      #æˆ‘ä»¬å–å‡ºæ•°æ®åº“çš„æœ€åä¸€åˆ—çš„å‰50æ¡è®°å½•æ¥è¿›è¡Œé¢„æµ‹
      counts = rides['cnt'][:50]
      
      # åˆ›å»ºå˜é‡xï¼Œå®ƒæ˜¯1ï¼Œ2ï¼Œâ€¦â€¦ï¼Œ50
      x = torch.tensor(np.arange(len(counts)), dtype=torch.double, requires_grad = True)
      
      # å°†countsè½¬æˆé¢„æµ‹å˜é‡ï¼ˆæ ‡ç­¾ï¼‰ï¼šy
      y = torch.tensor(np.array(counts), dtype=torch.double, requires_grad = True)
      
      a = torch.rand(1, dtype=torch.double, requires_grad = True) #åˆ›å»ºaå˜é‡ï¼Œå¹¶éšæœºèµ‹å€¼åˆå§‹åŒ–
      b = torch.rand(1, dtype=torch.double, requires_grad = True) #åˆ›å»ºbå˜é‡ï¼Œå¹¶éšæœºèµ‹å€¼åˆå§‹åŒ–
      print('Initial parameters:', [a, b])
      learning_rate = 0.00001 #è®¾ç½®å­¦ä¹ ç‡
      for i in range(10000):
          ### å¢åŠ äº†è¿™éƒ¨åˆ†ä»£ç ï¼Œæ¸…ç©ºå­˜å‚¨åœ¨å˜é‡aï¼Œbä¸­çš„æ¢¯åº¦ä¿¡æ¯ï¼Œä»¥å…åœ¨backwardçš„è¿‡ç¨‹ä¸­ä¼šåå¤ä¸åœåœ°ç´¯åŠ 
          predictions = a * x+ b  #è®¡ç®—åœ¨å½“å‰aã€bæ¡ä»¶ä¸‹çš„æ¨¡å‹é¢„æµ‹æ•°å€¼
          loss = torch.mean((predictions - y) ** 2) #é€šè¿‡ä¸æ ‡ç­¾æ•°æ®yæ¯”è¾ƒï¼Œè®¡ç®—è¯¯å·®
          print('loss:', loss)
          loss.backward() #å¯¹æŸå¤±å‡½æ•°è¿›è¡Œæ¢¯åº¦åä¼ 
          a.data.add_(- learning_rate * a.grad)  #åˆ©ç”¨ä¸Šä¸€æ­¥è®¡ç®—ä¸­å¾—åˆ°çš„açš„æ¢¯åº¦ä¿¡æ¯æ›´æ–°aä¸­çš„dataæ•°å€¼
          b.data.add_(- learning_rate * b.grad)  #åˆ©ç”¨ä¸Šä¸€æ­¥è®¡ç®—ä¸­å¾—åˆ°çš„bçš„æ¢¯åº¦ä¿¡æ¯æ›´æ–°bä¸­çš„dataæ•°å€¼
          a.grad.zero_() #æ¸…ç©ºaçš„æ¢¯åº¦æ•°å€¼
          b.grad.zero_() #æ¸…ç©ºbçš„æ¢¯åº¦æ•°å€¼
      ```

      ç”¨çº¿æ€§å›å½’è¿›è¡Œæ‹Ÿåˆ

      ```python
      x_data=x.data.numpy()
      y_data=y.data.numpy()
      plt.figure(figsize=(10,7))
      #ç»˜åˆ¶è®­ç»ƒé›†æ•£ç‚¹
      plt1, =plt.plot(x_data,y_data,'o')
      # ç»˜åˆ¶æ‹Ÿåˆç›´çº¿
      plt2, =plt.plot(x_data,a.data.numpy()*x_data+b.data.numpy())
      # åæ ‡æ ‡æ³¨
      plt.xlabel("X")
      plt.ylabel("Y")
      str1=str(a.data.numpy()[0])+'x+'+str(b.data.numpy()[0])
      plt.legend([plt1,plt2],['data',str1])
      plt.savefig("3-3.jpg")
      plt.show()
      ```

      ![](.\3-3.jpg)

   2. ç¬¬ä¸€ä¸ªäººå·¥ç¥ç»ç½‘ç»œé¢„æµ‹å™¨ï¼ˆå•è½¦é¢„æµ‹å™¨1.0ï¼‰

      è¾“å…¥å±‚1ä¸ªå•å…ƒï¼Œéšå«å±‚1å±‚æœ‰10ä¸ªå•å…ƒï¼Œè¾“å‡ºå±‚1ä¸ªå•å…ƒçš„äººå·¥ç¥ç»ç½‘ç»œ
      
      ```python
      #å–å‡ºæ•°æ®åº“ä¸­çš„æœ€åä¸€åˆ—çš„å‰50æ¡è®°å½•æ¥è¿›è¡Œé¢„æµ‹
      counts = rides['cnt'][:50]
      
      #åˆ›å»ºå˜é‡xï¼Œå®ƒæ˜¯1ï¼Œ2ï¼Œâ€¦â€¦ï¼Œ50
      x = torch.tensor(np.arange(len(counts), dtype = float), requires_grad = True)
      
      # å°†countsè½¬æˆé¢„æµ‹å˜é‡ï¼ˆæ ‡ç­¾ï¼‰ï¼šy
      y = torch.tensor(np.array(counts, dtype = float), requires_grad = True)
      
      # è®¾ç½®éšå«å±‚ç¥ç»å…ƒçš„æ•°é‡
      sz = 10
      
      # åˆå§‹åŒ–æ‰€æœ‰ç¥ç»ç½‘ç»œçš„æƒé‡ï¼ˆweightsï¼‰å’Œé˜ˆå€¼ï¼ˆbiasesï¼‰
      weights = torch.randn((1, sz), dtype = torch.double, requires_grad = True) #1*10çš„è¾“å…¥åˆ°éšå«å±‚çš„æƒé‡çŸ©é˜µ
      biases = torch.randn(sz, dtype = torch.double, requires_grad = True) #å°ºåº¦ä¸º10çš„éšå«å±‚èŠ‚ç‚¹åç½®å‘é‡
      weights2 = torch.randn((sz, 1), dtype = torch.double, requires_grad = True) #10*1çš„éšå«åˆ°è¾“å‡ºå±‚æƒé‡çŸ©é˜µ
      
      learning_rate = 0.001 #è®¾ç½®å­¦ä¹ ç‡
      losses = []
      
      # å°† x è½¬æ¢ä¸º(50,1)çš„ç»´åº¦ï¼Œä»¥ä¾¿ä¸ç»´åº¦ä¸º(1,10)çš„weightsçŸ©é˜µç›¸ä¹˜
      x = x.view(50, -1)
      # å°† y è½¬æ¢ä¸º(50,1)çš„ç»´åº¦
      y = y.view(50, -1)
      
      for i in range(100000):
          # ä»è¾“å…¥å±‚åˆ°éšå«å±‚çš„è®¡ç®—
          hidden = x * weights + biases
          # å°†sigmoidå‡½æ•°ä½œç”¨åœ¨éšå«å±‚çš„æ¯ä¸€ä¸ªç¥ç»å…ƒä¸Š
          hidden = torch.sigmoid(hidden)
          #print(hidden.size())
          # éšå«å±‚è¾“å‡ºåˆ°è¾“å‡ºå±‚ï¼Œè®¡ç®—å¾—åˆ°æœ€ç»ˆé¢„æµ‹
          predictions = hidden.mm(weights2)
          #print(predictions.size())
          # é€šè¿‡ä¸æ ‡ç­¾æ•°æ®yæ¯”è¾ƒï¼Œè®¡ç®—å‡æ–¹è¯¯å·®
          loss = torch.mean((predictions - y) ** 2) 
          #print(loss.size())
          losses.append(loss.data.numpy())
          
          # æ¯éš”10000ä¸ªå‘¨æœŸæ‰“å°ä¸€ä¸‹æŸå¤±å‡½æ•°æ•°å€¼
          if i % 10000 == 0:
              print('loss:', loss)
              
          #å¯¹æŸå¤±å‡½æ•°è¿›è¡Œæ¢¯åº¦åä¼ 
          loss.backward()
          
          #åˆ©ç”¨ä¸Šä¸€æ­¥è®¡ç®—ä¸­å¾—åˆ°çš„weightsï¼Œbiasesç­‰æ¢¯åº¦ä¿¡æ¯æ›´æ–°weightsæˆ–biasesä¸­çš„dataæ•°å€¼
          weights.data.add_(- learning_rate * weights.grad)  
          biases.data.add_(- learning_rate * biases.grad)
          weights2.data.add_(- learning_rate * weights2.grad)
          
          # æ¸…ç©ºæ‰€æœ‰å˜é‡çš„æ¢¯åº¦å€¼ã€‚
          # å› ä¸ºpytorchä¸­backwardä¸€æ¬¡æ¢¯åº¦ä¿¡æ¯ä¼šè‡ªåŠ¨ç´¯åŠ åˆ°å„ä¸ªå˜é‡ä¸Šï¼Œå› æ­¤éœ€è¦æ¸…ç©ºï¼Œå¦åˆ™ä¸‹ä¸€æ¬¡è¿­ä»£ä¼šç´¯åŠ ï¼Œé€ æˆå¾ˆå¤§çš„åå·®
          weights.grad.zero_()
          biases.grad.zero_()
          weights2.grad.zero_()
          
      loss: tensor(2256.8876, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(738.2561, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(561.8528, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(507.0011, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(473.5310, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(466.2351, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(462.0210, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(459.2573, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(457.6207, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(456.6714, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # æ‰“å°è¯¯å·®æ›²çº¿
      plt.plot(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.show()
      ```
      
      ![](.\3-4.jpg)
      
      
      
      ç»˜åˆ¶æ·±åº¦å­¦ä¹ æ‹Ÿåˆæ›²çº¿
      
      ```python
      x_data = x.data.numpy() # è·å¾—xåŒ…è£¹çš„æ•°æ®
      plt.figure(figsize = (10, 7)) #è®¾å®šç»˜å›¾çª—å£å¤§å°
      xplot, = plt.plot(x_data, y.data.numpy(), 'o') # ç»˜åˆ¶åŸå§‹æ•°æ®
      
      yplot, = plt.plot(x_data, predictions.data.numpy())  #ç»˜åˆ¶æ‹Ÿåˆæ•°æ®
      plt.xlabel('X') #æ›´æ”¹åæ ‡è½´æ ‡æ³¨
      plt.ylabel('Y') #æ›´æ”¹åæ ‡è½´æ ‡æ³¨
      plt.legend([xplot, yplot],['Data', 'Prediction under 1000000 epochs']) #ç»˜åˆ¶å›¾ä¾‹
      plt.savefig("3-5.jpg")
      plt.show()
      ```
      
      ![](.\3-5.jpg)
      
      ä¸Šé¢çš„ç¨‹åºä¹‹æ‰€ä»¥è·‘å¾—å¾ˆæ…¢ï¼Œæ˜¯å› ä¸ºxçš„å–å€¼èŒƒå›´1ï½50ã€‚ è€Œç”±äºæ‰€æœ‰æƒé‡å’Œbiasesçš„å–å€¼èŒƒå›´è¢«è®¾å®šä¸º-1,1çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼Œè¿™æ ·å°±å¯¼è‡´ æˆ‘ä»¬è¾“å…¥ç»™éšå«å±‚èŠ‚ç‚¹çš„æ•°å€¼èŒƒå›´ä¸º-50~50ï¼Œ è¦æƒ³å°†sigmoidå‡½æ•°çš„å¤šä¸ªå³°å€¼è°ƒèŠ‚åˆ°æˆ‘ä»¬æœŸæœ›çš„ä½ç½®éœ€è¦è€—è´¹å¾ˆå¤šçš„è®¡ç®—æ—¶é—´ã€‚
      
      æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå°±æ˜¯å°†è¾“å…¥å˜é‡çš„èŒƒå›´å½’ä¸€åŒ–(åªéœ€è¦æ”¹å˜è¾“å…¥å˜é‡x)
      
      ```python
      #åˆ›å»ºå½’ä¸€åŒ–çš„å˜é‡xï¼Œå®ƒçš„å–å€¼æ˜¯0.02,0.04,...,1
      x = torch.tensor(np.arange(len(counts), dtype = float) / len(counts), requires_grad = True)
      
      #è¾“å‡ºæŸå¤±å‡½æ•°ç»“æœ
      loss: tensor(2165.3436, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(940.2517, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(689.8824, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(471.5665, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(231.3214, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(123.2615, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(74.8383, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(56.6815, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(48.7930, dtype=torch.float64, grad_fn=<MeanBackward0>)
      loss: tensor(44.8804, dtype=torch.float64, grad_fn=<MeanBackward0>)
      ```
      
      ```python
      # æ‰“å°è¯¯å·®æ›²çº¿
      # semilogy()å¯¹yè½´æŒ‰å¯¹æ•°è¿›è¡Œç¼©æ”¾
      plt.semilogy(losses)
      plt.xlabel('Epoch')
      plt.ylabel('Loss')
      plt.savefig("3-6.jpg")
      plt.show()
      ```
      
      ![](.\3-6.jpg)
      
      ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿ï¼ˆä»£ç åŒä¸Šï¼‰
      
      ![](.\3-7.jpg)
      
      å¯¹æ¥ä¸‹æ¥çš„50ä¸ªæ•°æ®è¿›è¡Œé¢„æµ‹
      
      ```python
      counts_predict = rides['cnt'][50:100] #è¯»å–å¾…é¢„æµ‹çš„æ¥ä¸‹æ¥çš„50ä¸ªæ•°æ®ç‚¹
      
      #é¦–å…ˆå¯¹æ¥ä¸‹æ¥çš„50ä¸ªæ•°æ®ç‚¹è¿›è¡Œé€‰å–ï¼Œæ³¨æ„xåº”è¯¥å–51ï¼Œ52ï¼Œâ€¦â€¦ï¼Œ100ï¼Œç„¶åå†å½’ä¸€åŒ–
      x = torch.tensor((np.arange(len(counts_predict), dtype = float) + len(counts) )/ len(counts_predict)
                       , requires_grad = True)
      y=torch.tensor(np.array(counts_predict),dtype=torch.double,requires_grad=True)
      x=x.view(len(x),-1)
      hidden=x*weights+biases
      # hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)
      hidden=torch.sigmoid(hidden)
      predictions=hidden.mm(weights2)
      loss=torch.mean((y-predictions)**2)
      
      x_data=x.data.numpy()
      plt.figure(figsize=(10,7))
      xplot, =plt.plot(x_data,y.data.numpy(),'o')
      yplot, =plt.plot(x_data,predictions.data.numpy())
      plt.xlabel("X")
      plt.ylabel("y")
      plt.savefig("3-8.jpg")
      plt.show()
      ```
      
      ![](.\3-8.jpg)
      
      é¢„æµ‹å‘ç°å­˜åœ¨ç€éå¸¸ä¸¥é‡çš„è¿‡æ‹Ÿåˆç°è±¡ï¼ŒåŸå› æ˜¯xå’Œyæ ¹æœ¬æ²¡æœ‰å…³ç³»ï¼Œå³å•è½¦ä½¿ç”¨æ•°é‡ä¸ä¾èµ–äºä¸‹æ ‡ã€‚
      
   3. äººå·¥ç¥ç»ç½‘ç»œNeuï¼ˆå•è½¦é¢„æµ‹å™¨2.0ï¼‰

      1. æ•°æ®é¢„å¤„ç†

         å¯¹æ•°å€¼å˜é‡ï¼ˆè¿ç»­ï¼‰è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†

         ç”±äºæˆ‘ä»¬åˆ©ç”¨äº†å…¨éƒ¨æ•°æ®æ¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œæ‰€ä»¥é‡‡ç”¨ä¹‹å‰ä»‹ç»çš„ä¸€æ¬¡æ€§åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒç½‘ç»œçš„æ–¹æ³•å°±ä¼šå¾ˆæ…¢ï¼Œ æ‰€ä»¥æˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†æˆäº†ä¸åŒçš„æ’®ï¼ˆbatchï¼‰ï¼Œä¸€ä¸ªæ‰¹æ¬¡ä¸€ä¸ªæ‰¹æ¬¡åœ°è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå› æ­¤æˆ‘ä»¬è¿˜è¦å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†ã€‚

         å¯¹äºç±»å‹å˜é‡ï¼ˆç¦»æ•£ï¼‰å¤„ç†ï¼Œè½¬æ¢æˆç‹¬çƒ­ç¼–ç 

         æœ‰å¾ˆå¤šå˜é‡éƒ½å±äºç±»å‹å˜é‡ï¼Œä¾‹å¦‚season=1,2,3,4ï¼Œåˆ†å››å­£ã€‚æˆ‘ä»¬ä¸èƒ½å°†seasonå˜é‡ç›´æ¥è¾“å…¥åˆ°ç¥ç»ç½‘ç»œï¼Œè¿™æ˜¯å› ä¸ºseasonæ•°å€¼è¶Šé«˜å¹¶ä¸è¡¨ç¤ºç›¸åº”çš„ä¿¡å·å¼ºåº¦è¶Šå¤§ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯å°†ç±»å‹å˜é‡ç”¨ä¸€ä¸ªâ€œä¸€ä½çƒ­ç â€œï¼ˆone-hotï¼‰æ¥ç¼–ç ï¼Œä¹Ÿå°±æ˜¯ï¼š

         ğ‘ ğ‘’ğ‘ğ‘ ğ‘œğ‘›=1â†’(1,0,0,0)

         ğ‘ ğ‘’ğ‘ğ‘ ğ‘œğ‘›=2â†’(0,1,0,0)

         ğ‘ ğ‘’ğ‘ğ‘ ğ‘œğ‘›=3â†’(0,0,1,0)

         ğ‘ ğ‘’ğ‘ğ‘ ğ‘œğ‘›=4â†’(0,0,0,1)

         å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªç±»å‹å˜é‡æœ‰nä¸ªä¸åŒå–å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„â€œä¸€ä½çƒ­ç â€œæ‰€å¯¹åº”çš„å‘é‡é•¿åº¦å°±ä¸ºn

         ```python
         #å¯¹äºç±»å‹å˜é‡çš„ç‰¹æ®Šå¤„ç†
         # season=1,2,3,4, weathersi=1,2,3, mnth= 1,2,...,12, hr=0,1, ...,23, weekday=0,1,...,6
         # ç»è¿‡ä¸‹é¢çš„å¤„ç†åï¼Œå°†ä¼šå¤šå‡ºè‹¥å¹²ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¯¹äºseasonå˜é‡å°±ä¼šæœ‰ season_1, season_2, season_3, season_4
         # è¿™å››ç§ä¸åŒçš„ç‰¹å¾ã€‚
         dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
         for each in dummy_fields:
             #åˆ©ç”¨pandaså¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿åœ°å°†ä¸€ä¸ªç±»å‹å˜é‡å±æ€§è¿›è¡Œone-hotç¼–ç ï¼Œå˜æˆå¤šä¸ªå±æ€§
             dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)
             rides = pd.concat([rides, dummies], axis=1)
         
         # æŠŠåŸæœ‰çš„ç±»å‹å˜é‡å¯¹åº”çš„ç‰¹å¾å»æ‰ï¼Œå°†ä¸€äº›ä¸ç›¸å…³çš„ç‰¹å¾å»æ‰
         fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                           'weekday', 'atemp', 'mnth', 'workingday', 'hr']
         data = rides.drop(fields_to_drop, axis=1)
         data.head()
         ```
         
         å¯¹æ•°å€¼æ•°å€¼ç±»å‹å˜é‡è¿›è¡Œæ ‡å‡†åŒ–
         
         ```python
         # è°ƒæ•´æ‰€æœ‰çš„ç‰¹å¾ï¼Œæ ‡å‡†åŒ–å¤„ç†
         quant_features = ['cnt', 'temp', 'hum', 'windspeed']
         #quant_features = ['temp', 'hum', 'windspeed']
         
         # æˆ‘ä»¬å°†æ¯ä¸€ä¸ªå˜é‡çš„å‡å€¼å’Œæ–¹å·®éƒ½å­˜å‚¨åˆ°scaled_featureså˜é‡ä¸­ã€‚
         scaled_features = {}
         for each in quant_features:
             mean, std = data[each].mean(), data[each].std()
             scaled_features[each] = [mean, std]
             data.loc[:, each] = (data[each] - mean)/std
         ```
         
         å°†æ•°æ®é›†è¿›è¡Œåˆ†å‰²ï¼Œåˆ†å‰²æˆæµ‹è¯•é›†å’Œè®­ç»ƒé›†
         
         ```python
         # å°†æ‰€æœ‰çš„æ•°æ®é›†åˆ†ä¸ºæµ‹è¯•é›†å’Œè®­ç»ƒé›†ï¼Œæˆ‘ä»¬ä»¥å21å¤©æ•°æ®ä¸€å…±21*24ä¸ªæ•°æ®ç‚¹ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶å®ƒæ˜¯è®­ç»ƒé›†
         test_data = data[-21*24:]
         train_data = data[:-21*24]
         print('è®­ç»ƒæ•°æ®ï¼š',len(train_data),'æµ‹è¯•æ•°æ®ï¼š',len(test_data))
         
         # å°†æˆ‘ä»¬çš„æ•°æ®åˆ—åˆ†ä¸ºç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
         
         #ç›®æ ‡åˆ—
         target_fields = ['cnt', 'casual', 'registered']
         features, targets = train_data.drop(target_fields, axis=1), train_data[target_fields]
         test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]
         
         # å°†æ•°æ®ä»pandas dataframeè½¬æ¢ä¸ºnumpy
         X = features.values
         Y = targets['cnt'].values
         Y = Y.astype(float)
         Y = Y.reshape([-1])
         
         è®­ç»ƒæ•°æ®ï¼š 16875 æµ‹è¯•æ•°æ®ï¼š 504
         ```
         
      2. æ„å»ºç¥ç»ç½‘ç»œå¹¶è¿›è¡Œè®­ç»ƒ
      
         1. æ‰‹åŠ¨ç¼–å†™ç”¨Tensorè¿ç®—çš„äººå·¥ç¥ç»ç½‘ç»œ
      
            ```python
            # å®šä¹‰ç¥ç»ç½‘ç»œæ¶æ„ï¼Œfeatures.shape[1]ä¸ªè¾“å…¥å±‚å•å…ƒï¼Œ10ä¸ªéšå«å±‚ï¼Œ1ä¸ªè¾“å‡ºå±‚
            input_size = features.shape[1] #è¾“å…¥å±‚å•å…ƒä¸ªæ•°
            hidden_size = 10 #éšå«å±‚å•å…ƒä¸ªæ•°
            output_size = 1 #è¾“å‡ºå±‚å•å…ƒä¸ªæ•°
            batch_size = 128 #æ¯éš”batchçš„è®°å½•æ•°
            weights1 = torch.randn([input_size, hidden_size], dtype = torch.double,  requires_grad = True) #ç¬¬ä¸€åˆ°äºŒå±‚æƒé‡
            biases1 = torch.randn([hidden_size], dtype = torch.double, requires_grad = True) #éšå«å±‚åç½®
            weights2 = torch.randn([hidden_size, output_size], dtype = torch.double, requires_grad = True) #éšå«å±‚åˆ°è¾“å‡ºå±‚æƒé‡
            def neu(x):
                #è®¡ç®—éšå«å±‚è¾“å‡º
                #xä¸ºbatch_size * input_sizeçš„çŸ©é˜µï¼Œweights1ä¸ºinput_size*hidden_sizeçŸ©é˜µï¼Œ
                #biasesä¸ºhidden_sizeå‘é‡ï¼Œè¾“å‡ºä¸ºbatch_size * hidden_sizeçŸ©é˜µ    
                hidden = x.mm(weights1) + biases1.expand(x.size()[0], hidden_size)
                hidden = torch.sigmoid(hidden)
                
                #è¾“å…¥batch_size * hidden_sizeçŸ©é˜µï¼Œmmä¸Šweights2, hidden_size*output_sizeçŸ©é˜µï¼Œ
                #è¾“å‡ºbatch_size*output_sizeçŸ©é˜µ
                output = hidden.mm(weights2)
                return output
            def cost(x, y):
                # è®¡ç®—æŸå¤±å‡½æ•°
                error = torch.mean((x - y)**2)
                return error
            def zero_grad():
                # æ¸…ç©ºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ä¿¡æ¯
                if weights1.grad is not None and biases1.grad is not None and weights2.grad is not None:
                    weights1.grad.data.zero_()
                    weights2.grad.data.zero_()
                    biases1.grad.data.zero_()
            def optimizer_step(learning_rate):
                # æ¢¯åº¦ä¸‹é™ç®—æ³•
                weights1.data.add_(- learning_rate * weights1.grad.data)
                weights2.data.add_(- learning_rate * weights2.grad.data)
                biases1.data.add_(- learning_rate * biases1.grad.data)
            ```
      
            ```python
            # ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯
            losses = []
            for i in range(1000):
                # æ¯128ä¸ªæ ·æœ¬ç‚¹è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªæ’®ï¼Œåœ¨å¾ªç¯çš„æ—¶å€™ä¸€æ‰¹ä¸€æ‰¹åœ°è¯»å–
                batch_loss = []
                # startå’Œendåˆ†åˆ«æ˜¯æå–ä¸€ä¸ªbatchæ•°æ®çš„èµ·å§‹å’Œç»ˆæ­¢ä¸‹æ ‡
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.double, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.double, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    zero_grad()
                    loss.backward()
                    optimizer_step(0.01)
                    batch_loss.append(loss.data.numpy())
                
                # æ¯éš”100æ­¥è¾“å‡ºä¸€ä¸‹æŸå¤±å€¼ï¼ˆlossï¼‰
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            ```python
            # æ‰“å°è¾“å‡ºæŸå¤±å€¼
            fig = plt.figure(figsize=(10, 7))
            plt.plot(np.arange(len(losses))*100,losses, 'o-')
            plt.xlabel('epoch')
            plt.ylabel('MSE')
            plt.savefig("3-9.jpg")
            ```
      
            ![](.\3-9.jpg)
      
         2. è°ƒç”¨PyTorchç°æˆçš„å‡½æ•°ï¼Œæ„å»ºåºåˆ—åŒ–çš„ç¥ç»ç½‘ç»œ
      
            ```python
            # å®šä¹‰ç¥ç»ç½‘ç»œæ¶æ„ï¼Œfeatures.shape[1]ä¸ªè¾“å…¥å±‚å•å…ƒï¼Œ10ä¸ªéšå«å±‚ï¼Œ1ä¸ªè¾“å‡ºå±‚
            input_size = features.shape[1]
            hidden_size = 10
            output_size = 1
            batch_size = 128
            neu = torch.nn.Sequential(
                torch.nn.Linear(input_size, hidden_size),
                torch.nn.Sigmoid(),
                torch.nn.Linear(hidden_size, output_size),
            )
            cost = torch.nn.MSELoss()
            # # pytorchè‡ªå·±ä¼šå‡†å¤‡å‚æ•°ï¼Œç›´æ¥ä½¿ç”¨neu.parameterså°±è¡Œ
            optimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)
            
            ```
      
            ```python
            # ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯
            losses = []
            for i in range(1000):
                # æ¯128ä¸ªæ ·æœ¬ç‚¹è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªæ’®ï¼Œåœ¨å¾ªç¯çš„æ—¶å€™ä¸€æ‰¹ä¸€æ‰¹åœ°è¯»å–
                batch_loss = []
                # startå’Œendåˆ†åˆ«æ˜¯æå–ä¸€ä¸ªbatchæ•°æ®çš„èµ·å§‹å’Œç»ˆæ­¢ä¸‹æ ‡
                for start in range(0, len(X), batch_size):
                    end = start + batch_size if start + batch_size < len(X) else len(X)
                    xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)
                    yy = torch.tensor(Y[start:end], dtype = torch.float, requires_grad = True)
                    predict = neu(xx)
                    loss = cost(predict, yy)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    batch_loss.append(loss.data.numpy())
                
                # æ¯éš”100æ­¥è¾“å‡ºä¸€ä¸‹æŸå¤±å€¼ï¼ˆlossï¼‰
                if i % 100==0:
                    losses.append(np.mean(batch_loss))
                    print(i, np.mean(batch_loss))
            ```
      
            è°ƒç”¨PyTorchç°æˆå‡½æ•°ï¼Œæ”¶æ•›æ›´å¿«
         
         3. æµ‹è¯•ç¥ç»ç½‘ç»œ
         
            ```python
            # ç”¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
            targets = test_targets['cnt'] #è¯»å–æµ‹è¯•é›†çš„cntæ•°å€¼
            targets = targets.values.reshape([len(targets),1]) #å°†æ•°æ®è½¬æ¢æˆåˆé€‚çš„tensorå½¢å¼
            targets = targets.astype(float) #ä¿è¯æ•°æ®ä¸ºå®æ•°
            
            # å°†å±æ€§å’Œé¢„æµ‹å˜é‡åŒ…è£¹åœ¨Variableå‹å˜é‡ä¸­
            x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)
            y = torch.tensor(targets, dtype = torch.float, requires_grad = True)
            
            # ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹
            predict = neu(x)
            predict = predict.data.numpy()
            # å°†å21å¤©çš„é¢„æµ‹æ•°æ®ä¸çœŸå®æ•°æ®ç”»åœ¨ä¸€èµ·å¹¶æ¯”è¾ƒ
            # æ¨ªåæ ‡è½´æ˜¯ä¸åŒçš„æ—¥æœŸï¼Œçºµåæ ‡è½´æ˜¯é¢„æµ‹æˆ–è€…çœŸå®æ•°æ®çš„å€¼
            fig, ax = plt.subplots(figsize = (10, 7))
            
            mean, std = scaled_features['cnt']
            ax.plot(predict * std + mean, label='Prediction', linestyle = '--')
            ax.plot(targets * std + mean, label='Data', linestyle = '-')
            ax.legend()
            ax.set_xlabel('Date-time')
            ax.set_ylabel('Counts')
            # å¯¹æ¨ªåæ ‡è½´è¿›è¡Œæ ‡æ³¨
            dates = pd.to_datetime(rides.loc[test_data.index]['dteday'])
            # lambda d: d.strftime() dæ˜¯å‚æ•°ï¼Œd.strftime()æ˜¯è¿”å›çš„ç»“æœ
            #%b æœ¬åœ°ç®€åŒ–çš„æœˆä»½åç§° %d  æœˆå†…ä¸­çš„ä¸€å¤©ï¼ˆ0-31ï¼‰
            dates = dates.apply(lambda d: d.strftime('%b %d'))
            #set_xticksè®¾ç½®æ¨ªè½´æ ‡è®°
            #set_xticklabelsè®¾ç½®æ¨ªè½´æ ‡ç­¾
            ax.set_xticks(np.arange(len(dates))[12::24])
            ax.set_xticklabels(dates[12::24], rotation=45)
            ```
         
            ![](.\3-11.jpg)
         
         å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ä¸ªæ›²çº¿åŸºæœ¬æ˜¯å»åˆçš„ ï¼Œä½†æ˜¯åœ¨12æœˆ25æ—¥å‰åå‡ å¤©çš„å®é™…å€¼å’Œé¢„æµ‹å€¼åå·®è¾ƒå¤§ã€‚ä»”ç»†è§‚å¯Ÿæ•°æ®ï¼Œæˆ‘ä»¬å‘ç°12æœˆ25æ—¥æ­£å¥½æ˜¯åœ£è¯èŠ‚ï¼Œäººä»¬çš„å‡ºè¡Œä¹ æƒ¯ä¼šä¸å¾€æ—¥æœ‰å¾ˆå¤§çš„ä¸åŒã€‚åœ¨æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬ä¸­ï¼Œå› ä¸ºæ•´ä¸ªæ•°æ®ä»…æœ‰ä¸¤å¹´é•¿åº¦ï¼Œæ‰€ä»¥åŒ…å«åœ£è¯èŠ‚å‰åçš„æ ·æœ¬ä»…æœ‰ä¸€æ¬¡ï¼Œè¿™å°±å¯¼è‡´æˆ‘ä»¬æ²¡åŠæ³•å¯¹è¿™ä¸€ç‰¹æ®Šå‡æœŸæ¨¡å¼è¿›è¡Œå¾ˆå¥½çš„é¢„æµ‹ã€‚

         4. å‰–æç¥ç»ç½‘ç»œNeu
         
            å¯¹ç½‘ç»œå‡ºç°çš„é—®é¢˜è¿›è¡Œè¯Šæ–­ï¼Œçœ‹çœ‹å“ªä¸€äº›ç¥ç»å…ƒå¯¼è‡´äº†é¢„æµ‹åå·®
         
            ```python
            # é€‰å‡ºä¸‰å¤©é¢„æµ‹ä¸å‡†çš„æ—¥æœŸï¼šDec 22ï¼Œ23ï¼Œ24
            # å°†è¿™ä¸‰å¤©çš„æ•°æ®èšé›†åˆ°ä¸€èµ·ï¼Œå­˜å…¥subsetå’Œsubtargetsä¸­
            # æ ¹æ®ridesæ¯ä¸€è¡Œ"dteday"é¡¹æ˜¯å¦ç­‰äºæ‰€ç»™æ—¥æœŸï¼Œå½¢æˆä¸€ä¸ªboolåˆ—è¡¨
            bool1 = rides['dteday'] == '2012-12-22'
            bool2 = rides['dteday'] == '2012-12-23'
            bool3 = rides['dteday'] == '2012-12-24'
            
            # zip(bool1,bool2,bool2)æ˜¯å°†bool1,bool2,bool3æ¯ié¡¹,
            # ç»„æˆ1ä¸ª3å…ƒç´ åˆ—è¡¨ï¼Œç„¶åå°†æ‰€æœ‰3å…ƒç´ åˆ—è¡¨åˆæˆä¸€ä¸ªåˆ—è¡¨
            #any() å‡½æ•°ç”¨äºåˆ¤æ–­ç»™å®šçš„å¯è¿­ä»£å‚æ•° iterable æ˜¯å¦å…¨éƒ¨ä¸º Falseï¼Œåˆ™è¿”å› Falseï¼Œ
            # å¦‚æœæœ‰ä¸€ä¸ªä¸º Trueï¼Œåˆ™è¿”å› Trueã€‚å…ƒç´ é™¤äº†æ˜¯ 0ã€ç©ºã€FALSE å¤–éƒ½ç®— TRUEã€‚
            bools = [any(tup) for tup in zip(bool1,bool2,bool3) ]
            # å°†ç›¸åº”çš„å˜é‡å–å‡ºæ¥
            # test_featureså’Œtest_targetsçš„ç´¢å¼•å¹¶ä¸æ˜¯ä»0å¼€å§‹ï¼Œè€Œæ˜¯ä»æˆªå–çš„åœ°æ–¹å¼€å§‹ï¼Œæ‰€ä»¥èƒ½ç”¨locç¡®å®š
            # print(test_features.index)
            subset = test_features.loc[rides[bools].index]
            # print(subset)
            subtargets = test_targets.loc[rides[bools].index]
            # print(subtargets)
            subtargets = subtargets['cnt']
            subtargets = subtargets.values.reshape([len(subtargets),1])
            
            def feature(X, net):
                # å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°å¯ä»¥æå–ç½‘ç»œçš„æƒé‡ä¿¡æ¯ï¼Œæ‰€æœ‰çš„ç½‘ç»œå‚æ•°ä¿¡æ¯å…¨éƒ¨å­˜å‚¨åœ¨äº†neuçš„named_parametersé›†åˆä¸­äº†
                X = torch.tensor(X, dtype = torch.float, requires_grad = False)
                dic = dict(net.named_parameters()) #æå–å‡ºæ¥è¿™ä¸ªé›†åˆ
                weights = dic['0.weight'] #å¯ä»¥æŒ‰ç…§å±‚æ•°.åç§°æ¥ç´¢å¼•é›†åˆä¸­çš„ç›¸åº”å‚æ•°å€¼
                biases = dic['0.bias'] #å¯ä»¥æŒ‰ç…§å±‚æ•°.åç§°æ¥ç´¢å¼•é›†åˆä¸­çš„ç›¸åº”å‚æ•°å€¼
                h = torch.sigmoid(X.mm(weights.t()) + biases.expand([len(X), len(biases)])) # éšå«å±‚çš„è®¡ç®—è¿‡ç¨‹
                return h # è¿”å›éšè—å±‚çš„è®¡ç®—
            
            # å°†è¿™å‡ å¤©çš„æ•°æ®è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œè¯»å–å‡ºéšå«å±‚ç¥ç»å…ƒçš„æ¿€æ´»æ•°å€¼ï¼Œå­˜å…¥resultsä¸­
            # resultséšè—å±‚è¾“å‡ºæ•°æ®
            results = feature(subset.values, neu).data.numpy()
            # è¿™äº›æ•°æ®å¯¹åº”çš„é¢„æµ‹å€¼ï¼ˆè¾“å‡ºå±‚ï¼‰
            #predictè¾“å‡ºå±‚è¾“å‡ºæ•°æ®
            predict = neu(torch.tensor(subset.values, dtype = torch.float, requires_grad = True)).data.numpy()
            
            #å°†é¢„æµ‹å€¼è¿˜åŸæˆåŸå§‹æ•°æ®çš„æ•°å€¼èŒƒå›´
            mean, std = scaled_features['cnt']
            predict = predict * std + mean
            subtargets = subtargets * std + mean
            # å°†æ‰€æœ‰çš„ç¥ç»å…ƒæ¿€æ´»æ°´å¹³ç”»åœ¨åŒä¸€å¼ å›¾ä¸Šï¼Œè“è‰²çš„æ˜¯æ¨¡å‹é¢„æµ‹çš„æ•°å€¼
            fig, ax = plt.subplots(figsize = (8, 6))
            # resultsæ¯ä¸€åˆ—å…ƒç´ æ˜¯è¾“å…¥çš„åŒä¸€è¡Œå…ƒç´ çš„å„ä¸ªç¥ç»å…ƒè¾“å‡ºï¼Œæ¯è¡Œæ˜¯ridesæ¯ä¸€è¡Œæ•°æ®
            # .ç‚¹ :è™šçº¿ï¼Œalphaé€æ˜åº¦
            ax.plot(results[:,:],'.:',alpha = 0.3)
            # bè“è‰² sæ­£æ–¹å½¢ -çº¿ ï¼Œcnté¢„æµ‹å€¼ï¼Œ
            ax.plot((predict - min(predict)) / (max(predict) - min(predict)),'bs-',label='Prediction')
            # rçº¢è‰² oåœ†åœˆ -çº¿,cntå®é™…æƒ…å†µ
            ax.plot((subtargets - min(predict)) / (max(predict) - min(predict)),'ro-',label='Real')
            #:è™šçº¿ *äº”è§’æ˜Ÿ éšè—å±‚ç¬¬5ä¸ªç¥ç»å…ƒè¾“å‡ºå€¼
            ax.plot(results[:, 4],':*',alpha=1, label='Neuro 5')
            
            # set_xlimè®¾ç½®æ¨ªè½´èŒƒå›´
            ax.set_xlim(right=len(predict))
            # æ˜¾ç¤ºæ ‡ç­¾
            ax.legend()
            plt.ylabel('Normalized Values')
            
            dates = pd.to_datetime(rides.loc[subset.index]['dteday'])
            dates = dates.apply(lambda d: d.strftime('%b %d'))
            # set_xticksè®¾ç½®åæ ‡è½´åˆ»åº¦
            ax.set_xticks(np.arange(len(dates))[12::24])
            # set_xticklabelsè®¾ç½®åˆ»åº¦çš„æ˜¾ç¤ºæ–‡æœ¬
            ax.set_xticklabels(dates[12::24], rotation=45)
            fig.savefig("3-12.jpg")
            ```
         
            ![](.\3-12.jpg)
         
            å¯ä»¥å‘ç°ï¼Œ4å·ç¥ç»å…ƒçš„è¾“å‡ºæ›²çº¿ä¸çœŸå®è¾“å‡ºæ›²çº¿æ¯”è¾ƒæ¥è¿‘ã€‚å› æ­¤ï¼Œå¯ä»¥è®¤ä¸ºè¯¥ç¥ç»å…ƒå¯¹æé«˜é¢„æµ‹å‡†ç¡®æ€§æœ‰æ›´é«˜çš„è´¡çŒ®ã€‚
         
            åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æƒ³çŸ¥é“Neuro 5ç¥ç»å…ƒè¡¨ç°è¾ƒå¥½çš„åŸå› ä»¥åŠå®ƒçš„æ¿€æ´»æ˜¯ç”±è°å†³å®šçš„ã€‚è¿›ä¸€æ­¥åˆ†æå®ƒçš„å½±å“å› ç´ ï¼Œå¯ä»¥çŸ¥é“æ˜¯ä»è¾“å…¥å±‚æŒ‡å‘å®ƒçš„æƒé‡ã€‚
         
            å¯¹æƒé‡è¿›è¡Œå¯è§†åŒ–
         
            è¾“å‡ºç¥ç»ç½‘ç»œæ‰€æœ‰çš„å‚æ•°
         
            ```python
            for para in neu.named_parameters():
                print(para) 
            #ç”±æ˜¾ç¤ºå¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªlistæ˜¯æ‰€æœ‰è¾“å…¥å±‚èŠ‚ç‚¹åˆ°å•ä¸ªéšè—å±‚èŠ‚ç‚¹çš„æƒé‡
            ('0.weight', Parameter containing:
            tensor([[-2.1517e-01, -4.0614e-02, -1.0516e-01,  5.4308e-02, -1.1437e-01,
                     -6.2382e-02,  7.6552e-02,  5.8724e-02,  1.6933e-01,  6.4778e-02,
                     -1.0376e-03,  1.1390e-01,  5.9447e-02,  1.9477e-02,  9.5131e-02,
                      7.1202e-03, -3.4932e-03,  1.8305e-02,  1.5940e-01, -3.6335e-02,
                     -9.0728e-02, -5.4818e-03,  1.8764e-02, -1.2173e-01, -1.5531e-01,
                      2.9141e-02,  1.1026e-01,  6.3001e-03, -5.6181e-02, -2.1444e-03,
                      2.0125e-02,  3.7873e-03, -9.8517e-02, -2.6571e-02,  2.8248e-01,
                     -1.0817e-02,  3.3487e-02,  2.3034e-01,  8.3346e-02,  1.3844e-01,
                      7.7592e-02,  1.2086e-01, -1.8834e-01, -1.1458e-01, -9.3430e-02,
                     -1.1976e-01, -1.1405e-01,  6.5605e-04, -3.8784e-02, -4.9509e-02,
                      8.3201e-02,  1.4002e-02,  8.9738e-02, -1.0683e-01,  9.2824e-02,
                     -9.4244e-02],
                    [-4.5748e-02, -5.6792e-02,  3.4704e-03, -8.0855e-02,  8.0398e-03,
                     -8.4377e-02, -7.2572e-02,  1.2197e-01, -1.2841e-02, -4.6603e-02,
                     -3.4122e-02, -6.0449e-02, -5.5149e-02,  1.1901e-01,  2.2428e-03,
                     -5.9819e-03,  1.8554e-02, -3.5701e-02, -6.2146e-02,  2.8831e-02,
                      1.0577e-01,  3.2351e-03, -3.5365e-02,  9.5441e-03, -9.0695e-02,
                      1.6664e-02, -8.1705e-02,  2.3343e-02,  5.9412e-02, -6.8365e-02,
                      9.0864e-02,  2.3068e-02,  4.9710e-02,  8.5133e-02,  1.9855e-01,
                     -1.4844e-02, -5.8998e-02,  4.5605e-02, -9.2452e-02,  5.2184e-02,
                     -1.4754e-02, -1.1379e-01, -8.7882e-02, -1.2133e-01,  2.7798e-03,
                      8.9572e-02,  4.5472e-02,  8.7699e-02,  1.0058e-01,  2.3036e-02,
                      5.2959e-03,  2.3517e-02, -1.8909e-02,  6.5946e-02,  1.0925e-01,
                      1.2119e-02],
                    [-3.8906e-02,  2.5722e-01, -1.7514e+00,  1.2741e-01,  8.8538e-02,
                      4.8992e-01,  2.7472e-02, -4.5769e-02, -2.9506e-02,  1.1365e-01,
                      3.0334e-01,  2.5514e-01,  4.0342e-02,  2.1697e-01,  2.1132e-01,
                      1.5857e-01, -1.0447e-01, -1.7504e-01, -5.4133e-02, -1.1488e-01,
                      2.6780e-01, -2.0297e-02, -5.0323e-02,  7.1371e-03,  3.4310e-02,
                      1.6122e-01,  2.4689e-01,  2.9974e-01,  2.6373e-01,  2.5165e-01,
                      3.2716e-01,  2.8187e-01,  8.1486e-02,  1.8649e-01, -1.6147e-01,
                     -6.5835e-02, -5.9280e-02, -2.1905e-01, -1.9243e-01, -1.6769e-01,
                     -1.7760e-01, -2.9353e-01, -4.6459e-01, -3.7805e-01, -1.1472e-01,
                      1.1463e-01,  1.8257e-01,  1.4568e-01,  1.1474e-01,  2.3399e-02,
                      2.6706e-01,  2.7155e-01,  2.8522e-01,  1.3846e-01, -7.1238e-02,
                     -2.4729e-01],
                    [ 3.3648e-01, -6.3774e-01,  1.4335e+00,  5.5639e-01,  2.7001e-04,
                     -8.2939e-01, -3.4847e-01, -6.4637e-02, -2.0774e-01, -3.2223e-01,
                     -3.8961e-01, -2.1898e-01, -5.5440e-02, -2.2807e-01, -7.8723e-02,
                     -4.8461e-01, -1.9355e-01,  3.7654e-02,  1.4289e-01, -8.0443e-02,
                      1.3063e-01,  7.4473e-02, -4.7923e-02, -3.3240e-01, -2.2588e-01,
                      3.9526e-02,  1.4570e-01,  1.3986e-01,  2.6883e-01,  3.3923e-01,
                      2.8692e-01,  1.4695e-01,  2.2072e-01,  3.4411e-01,  3.3423e-01,
                      6.6300e-03, -1.1355e-01, -1.1812e-01, -5.0629e-02, -5.0916e-02,
                     -7.2028e-02, -2.6627e-01, -5.6601e-01, -6.2902e-01, -6.4426e-01,
                     -5.7755e-01, -4.1909e-01, -2.4197e-01,  2.4454e-02, -1.6738e-01,
                     -5.9531e-02, -9.3488e-02, -1.6436e-01, -6.9497e-02,  5.1467e-02,
                     -6.7463e-02],
                    [ 5.8546e-02, -1.7747e-01,  6.0087e-01, -5.2252e-04,  2.4187e-02,
                     -2.8056e-02, -6.0695e-02,  2.3104e-01,  1.6305e-01,  1.0924e-01,
                      8.8236e-02,  5.9427e-03,  4.2962e-02,  1.0267e-01,  1.3196e-01,
                     -1.0830e-01, -8.8868e-02,  2.9426e-01,  6.0332e-02,  2.2017e-02,
                      8.6852e-03, -7.6501e-02, -1.0688e-01, -2.0578e-01, -1.3539e-02,
                      2.5629e-02, -1.2307e-01, -4.1725e-02, -1.8924e-02, -1.4393e-01,
                     -2.7134e-01, -9.7338e-02, -1.3694e-01, -3.3830e-02,  4.9438e-01,
                      2.1309e-01,  2.4636e-01,  1.2325e-01,  1.5591e-01,  3.2296e-02,
                      1.1432e-01,  1.3553e-01, -1.0793e-01, -1.6454e-01, -1.0692e-01,
                     -1.6603e-01,  8.9779e-03, -7.9099e-02, -6.8704e-02, -6.0136e-02,
                      1.4625e-01,  8.0313e-02, -4.8874e-02,  5.5786e-02,  8.1427e-02,
                     -2.2995e-02],
                    [-1.1119e+00, -9.7381e-01, -3.7606e-01,  1.5340e-01,  5.6004e-02,
                      7.9537e-01,  5.1514e-01,  6.7084e-01,  3.6069e-01,  4.6672e-01,
                      6.3586e-01,  1.2361e+00,  5.4458e-02,  4.4888e-01,  3.4623e-01,
                     -7.8211e-02,  1.7609e-01,  2.5912e-01,  2.9812e-01,  2.3110e-01,
                      2.0134e-01, -6.1154e-02, -6.4564e-03,  3.1285e-01,  3.6553e-02,
                      4.4154e-01,  6.3237e-01,  8.4420e-01,  1.1231e+00,  1.3096e+00,
                      1.3350e+00,  1.3169e+00,  1.1446e+00,  5.8702e-01,  8.7077e-02,
                     -7.6181e-01, -1.0734e+00, -1.2484e+00, -1.2376e+00, -1.2156e+00,
                     -1.1791e+00, -9.7212e-01, -6.4376e-01, -3.4615e-01, -1.1356e-01,
                      2.5381e-01,  4.5731e-01,  6.0883e-01,  6.8720e-01, -8.3891e-01,
                      6.2651e-01,  8.1837e-01,  8.9260e-01,  9.1260e-01,  3.2260e-01,
                     -9.8295e-01],
                    [ 9.1535e-01, -1.8074e+00,  1.8815e-01, -6.2041e-02, -1.2599e-02,
                     -7.4529e-01, -5.4648e-01, -5.8447e-01, -4.5230e-01, -3.9348e-01,
                     -4.8381e-01, -1.0659e+00, -2.9770e-01, -5.1499e-01, -4.2147e-01,
                     -2.4996e-01, -2.5094e-01, -2.4765e-01, -1.1093e-01, -8.9495e-02,
                     -3.9652e-02,  3.5043e-02, -2.2566e-02, -2.3263e-01, -1.6356e-01,
                     -1.1322e+00, -1.3055e+00, -1.2845e+00, -1.0519e+00, -9.2288e-01,
                     -5.4272e-01,  3.5072e-01,  1.5925e+00,  2.3092e+00,  6.5942e-01,
                     -8.3098e-01, -1.0067e+00, -8.9203e-01, -1.0039e+00, -1.1965e+00,
                     -8.5223e-01,  2.2133e-01,  1.7310e+00,  1.6965e+00,  1.0636e+00,
                      6.4949e-01,  2.6775e-01, -4.7165e-02, -3.7327e-01, -2.8686e+00,
                      6.4927e-01,  6.9670e-01,  7.0795e-01,  7.1478e-01,  3.1973e-01,
                     -2.5693e+00],
                    [-7.3934e-02,  7.0198e-02,  2.2590e-01,  4.9484e-02, -1.0391e-01,
                     -1.6684e-01, -1.1900e-02, -2.3277e-02,  6.3789e-02, -1.2004e-02,
                      1.4248e-01, -6.8000e-02, -2.2170e-02,  6.0601e-02,  5.4319e-02,
                      5.3706e-03,  3.6641e-02,  4.4790e-02,  5.2938e-03, -3.6688e-02,
                      3.3774e-02,  1.2327e-02, -1.7138e-01, -1.1597e-02,  6.7697e-02,
                     -8.6218e-02, -1.5837e-02, -3.3704e-03, -6.9258e-02, -1.4105e-01,
                      2.5695e-03, -2.0978e-01, -3.4207e-01, -2.1496e-01,  2.4776e-01,
                      2.0992e-01,  6.4422e-02,  5.0799e-02,  1.4712e-01,  3.2227e-02,
                      1.4373e-01,  1.5062e-01,  6.3096e-03, -1.7201e-01,  4.6564e-02,
                      9.5400e-02,  6.6297e-03,  2.8192e-02, -3.6574e-02,  3.1847e-02,
                     -2.9076e-02, -7.6315e-02,  7.6755e-03,  6.6343e-02,  9.9783e-02,
                     -5.2374e-02],
                    [-2.5522e-01,  4.6821e-01, -9.5963e-01,  5.8046e-01,  1.7474e-01,
                      4.1009e-01,  2.7997e-01,  4.3716e-01,  3.1789e-02,  4.4291e-02,
                      2.4212e-01,  9.2161e-01,  2.2880e-03,  2.0859e-01,  4.3318e-02,
                     -5.1349e-02, -9.7250e-02, -4.0511e-02,  1.9764e-01,  1.2849e-01,
                      4.3351e-01,  2.4613e-02,  5.4710e-02,  4.4987e-02, -8.5056e-02,
                     -3.1730e-01, -3.5047e-01, -4.1137e-01, -4.7066e-01, -3.7748e-01,
                     -3.7737e-01, -3.0269e-01, -1.9244e-01, -2.2264e-01, -1.7633e-01,
                     -8.7452e-02,  5.8684e-02,  2.2100e-01,  3.3221e-01,  3.2697e-01,
                      2.8390e-01,  4.8635e-01,  6.0471e-01,  6.6388e-01,  5.3513e-01,
                      4.5366e-01,  3.5571e-01,  1.8685e-01, -3.5974e-02,  4.6679e-01,
                     -1.1762e-01,  1.6446e-01,  1.6282e-01,  8.7484e-02,  1.6215e-01,
                      4.2863e-01],
                    [ 1.7679e-02, -1.7197e-01,  1.1369e-01,  4.6212e-01, -1.0654e-01,
                     -2.3073e-01,  1.8183e-01,  5.7336e-02,  3.0706e-02, -1.4014e-01,
                      5.0070e-02,  3.2003e-02,  7.0526e-02,  6.3089e-02,  3.8743e-02,
                     -5.6618e-02,  2.5570e-02, -4.6891e-02,  5.3802e-02,  2.0393e-03,
                     -4.1424e-02,  1.3109e-03,  1.3134e-01, -1.3910e-01, -9.1821e-02,
                      2.2063e-01,  9.2872e-02,  2.9796e-01,  3.4915e-01,  2.6936e-01,
                      2.6538e-01,  2.0978e-01,  2.9419e-01, -1.1763e-01, -1.7401e-01,
                     -2.1172e-01, -8.9650e-02, -2.0237e-01, -1.7990e-01, -1.4242e-01,
                     -2.7767e-01, -1.2547e-01, -1.4733e-01, -2.0316e-01, -4.9766e-02,
                     -3.6871e-02,  4.7350e-02,  1.1792e-01,  5.1592e-02,  1.6399e-02,
                     -1.1771e-01,  1.4295e-01,  8.6799e-02, -1.0577e-01,  7.2028e-02,
                     -7.2114e-02]], requires_grad=True))
            ('0.bias', Parameter containing:
            tensor([-1.5522e-03,  6.9083e-02,  6.7617e-01, -1.0894e+00,  2.1665e-01,
                     2.1052e+00, -2.3456e+00,  1.1926e-01,  1.2289e+00, -2.6396e-02],
                   requires_grad=True))
            ('2.weight', Parameter containing:
            tensor([[ 0.7876,  0.4478, -1.7032, -1.8252,  1.1987, -4.3419,  6.1314,  0.9146,
                      2.4574, -0.3667]], requires_grad=True))
            ('2.bias', Parameter containing:
            tensor([1.3419], requires_grad=True))
            ```
         
            æ˜¾ç¤ºéšè—å±‚æ‰€æœ‰èŠ‚ç‚¹åˆ°è¾“å‡ºå±‚çš„æƒé‡
         
            ```python
            dic = dict(neu.named_parameters())
            weights = dic['2.weight']
            # å› ä¸ºweights.data.numpy()æ˜¯1*1*10çš„çŸ©é˜µï¼Œå–[0]å˜æˆ1*10çš„çŸ©é˜µ
            fig=plt.figure()
            plt.plot(weights.data.numpy()[0],'o-')
            plt.xlabel('hidden Neurons')
            plt.ylabel('hidden-output Weight')
            fig.savefig("3-13.jpg")
            ```
         
            ![](.\3-13.jpg)
         
            æŠŠè¾“å…¥å±‚åˆ°éšè—å±‚ç¬¬5å·èŠ‚ç‚¹çš„æƒé‡éƒ½æ˜¾ç¤ºå‡ºæ¥
         
            ```python
            # æ‰¾åˆ°äº†ä¸å³°å€¼ç›¸åº”çš„ç¥ç»å…ƒï¼ŒæŠŠå®ƒåˆ°è¾“å…¥å±‚çš„æƒé‡è¾“å‡ºå‡ºæ¥
            dic = dict(neu.named_parameters())
            weights = dic['0.weight'][4]
            fig=plt.figure()
            plt.plot(weights.data.numpy(),'o-')
            plt.xlabel('Input Neurons')
            plt.ylabel('Weight')
            fig.savefig("3-14.jpg")
            ```
         
            ![](.\3-14.jpg)
         
            ```python
            # åˆ—å‡ºæ‰€æœ‰çš„featuresä¸­çš„æ•°æ®åˆ—ï¼Œæ‰¾åˆ°å¯¹åº”çš„ç¼–å·
            for (i, c) in zip(range(len(features.columns)), features.columns):
                print(i,c)
            0 yr
            1 holiday
            2 temp
            3 hum
            4 windspeed
            5 season_1
            6 season_2
            7 season_3
            8 season_4
            9 weathersit_1
            10 weathersit_2
            11 weathersit_3
            12 weathersit_4
            13 mnth_1
            14 mnth_2
            15 mnth_3
            16 mnth_4
            17 mnth_5
            18 mnth_6
            19 mnth_7
            20 mnth_8
            21 mnth_9
            22 mnth_10
            23 mnth_11
            24 mnth_12
            25 hr_0
            26 hr_1
            27 hr_2
            28 hr_3
            29 hr_4
            30 hr_5
            31 hr_6
            32 hr_7
            33 hr_8
            34 hr_9
            35 hr_10
            36 hr_11
            37 hr_12
            38 hr_13
            39 hr_14
            40 hr_15
            41 hr_16
            42 hr_17
            43 hr_18
            44 hr_19
            45 hr_20
            46 hr_21
            47 hr_22
            48 hr_23
            49 weekday_0
            50 weekday_1
            51 weekday_2
            52 weekday_3
            53 weekday_4
            54 weekday_5
            55 weekday_6
            ```
         
            ç”±æ­¤å¯ä»¥å¾—åˆ°è¯¥ç¥ç»å…ƒæ£€æµ‹çš„æ˜¯ ç´¢å¼•2æ¸©åº¦ï¼Œç´¢å¼•7 ç§‹å¤©ï¼Œç´¢å¼•17äº”æœˆï¼Œç´¢å¼•34 ä¸Šåˆä¹ç‚¹
         
            çºµè½´çš„å€¼ä¸ºæ­£å°±æ˜¯ä¿ƒè¿›ï¼Œå€¼ä¸ºè´Ÿå°±æ˜¯æŠ‘åˆ¶ï¼Œå›¾ä¸­çš„æ³¢å³°å°±æ˜¯è®©è¯¥ç¥ç»å…ƒæ¿€æ´»ï¼Œæ³¢è°·å°±æ˜¯ç¥ç»å…ƒæœªæ¿€æ´»
      
   4. åˆ†ç±»äººå·¥ç¥ç»ç½‘è·¯Neuc

      æˆ‘ä»¬ç”¨ç¥ç»ç½‘ç»œè§£å†³ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå³å°†é¢„æµ‹æ•°å€¼æ ¹æ®å¤§äºæˆ–è€…å°äºé¢„æµ‹æ•°é‡çš„å¹³å‡å€¼æ¥åˆ†æˆä¸¤ç±» æˆ‘ä»¬åªéœ€è¦å¯¹Neucè¿›è¡Œå°å°çš„æ›´æ”¹ï¼Œå°†å…¶è¾“å‡ºå•å…ƒæ•°é‡è®¾ç½®ä¸º2ï¼Œå¹¶åŠ ä¸ŠSigmoidå‡½æ•°å°±å¯ä»¥äº†ã€‚å¯¹äºNeucæ¥è¯´ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸¤ä¸ªæ•°å€¼ï¼Œåˆ†åˆ«è¡¨ç¤ºå±äºç¬¬0ç±»å’Œç¬¬1ç±»çš„æ¦‚ç‡

      äº¤å‰ç†µå…·ä½“å®ç°
      
      ```python
      import torch
      import torch.nn.functional as F
      import torch.nn as nn
      x = torch.randn(5, 5)
      target = torch.tensor([0, 2, 3, 1, 4]) # æ ‡ç­¾ è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªtorch.tensorä¸torch.Tensorçš„çŸ¥è¯†ç‚¹https://blog.csdn.net/weixin_40607008/article/details/107348254
      # one_hotæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€è¡Œçš„æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªç±»åˆ«
      one_hot = F.one_hot(target).float() # å¯¹æ ‡ç­¾è¿›è¡Œone_hotç¼–ç 
      # reshapeå±•å¼€æˆ1åˆ—
      print(one_hot)
      # print(x)
      # print(torch.sum(torch.exp(x),dim=1))
      softmax = torch.exp(x)/torch.sum(torch.exp(x), dim = 1).reshape(-1, 1)
      logsoftmax = torch.log(softmax)
      print(logsoftmax)
      print(one_hot*logsoftmax)
      nllloss = -torch.sum(one_hot*logsoftmax)/target.shape[0]
      nllloss
      
      tensor([[1., 0., 0., 0., 0.],
              [0., 0., 1., 0., 0.],
           [0., 0., 0., 1., 0.],
              [0., 1., 0., 0., 0.],
              [0., 0., 0., 0., 1.]])
      tensor([[-4.3277, -1.2263, -0.9527, -1.4808, -2.5225],
              [-2.1342, -1.3695, -3.6855, -0.6750, -2.3730],
              [-2.4937, -2.1367, -1.6788, -1.2862, -1.0893],
              [-2.3628, -1.9214, -1.6912, -0.5993, -3.6517],
              [-0.4310, -2.0773, -5.0553, -4.5454, -1.5709]])
      tensor([[-4.3277, -0.0000, -0.0000, -0.0000, -0.0000],
              [-0.0000, -0.0000, -3.6855, -0.0000, -0.0000],
              [-0.0000, -0.0000, -0.0000, -1.2862, -0.0000],
              [-0.0000, -1.9214, -0.0000, -0.0000, -0.0000],
              [-0.0000, -0.0000, -0.0000, -0.0000, -1.5709]])
      tensor(2.5583)
      ```
      
      äºŒåˆ†ç±»é—®é¢˜ï¼Œäººå·¥ç¥ç»ç½‘ç»œå®ç°
      
      ```python
      # é‡æ–°æ„é€ ç”¨äºåˆ†ç±»çš„äººå·¥ç¥ç»ç½‘ç»œNeuc
      
      input_size = features.shape[1]
      hidden_size = 10
      output_size = 2
      batch_size = 128
      neuc = torch.nn.Sequential(
          torch.nn.Linear(input_size, hidden_size),
          torch.nn.Sigmoid(),
          torch.nn.Linear(hidden_size, output_size),
          torch.nn.Sigmoid(),
      )
      # å°†æŸå¤±å‡½æ•°å®šä¹‰ä¸ºäº¤å‰ç†µ
      # ä¸ºä»€ä¹ˆä¸é‡‡ç”¨å‡æ–¹è¯¯å·®ï¼Œä¸»è¦åŸå› æ˜¯é€»è¾‘å›å½’é…åˆMSEæŸå¤±å‡½æ•°æ—¶
      # é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œå­¦ä¹ æ—¶ï¼Œä¼šå‡ºç°æ¨¡å‹ä¸€å¼€å§‹è®­ç»ƒæ—¶ï¼Œå­¦ä¹ é€Ÿç‡éå¸¸æ…¢çš„æƒ…å†µï¼ˆMSEæŸå¤±å‡½æ•°ï¼‰ã€‚
      cost = torch.nn.CrossEntropyLoss()
      optimizer = torch.optim.SGD(neuc.parameters(), lr = 0.1)
      
      # Yæ˜¯è®­ç»ƒæ•°æ®cntåˆ—
      Y_labels = Y > np.mean(Y)
      Y_labels = Y_labels.astype(int)
      Y_labels = Y_labels.reshape(-1)
      Y_labels
      ```
      
      ```python
      # å®šä¹‰ä¸€ä¸ªä¸“é—¨è®¡ç®—åˆ†ç±»é”™è¯¯ç‡çš„å‡½æ•°ï¼Œå®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œå¯¹äºé¢„æµ‹å‘é‡predictionsçš„æ¯ä¸€è¡Œï¼Œ
      # å–æœ€å¤§çš„é‚£ä¸ªå…ƒç´ çš„ä¸‹æ ‡ï¼Œä¸æ ‡ç­¾labelsä¸­çš„å…ƒç´ åšæ¯”è¾ƒ,å¤§çš„é‚£ä¸ªè¾“å‡ºå€¼å¯¹åº”é¢„æµ‹ç±»åˆ«
      def error_rate(predictions, labels):
          """è®¡ç®—é¢„æµ‹é”™è¯¯ç‡çš„å‡½æ•°ï¼Œå…¶ä¸­predictionsæ˜¯æ¨¡å‹ç»™å‡ºçš„ä¸€ç»„é¢„æµ‹ç»“æœï¼Œlabelsæ˜¯æ•°æ®ä¹‹ä¸­çš„æ­£ç¡®ç­”æ¡ˆ"""
      #     å–æ¯ä¸€è¡Œå…³äºåˆ—çš„æœ€å¤§    
          predictions = np.argmax(predictions, 1)
          return 100.0 - (100.0 *np.sum( predictions == labels) /predictions.shape[0])
      
      # ç¥ç»ç½‘ç»œè®­ç»ƒå¾ªç¯
      losses = []
      errors = []
      for i in range(4000):
          # æ¯128ä¸ªæ ·æœ¬ç‚¹è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªæ’®
       batch_loss = []
          batch_errors = []
          for start, end in zip(range(0, len(X), batch_size), range(batch_size, len(X)+1, batch_size)):
      #        startä¸endç›¸å·®128,åªæ˜¯ä¸ºäº†å–æ•°æ®
      # 
              xx = torch.tensor(X[start:end], dtype = torch.float, requires_grad = True)
              #yyæ˜¯ç±»åˆ«æ ‡ç­¾
              yy = torch.tensor(Y_labels[start:end], dtype = torch.long)
              #predict æ˜¯è¾“å‡ºå±‚ä¸¤ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå€¼
              predict = neuc(xx)
              #lossæ˜¯äº¤å‰ç†µ
              loss = cost(predict, yy)
              err = error_rate(predict.data.numpy(), yy.data.numpy())
              optimizer.zero_grad()
              loss.backward()
              optimizer.step()
           batch_loss.append(loss.data.numpy())
              batch_errors.append(err)
       
          # æ¯éš”100æ­¥è¾“å‡ºä¸€ä¸‹æŸå¤±å€¼ï¼ˆlossï¼‰
       if i % 100==0:
              losses.append(np.mean(batch_loss))
           errors.append(np.mean(batch_errors))
              print(i, np.mean(batch_loss), np.mean(batch_errors))
          
      0 0.67895323 41.19155534351145
      100 0.4432484 12.750477099236642
      200 0.43353826 12.213740458015268
      300 0.42870235 11.838024809160306
      400 0.41688758 10.239742366412214
      500 0.4048622 8.778625954198473
      600 0.39466304 7.615696564885496
      700 0.38711914 6.882156488549619
      800 0.3819186 6.309637404580153
      900 0.3783126 5.898139312977099
      1000 0.3756589 5.689408396946565
      1100 0.373587 5.516459923664122
      1200 0.3719284 5.337547709923665
      1300 0.37057498 5.194417938931298
      1400 0.36945093 5.093034351145038
      1500 0.36849403 5.01550572519084
      1600 0.36765516 4.9856870229007635
      1700 0.36690468 4.9439408396946565
      1800 0.36622706 4.890267175572519
      1900 0.3656102 4.818702290076335
      2000 0.36504015 4.747137404580153
      2100 0.36450353 4.675572519083969
      2200 0.363987 4.639790076335878
      2300 0.36347347 4.58611641221374
      2400 0.36297363 4.538406488549619
      2500 0.36251283 4.550333969465649
      2600 0.36208788 4.490696564885496
      2700 0.36169383 4.413167938931298
      2800 0.36132643 4.383349236641221
      2900 0.3609817 4.341603053435114
      3000 0.360656 4.293893129770993
      3100 0.36034608 4.276001908396947
      3200 0.36004904 4.270038167938932
      3300 0.3597618 4.270038167938932
      3400 0.35948175 4.2461832061068705
      3500 0.35920674 4.240219465648855
      3600 0.35893834 4.2044370229007635
      3700 0.35867992 4.198473282442748
      3800 0.35843205 4.168654580152672
      3900 0.35819414 4.138835877862595
      ```
      
      ```python
      # æ‰“å°è¾“å‡ºäº¤å‰ç†µæŸå¤±å€¼å’Œé”™è¯¯ç‡
      fig=plt.figure()
      plt.plot(np.arange(len(losses))*100,losses, label = 'Cross Entropy')
      plt.plot(np.arange(len(losses))*100, np.array(errors) / float(100), label = 'Error Rate')
      plt.xlabel('epoch')
      plt.ylabel('Cross Entropy/Error rates')
      plt.legend()
      plt.savefig("3-15.jpg")
      ```
      
      ![](.\3-15.jpg)
      
      å°†ç¥ç»ç½‘ç»œçš„åˆ†ç±»ç»“æœé€šè¿‡å›¾åƒæ˜¾ç¤º
      
      ```python
      # è¯»å–æµ‹è¯•æ•°æ®
      targets = test_targets['cnt']
      targets = targets.values.reshape(-1, 1)
      Y_labels = targets > np.mean(Y)
      Y_labels = Y_labels.astype(int)
      Y_labels = Y_labels.reshape(-1)
      x = torch.tensor(test_features.values, dtype = torch.float, requires_grad = True)
      
      # æ‰“å°ç¥ç»ç½‘ç»œé¢„æµ‹çš„é”™è¯¯ç‡
      predict = neuc(x)
      print("error_rate ",error_rate(predict.data.numpy(), Y_labels))
      
      # æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠé¢„æµ‹æ­£ç¡®çš„æ•°æ®å’Œé”™è¯¯çš„æ•°æ®åˆ†åˆ«ç”»å‡ºæ¥ï¼Œçºµåæ ‡åˆ†åˆ«æ˜¯é¢„æµ‹æ­£ç¡®çš„æ¦‚ç‡å’Œé¢„æµ‹é”™è¯¯çš„æ¦‚ç‡
      prob = predict.data.numpy()
      # rights é¢„æµ‹æ­£ç¡®çš„è¡Œå·ä¸ºtrueï¼Œé¢„æµ‹é”™è¯¯çš„è¡Œå·ä¸ºfalseï¼Œè¿”å›ä¸€ä¸ªtrue ï¼Œfalseçš„åˆ—è¡¨
      rights = np.argmax(prob, 1) == Y_labels
      wrongs = np.argmax(prob, 1) != Y_labels
      # å°†rightsä¸­ä¸ºtrueçš„è¡Œå·ä»£å…¥ï¼Œå¹¶è¿”å›æ ‡ç­¾
      right_labels = Y_labels[rights]
      wrong_labels = Y_labels[wrongs]
      probs = prob[rights, :]
      probs1 = prob[wrongs, :]
      # rightnessæœ€å¤§çš„è¾“å‡ºå€¼é¢„æµ‹æ­£ç¡®ï¼Œæœ€å¤§è¾“å‡ºå€¼çš„æ‰€å½¢æˆçš„list
      rightness = [probs[i, right_labels[i]] for i in range(len(right_labels))]
      # right_indexæ˜¯targetsä¸­é¢„æµ‹æ­£ç¡®çš„ä¸‹æ ‡
      right_index = np.arange(len(targets))[rights]
      # wrongnessæœ€å¤§çš„è¾“å‡ºå€¼é¢„æµ‹é”™è¯¯ï¼Œæœ€å¤§è¾“å‡ºå€¼çš„æ‰€å½¢æˆçš„list
      wrongness = [probs1[i, wrong_labels[i]] for i in range(len(wrong_labels))]
      # wrong_indexæ˜¯targetsä¸­é¢„æµ‹æ­£ç¡®çš„ä¸‹æ ‡
      wrong_index = np.arange(len(targets))[wrongs]
      fig, ax = plt.subplots(figsize = (8, 6))
      ax.plot(right_index, rightness, '.', label='Right')
      ax.plot(wrong_index, wrongness,'o',label='Wrong')
      
      ax.legend()
      plt.ylabel('Probabilities')
      
      dates = pd.to_datetime(rides.loc[test_features.index]['dteday'])
      dates = dates.apply(lambda d: d.strftime('%b %d'))
      ax.set_xticks(np.arange(len(dates))[12::24])
      ax.set_xticklabels(dates[12::24], rotation=45)
      fig.savefig("3-16.jpg")
      ```
      
      ![](.\3-16.jpg)

4. æœºå™¨ä¹Ÿæ‡‚å¾—æ„Ÿæƒ…-ä¸­æ–‡æƒ…ç»ªåˆ†ç±»å™¨

   1. æ•°æ®å¤„ç†

      1. å¯¹äº¬ä¸œä¸ŠæŠ“å–çš„è¯„è®ºè¿›è¡Œé¢„å¤„ç†

         ```python
         # æ•°æ®æ¥æºæ–‡ä»¶
         good_file = 'data/good.txt'
         bad_file  = 'data/bad.txt'
         
         # å°†æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·è¿‡æ»¤æ‰
         def filter_punc(sentence):
             sentence = re.sub("[\s+\.\!\/_,$%^*(+\"\'â€œâ€ã€Šã€‹?â€œ]+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰ï¼š]+", "", sentence)  
             return(sentence)
         
         #æ‰«ææ‰€æœ‰çš„æ–‡æœ¬ï¼Œåˆ†è¯ã€å»ºç«‹è¯å…¸ï¼Œåˆ†å‡ºæ­£å‘è¿˜æ˜¯è´Ÿå‘çš„è¯„è®ºï¼Œis_filterå¯ä»¥è¿‡æ»¤æ˜¯å¦ç­›é€‰æ‰æ ‡ç‚¹ç¬¦å·
         def Prepare_data(good_file, bad_file, is_filter = True):
             all_words = [] #å­˜å‚¨æ‰€æœ‰çš„å•è¯
             pos_sentences = [] #å­˜å‚¨æ­£å‘çš„è¯„è®º
             neg_sentences = [] #å­˜å‚¨è´Ÿå‘çš„è¯„è®º
             with open(good_file, 'r',encoding = 'UTF-8') as fr:
                 for idx, line in enumerate(fr):
                     if is_filter:
                         #è¿‡æ»¤æ ‡ç‚¹ç¬¦å·
                         line = filter_punc(line)
                     #åˆ†è¯,å°†å¥å­lineåˆ†æˆå¤šä¸ªåˆ†è¯ï¼Œå½¢æˆ1ä¸ªåˆ†è¯list
                     words = jieba.lcut(line)
         #             print(line,words)
                     if len(words) > 0:
                         all_words += words
                         pos_sentences.append(words)
             print('{0} åŒ…å« {1} è¡Œ, {2} ä¸ªè¯.'.format(good_file, idx+1, len(all_words)))
         
             count = len(all_words)
             with open(bad_file,'r',encoding = 'UTF-8') as fr:
                 for idx, line in enumerate(fr):
                     if is_filter:
                         line = filter_punc(line)
                     words = jieba.lcut(line)
                     if len(words) > 0:
         #                 +=listå»æ‰æ‹¬å·å†åŠ å…¥
                         all_words += words
         #               appendä¸å»æ‰æ‹¬å·æŠŠæ•´ä¸ªliståŠ å…¥    
                         neg_sentences.append(words)
             print('{0} åŒ…å« {1} è¡Œ, {2} ä¸ªè¯.'.format(bad_file, idx+1, len(all_words)-count))
         
             #å»ºç«‹è¯å…¸ï¼Œdictionçš„æ¯ä¸€é¡¹ä¸º{w:[id, å•è¯å‡ºç°æ¬¡æ•°]}
             diction = {}
             cnt = Counter(all_words)
             for word, freq in cnt.items():
                 #ç¬¬ä¸€é¡¹ä¸ºè¯çš„æ ‡å·ï¼Œç¬¬äºŒé¡¹ä¸ºè¯çš„å‡ºç°æ¬¡æ•°
                 diction[word] = [len(diction), freq]
             print('å­—å…¸å¤§å°ï¼š{}'.format(len(diction)))
             return(pos_sentences, neg_sentences, diction)
         
         #æ ¹æ®å•è¯è¿”è¿˜å•è¯çš„ç¼–ç 
         def word2index(word, diction):
             if word in diction:
                 value = diction[word][0]
             else:
                 value = -1
             return(value)
         
         #æ ¹æ®ç¼–ç è·å¾—å•è¯
         def index2word(index, diction):
             for w,v in diction.items():
                 if v[0] == index:
                     return(w)
             return(None)
         
         pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, True)
         st = sorted([(v[1], w) for w, v in diction.items()])
         st
         ```

   2. è¯è¢‹æ¨¡å‹

      è¯è¢‹æ¨¡å‹å®é™…ä¸Šæ˜¯ä¸€ç§å¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–çš„æ‰‹æ®µï¼Œé€šè¿‡ç»Ÿè®¡å‡ºè¯è¡¨ä¸Šçš„æ¯ä¸ªå•è¯å‡ºç°çš„é¢‘ç‡ï¼Œä»è€Œå°†ä¸€ç¯‡æ–‡ç« å‘é‡åŒ–ã€‚

      1. è®­ç»ƒæ•°æ®å‡†å¤‡

         ```python
         # è¾“å…¥ä¸€ä¸ªå¥å­å’Œç›¸åº”çš„è¯å…¸ï¼Œå¾—åˆ°è¿™ä¸ªå¥å­çš„å‘é‡åŒ–è¡¨ç¤º
         # å‘é‡çš„å°ºå¯¸ä¸ºè¯å…¸ä¸­è¯æ±‡çš„ä¸ªæ•°ï¼Œiä½ç½®ä¸Šé¢çš„æ•°å€¼ä¸ºç¬¬iä¸ªå•è¯åœ¨sentenceä¸­å‡ºç°çš„é¢‘ç‡
         def sentence2vec(sentence, dictionary):
             #vectoræ˜¯å­—å…¸ä¸­æ‰€æœ‰åˆ†è¯çš„å‘é‡
             vector = np.zeros(len(dictionary))
             for l in sentence:
                 vector[l] += 1
             #vectorå‘é‡å…ƒç´ å’Œä¸º1
             return(1.0 * vector / len(sentence))
         
         # éå†æ‰€æœ‰å¥å­ï¼Œå°†æ¯ä¸€ä¸ªè¯æ˜ å°„æˆç¼–ç 
         dataset = [] #æ•°æ®é›†(å­˜æ”¾æ‰€æœ‰å¥å­çš„ç¼–ç )
         labels = [] #æ ‡ç­¾
         sentences = [] #åŸå§‹å¥å­ï¼Œè°ƒè¯•ç”¨
         # å¤„ç†æ­£å‘è¯„è®º
         for sentence in pos_sentences:
             new_sentence = []
             for l in sentence:
                 if l in diction:
                     #æ ¹æ®åˆ†è¯å¾—åˆ°ç¼–ç ï¼Œå¹¶å½¢æˆåˆ†è¯ç´¢å¼•å½¢æˆçš„åˆ—è¡¨ï¼ˆä¸€ä¸ªå¥å­ï¼‰
                     new_sentence.append(word2index(l, diction))
             #æŠŠæ‰€æœ‰çš„åˆ†è¯ç´¢å¼•æ‰€å½¢æˆçš„åˆ—è¡¨ï¼ˆä¸€ä¸ªå¥å­ï¼‰è½¬æ¢æˆå…³äºå­—å…¸çš„å‘é‡
             dataset.append(sentence2vec(new_sentence, diction))
             labels.append(0) #æ­£æ ‡ç­¾ä¸º0
             sentences.append(sentence)
         
         # å¤„ç†è´Ÿå‘è¯„è®º
         for sentence in neg_sentences:
             new_sentence = []
             for l in sentence:
                 if l in diction:
                     new_sentence.append(word2index(l, diction))
             dataset.append(sentence2vec(new_sentence, diction))
             labels.append(1) #è´Ÿæ ‡ç­¾ä¸º1
             sentences.append(sentence)
         
         #æ‰“ä¹±æ‰€æœ‰çš„æ•°æ®é¡ºåºï¼Œå½¢æˆæ•°æ®é›†
         # indicesä¸ºæ‰€æœ‰æ•°æ®ä¸‹æ ‡çš„ä¸€ä¸ªå…¨æ’åˆ—
         # permutationéšæœºæ’åˆ—ä¸€ä¸ªæ•°ç»„
         indices = np.random.permutation(len(dataset))
         
         #é‡æ–°æ ¹æ®æ‰“ä¹±çš„ä¸‹æ ‡ç”Ÿæˆæ•°æ®é›†datasetï¼Œæ ‡ç­¾é›†labelsï¼Œä»¥åŠå¯¹åº”çš„åŸå§‹å¥å­sentences
         dataset = [dataset[i] for i in indices]
         labels = [labels[i] for i in indices]
         sentences = [sentences[i] for i in indices]
         
         #å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ’åˆ†ï¼Œåˆ†ä¸ºï¼šè®­ç»ƒé›†ã€æ ¡å‡†é›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­æ ¡å‡†å’Œæµ‹è¯•é›†åˆçš„é•¿åº¦éƒ½æ˜¯æ•´ä¸ªæ•°æ®é›†çš„10åˆ†ä¹‹ä¸€
         test_size = len(dataset) // 10
         # è®­ç»ƒé›†
         train_data = dataset[2 * test_size :]
         train_label = labels[2 * test_size :]
         
         # æ ¡å‡†é›†
         valid_data = dataset[: test_size]
         valid_label = labels[: test_size]
         
         # æµ‹è¯•é›†
         test_data = dataset[test_size : 2 * test_size]
         test_label = labels[test_size : 2 * test_size]
         ```
         
      2. å®šä¹‰æ¨¡å‹

         ```python
         # ä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œä¸‰å±‚ï¼Œç¬¬ä¸€å±‚çº¿æ€§å±‚ï¼ŒåŠ ä¸€ä¸ªéçº¿æ€§ReLUï¼Œç¬¬äºŒå±‚çº¿æ€§å±‚ï¼Œä¸­é—´æœ‰10ä¸ªéšå«å±‚ç¥ç»å…ƒ
         
         # è¾“å…¥ç»´åº¦ä¸ºè¯å…¸çš„å¤§å°ï¼šæ¯ä¸€æ®µè¯„è®ºçš„è¯è¢‹æ¨¡å‹
         model = nn.Sequential(
             nn.Linear(len(diction), 10),
             nn.ReLU(),
             nn.Linear(10, 2),
             nn.LogSoftmax(),
         )
         
         def rightness(predictions, labels):
             """è®¡ç®—é¢„æµ‹é”™è¯¯ç‡çš„å‡½æ•°ï¼Œå…¶ä¸­predictionsæ˜¯æ¨¡å‹ç»™å‡ºçš„ä¸€ç»„é¢„æµ‹ç»“æœï¼Œbatch_sizeè¡Œnum_classesåˆ—çš„çŸ©é˜µï¼Œlabelsæ˜¯æ•°æ®ä¹‹ä¸­çš„æ­£ç¡®ç­”æ¡ˆ"""
             pred = torch.max(predictions.data, 1)[1] # å¯¹äºä»»æ„ä¸€è¡Œï¼ˆä¸€ä¸ªæ ·æœ¬ï¼‰çš„è¾“å‡ºå€¼çš„ç¬¬1ä¸ªç»´åº¦ï¼Œæ±‚æœ€å¤§ï¼Œå¾—åˆ°æ¯ä¸€è¡Œçš„æœ€å¤§å…ƒç´ çš„ä¸‹æ ‡
             rights = pred.eq(labels.data.view_as(pred)).sum() #å°†ä¸‹æ ‡ä¸labelsä¸­åŒ…å«çš„ç±»åˆ«è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ç´¯è®¡å¾—åˆ°æ¯”è¾ƒæ­£ç¡®çš„æ•°é‡
             return rights, len(labels) #è¿”å›æ­£ç¡®çš„æ•°é‡å’Œè¿™ä¸€æ¬¡ä¸€å…±æ¯”è¾ƒäº†å¤šå°‘å…ƒç´ 
         ```

      3. æ¨¡å‹è®­ç»ƒ

         ```python
         # æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µ
         cost = torch.nn.NLLLoss()
         # ä¼˜åŒ–ç®—æ³•ä¸ºAdamï¼Œå¯ä»¥è‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡
         optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
         records = []
         
         #å¾ªç¯10ä¸ªEpoch
         losses = []
         for epoch in range(10):
             for i, data in enumerate(zip(train_data, train_label)):
                 x, y = data
                 
                 # éœ€è¦å°†è¾“å…¥çš„æ•°æ®è¿›è¡Œé€‚å½“çš„å˜å½¢ï¼Œä¸»è¦æ˜¯è¦å¤šå‡ºä¸€ä¸ªbatch_sizeçš„ç»´åº¦ï¼Œä¹Ÿå³ç¬¬ä¸€ä¸ªä¸º1çš„ç»´åº¦
                 x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)
                 # xçš„å°ºå¯¸ï¼šbatch_size=1, len_dictionary
                 # æ ‡ç­¾ä¹Ÿè¦åŠ ä¸€å±‚å¤–è¡£ä»¥å˜æˆ1*1çš„å¼ é‡
                 y = torch.tensor(np.array([y]), dtype = torch.long)
                 # yçš„å°ºå¯¸ï¼šbatch_size=1, 1
                 
                 # æ¸…ç©ºæ¢¯åº¦
                 optimizer.zero_grad()
                 # æ¨¡å‹é¢„æµ‹
                 predict = model(x)
                 # è®¡ç®—æŸå¤±å‡½æ•°
                 loss = cost(predict, y)
                 # å°†æŸå¤±å‡½æ•°æ•°å€¼åŠ å…¥åˆ°åˆ—è¡¨ä¸­
                 losses.append(loss.data.numpy())
                 # å¼€å§‹è¿›è¡Œæ¢¯åº¦åä¼ 
                 loss.backward()
                 # å¼€å§‹å¯¹å‚æ•°è¿›è¡Œä¸€æ­¥ä¼˜åŒ–
                 optimizer.step()
                 
                 # æ¯éš”3000æ­¥ï¼Œè·‘ä¸€ä¸‹æ ¡éªŒæ•°æ®é›†çš„æ•°æ®ï¼Œè¾“å‡ºä¸´æ—¶ç»“æœ
                 if i % 3000 == 0:
                     val_losses = []
                     rights = []
                     # åœ¨æ‰€æœ‰æ ¡éªŒæ•°æ®é›†ä¸Šå®éªŒ
                     #jæ˜¯æ ‡å·ï¼Œvalæ˜¯(valid_data,valid_label)çš„ç»„åˆ
                     for j, val in enumerate(zip(valid_data, valid_label)):
                         x, y = val
                         # xå±•å¼€æˆ1è¡Œå¤šåˆ—
                         x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)
                         y = torch.tensor(np.array([y]), dtype = torch.long)
                         predict = model(x)
                         # è°ƒç”¨rightnesså‡½æ•°è®¡ç®—å‡†ç¡®åº¦
                         right = rightness(predict, y)
                         rights.append(right)
                         loss = cost(predict, y)
                         val_losses.append(loss.data.numpy())
                         
                     # å°†æ ¡éªŒé›†åˆä¸Šé¢çš„å¹³å‡å‡†ç¡®åº¦è®¡ç®—å‡ºæ¥
                     right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])
                     print('ç¬¬{}è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š{:.2f}, æ ¡éªŒæŸå¤±ï¼š{:.2f}, æ ¡éªŒå‡†ç¡®ç‡: {:.2f}'.format(epoch, np.mean(losses),
                                                                                 np.mean(val_losses), right_ratio))
                     records.append([np.mean(losses), np.mean(val_losses), right_ratio])
         
         ç¬¬0è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.72, æ ¡éªŒæŸå¤±ï¼š0.70, æ ¡éªŒå‡†ç¡®ç‡: 0.40
         ç¬¬0è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.35, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.89
         ç¬¬0è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.31, æ ¡éªŒæŸå¤±ï¼š0.29, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬0è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.30, æ ¡éªŒæŸå¤±ï¼š0.28, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬1è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.30, æ ¡éªŒæŸå¤±ï¼š0.28, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬1è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.29, æ ¡éªŒæŸå¤±ï¼š0.30, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬1è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.27, æ ¡éªŒæŸå¤±ï¼š0.29, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬1è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.27, æ ¡éªŒæŸå¤±ï¼š0.28, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬2è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.26, æ ¡éªŒæŸå¤±ï¼š0.28, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬2è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.26, æ ¡éªŒæŸå¤±ï¼š0.31, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬2è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.25, æ ¡éªŒæŸå¤±ï¼š0.30, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬2è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.25, æ ¡éªŒæŸå¤±ï¼š0.29, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬3è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.24, æ ¡éªŒæŸå¤±ï¼š0.29, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬3è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.24, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬3è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.23, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬3è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.23, æ ¡éªŒæŸå¤±ï¼š0.30, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬4è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.23, æ ¡éªŒæŸå¤±ï¼š0.30, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬4è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.23, æ ¡éªŒæŸå¤±ï¼š0.35, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬4è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.22, æ ¡éªŒæŸå¤±ï¼š0.34, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬4è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.22, æ ¡éªŒæŸå¤±ï¼š0.31, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬5è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.22, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬5è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.22, æ ¡éªŒæŸå¤±ï¼š0.37, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬5è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.35, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬5è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬6è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.32, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬6è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.39, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬6è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.36, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬6è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.34, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬7è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.21, æ ¡éªŒæŸå¤±ï¼š0.33, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬7è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.42, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬7è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.37, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬7è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.36, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ç¬¬8è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.35, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬8è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.46, æ ¡éªŒå‡†ç¡®ç‡: 0.89
         ç¬¬8è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.20, æ ¡éªŒæŸå¤±ï¼š0.41, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬8è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.19, æ ¡éªŒæŸå¤±ï¼š0.39, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬9è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.19, æ ¡éªŒæŸå¤±ï¼š0.37, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬9è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.19, æ ¡éªŒæŸå¤±ï¼š0.45, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬9è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.19, æ ¡éªŒæŸå¤±ï¼š0.42, æ ¡éªŒå‡†ç¡®ç‡: 0.90
         ç¬¬9è½®ï¼Œè®­ç»ƒæŸå¤±ï¼š0.19, æ ¡éªŒæŸå¤±ï¼š0.40, æ ¡éªŒå‡†ç¡®ç‡: 0.91
         ```

         ç»˜åˆ¶è®­ç»ƒæŸå¤±ï¼Œæ•™ç ”æŸå¤±ï¼Œæ ¡éªŒå‡†ç¡®ç‡è®­ç»ƒæ›²çº¿

         ```python
         # ç»˜åˆ¶è¯¯å·®æ›²çº¿
         a = [i[0] for i in records]
         b = [i[1] for i in records]
         c = [i[2] for i in records]
         fig=plt.figure()
         plt.plot(a, label = 'Train Loss')
         plt.plot(b, label = 'Valid Loss')
         plt.plot(c, label = 'Valid Accuracy')
         plt.xlabel('Steps')
         plt.ylabel('Loss & Accuracy')
         plt.legend()
         fig.savefig("4-1.jpg")
         ```

         ![](.\4-1.jpg)

         

         è®¡ç®—æµ‹è¯•é›†é¢„æµ‹å‡†ç¡®ç‡

         ```python
         #åœ¨æµ‹è¯•é›†ä¸Šåˆ†æ‰¹è¿è¡Œï¼Œå¹¶è®¡ç®—æ€»çš„æ­£ç¡®ç‡
         vals = [] #è®°å½•å‡†ç¡®ç‡æ‰€ç”¨åˆ—è¡¨
         
         #å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œå¾ªç¯
         for data, target in zip(test_data, test_label):
             data, target = torch.tensor(data, dtype = torch.float).view(1,-1), torch.tensor(np.array([target]), dtype = torch.long)
             output = model(data) #å°†ç‰¹å¾æ•°æ®å–‚å…¥ç½‘ç»œï¼Œå¾—åˆ°åˆ†ç±»çš„è¾“å‡º
             val = rightness(output, target) #è·å¾—æ­£ç¡®æ ·æœ¬æ•°ä»¥åŠæ€»æ ·æœ¬æ•°
             vals.append(val) #è®°å½•ç»“æœ
         
         #è®¡ç®—å‡†ç¡®ç‡
         rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))
         right_rate = 1.0 * rights[0].data.numpy() / rights[1]
         right_rate
         
         0.8925556408288565
         ```

      4. åˆ†æç¥ç»ç½‘ç»œ

         1. æŸ¥çœ‹æ¯ä¸€å±‚çš„æ¨¡å¼

            ```python
            # å°†ç¥ç»ç½‘ç»œçš„æ¶æ„æ‰“å°å‡ºæ¥ï¼Œæ–¹ä¾¿åé¢çš„è®¿é—®
            model.named_parameters
            
            <bound method Module.named_parameters of Sequential(
              (0): Linear(in_features=7135, out_features=10, bias=True)
              (1): ReLU()
              (2): Linear(in_features=10, out_features=2, bias=True)
              (3): LogSoftmax(dim=None)
            )>
            ```

            ç»˜åˆ¶ç¬¬äºŒä¸ªå…¨é“¾æ¥å±‚æƒé‡å¤§å°

            **å¯ä»¥çœ‹å‡ºæ¥æƒé‡æ˜¯è½¬ç½®çš„ï¼Œåç½®æ˜¯1ç»´çš„**

            ```python
            # ç»˜åˆ¶å‡ºç¬¬äºŒä¸ªå…¨é“¾æ¥å±‚çš„æƒé‡å¤§å°
            # model[2]å³æå–ç¬¬2å±‚ï¼Œç½‘ç»œä¸€å…±4å±‚
            # ç¬¬0å±‚ä¸ºçº¿æ€§ç¥ç»å…ƒï¼ŒåŒ…æ‹¬weightå’Œbiasï¼Œç¬¬1å±‚ä¸ºReLUï¼Œ
            # ç¬¬2å±‚ä¸ºç¬¬äºŒå±‚ç¥ç»åŸé“¾æ¥ï¼ŒåŒ…æ‹¬weightå’Œbiasï¼Œç¬¬3å±‚ä¸ºlogsoftmax
            plt.figure(figsize = (10, 7))
            print(model[0].weight.size())
            print(model[0].bias.size())
            # model[2].weight
            for i in range(model[2].weight.size()[0]):
                #if i == 1:
                    weights = model[2].weight[i].data.numpy()
                    plt.plot(weights, 'o-', label = i)
            plt.legend()
            plt.xlabel('Neuron in Hidden Layer')
            plt.ylabel('Weights')
            plt.savefig("4-2.jpg")
            ```

            ![](.\4-2.jpg)

            ```python
            # å°†ç¬¬ä¸€å±‚ç¥ç»å…ƒçš„æƒé‡éƒ½æ‰“å°å‡ºæ¥ï¼Œä¸€æ¡æ›²çº¿è¡¨ç¤ºä¸€ä¸ªéšå«å±‚ç¥ç»å…ƒã€‚æ¨ªåæ ‡ä¸ºè¾“å…¥å±‚ç¥ç»å…ƒç¼–å·ï¼Œçºµåæ ‡ä¸ºæƒé‡å€¼å¤§å°
            plt.figure(figsize = (10, 7))
            print(model[0].weight.size())
            for i in range(model[0].weight.size()[0]):
                #if i == 1:
                    weights = model[0].weight[i].data.numpy()
                    plt.plot(weights, alpha = 0.5, label = i)
            plt.legend()
            plt.xlabel('Neuron in Input Layer')
            plt.ylabel('Weights
            
            torch.Size([10, 7135])           
            ```

            ![](.\4-3.jpg)

            ```python
            # å°†ç¬¬äºŒå±‚çš„å„ä¸ªç¥ç»å…ƒä¸è¾“å…¥å±‚çš„é“¾æ¥æƒé‡ï¼ŒæŒ‘å‡ºæ¥æœ€å¤§çš„æƒé‡å’Œæœ€å°çš„æƒé‡ï¼Œå¹¶è€ƒå¯Ÿæ¯ä¸€ä¸ªæƒé‡æ‰€å¯¹åº”çš„å•è¯æ˜¯ä»€ä¹ˆï¼ŒæŠŠå•è¯æ‰“å°å‡ºæ¥
            # model[0]æ˜¯å–å‡ºç¬¬ä¸€å±‚çš„ç¥ç»å…ƒ
            
            for i in range(len(model[0].weight)):
                print('\n')
                print('ç¬¬{}ä¸ªç¥ç»å…ƒ'.format(i))
                print('max:')
                #iæ˜¯åºå·ï¼Œwæ˜¯å€¼
                st = sorted([(w,i) for i,w in enumerate(model[0].weight[i].data.numpy())])
                for i in range(1, 20):
            #         st[][1]è¿”å›çš„æ˜¯å­—å…¸ä¸­çš„ç´¢å¼•å€¼
                    word = index2word(st[-i][1],diction)
                    print(word)
                print('min:')
                for i in range(20):
                    word = index2word(st[i][1],diction)
                    print(word)
            #åªæ˜¯æ˜¾ç¤ºäº†ç¬¬ä¸€çš„ç¥ç»å…ƒæœ€æ•æ„Ÿå’Œæœ€ä¸æ•æ„Ÿçš„å•è¯
            ç¬¬0ä¸ªç¥ç»å…ƒ
            max:
            ä¸¥é‡
            ç ´
            ä¸äº†
            é€€
            å¼€èƒ¶
            æ²¡æ³•
            å¾ˆå·®
            ç²—ç³™
            æ˜ŸæœŸ
            å‘
            å‘äºº
            å·®è¯„
            ä¸Šé¢
            ä¸¢
            ä¸å¦‚
            æ‰¾
            é†‰
            å‘é”™
            å¥½å·®
            min:
            å¾ˆæ£’
            æƒŠå–œ
            æ—¶å°š
            ç‰©ç¾ä»·å»‰
            åŠ›
            å®æƒ 
            å…‰ä¸´
            è°¢è°¢
            å®Œå…¨ä¸€è‡´
            å¯
            ç»§ç»­
            è¯•ç©¿
            æ‰˜
            å¸…æ°”
            ä¸é”™
            æ¼‚äº®
            å®Œç¾
            å´æ˜¯
            å¸å¼•
            ç‰©è¶…æ‰€å€¼
            ```

         2. å¯»æ‰¾åˆ¤æ–­é”™è¯¯çš„åŸå› 

            ```python
            # æ”¶é›†åˆ°åœ¨æµ‹è¯•é›†ä¸­åˆ¤æ–­é”™è¯¯çš„å¥å­
            # wrong_sentenceså­˜æ”¾åˆ¤æ–­å‡ºé”™çš„å¥å­
            wrong_sentences = []
            # targetå­˜æ”¾åˆ¤æ–­å‡ºé”™çš„ç›®æ ‡å€¼
            targets = []
            j = 0
            # å­˜æ”¾åˆ¤æ–­å‡ºé”™çš„ä¸‹æ ‡
            sent_indices = []
            for data, target in zip(test_data, test_label):
                predictions = model(torch.tensor(data, dtype = torch.float).view(1,-1))
                pred = torch.max(predictions.data, 1)[1]
                target = torch.tensor(np.array([target]), dtype = torch.long).view_as(pred)
                rights = pred.eq(target)
            #     print(rights,np.where(rights.numpy() == 0)[0])
                indices = np.where(rights.numpy() == 0)[0]
            #     print("\n")
            # 
                for i in indices:
                    wrong_sentences.append(data)
                    targets.append(target[i])
                    sent_indices.append(test_size + j + i)
                j += len(target)
            ```
         
            

   3. 

      

      

      

      

5. æ‰‹å†™æ•°å­—è¯†åˆ«-è®¤è¯†å·ç§¯ç¥ç»ç½‘ç»œ-è®¤è¯†å·ç§¯ç¥ç»ç½‘ç»œ

6. æ‰‹å†™æ•°å­—åŠ æ³•æœº-è¿ç§»å­¦ä¹ 

7. ä½ è‡ªå·±çš„Prisma-å›¾åƒé£æ ¼è¿ç§»

8. äººå·¥æ™ºèƒ½é€ å‡æœ¯--å›¾åƒç”Ÿæˆä¸å¯¹æŠ—å­¦ä¹ 

9. è¯æ±‡æ˜Ÿç©º--ç¥ç»è¯­è¨€æ¨¡å‹ä¸Word2Vec

10. LSTMä½œæ›²æœº-åºåˆ—ç”Ÿæˆæ¨¡å‹

11. ç¥ç»ç¿»è¯‘æœº--ç«¯åˆ°ç«¯æœºå™¨ç¿»è¯‘

12. AIæ¸¸æˆé«˜æ‰‹--æ·±åº¦å¼ºåŒ–å­¦ä¹ 

